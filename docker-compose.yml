x-kc-db-env:
  KC_DB: ${KC_DB}
  KC_DB_URL_HOST: ${KC_DB_URL_HOST}
  KC_DB_URL_PORT: ${KC_DB_URL_PORT}
  KC_DB_URL_DATABASE: ${KC_DB_NAME}
  KC_DB_USERNAME: ${KC_DB_USERNAME}
  KC_DB_PASSWORD: ${KC_DB_PASSWORD}
  KC_DB_URL: ${KC_DB_URL}
  KC_DB_URL_PROPERTIES: ${KC_DB_URL_PROPERTIES}
  KC_LOG_LEVEL: TRACE
  QUARKUS_TRANSACTION_MANAGER_DEFAULT_TRANSACTION_TIMEOUT: PT30M
  QUARKUS_DATASOURCE_JDBC_ACQUISITION_TIMEOUT: 600s
  QUARKUS_DATASOURCE_JDBC_BACKGROUND_VALIDATION_INTERVAL: 30S
  QUARKUS_DATASOURCE_JDBC_VALIDATION_QUERY: SELECT 1
  QUARKUS_LOG_LEVEL: INFO
  KC_DB_POOL_INITIAL_SIZE: '1'
  KC_DB_POOL_MIN_SIZE: '1'
  KC_DB_POOL_MAX_SIZE: '5'
  KEYCLOAK_URL: ${KEYCLOAK_URL}
  KEYCLOAK_ADMIN: admin
  KEYCLOAK_ADMIN_PASSWORD: admin
  KEYCLOAK_REALM: OSSS
  KEYCLOAK_HEALTH_URL: ${KEYCLOAK_HEALTH_URL}
  KC_FEATURES: hostname:v1
networks:
  osss-net:
    external: true
    name: osss-net
volumes:
  consul_data: null
  kc_postgres_data: null
  osss_postgres_data: null
  redis-data: null
  es-data: null
  filebeat-data: null
  es-shared: null
  pg_superset_data: null
  superset_redis_data: null
  trino_data: null
  mysql_data: null
  airflow-pgdata: null
  ingestion_data: null
  elasticsearch_data: null
services:
  consul:
    image: hashicorp/consul:1.18
    profiles:
    - consul
    container_name: consul
    command:
    - agent
    - -server
    - -bootstrap-expect=1
    - -client=0.0.0.0
    - -ui
    - -log-level=INFO
    - -config-dir=/consul/config
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    ports:
    - 8500:8500
    - 8600:8600/tcp
    - 8600:8600/udp
    volumes:
    - ./config_files/consul_data:/consul/data
    - ./config_files/consul/config:/consul/config:rw
    - ./config_files/consul/jwt:/consul/jwt:rw
    networks:
    - osss-net
    environment:
      CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN:-}
    healthcheck:
      test: consul info >/dev/null 2>&1 || exit 1
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped
  consul-jwt-init:
    image: hashicorp/consul:1.18
    profiles:
    - consul
    container_name: consul-jwt-init
    networks:
    - osss-net
    depends_on:
    - consul
    environment:
      CONSUL_HTTP_ADDR: http://consul:8500
      CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN}
    volumes:
    - ./config_files/consul/jwt/jwt.json:/cfg/jwt.json:ro
    - ./config_files/consul/init-jwt.sh:/cfg/init-jwt.sh:ro
    entrypoint:
    - /bin/sh
    - -exc
    command:
    - /cfg/init-jwt.sh
    restart: 'no'
  app:
    profiles:
    - app
    networks:
    - osss-net
    container_name: app
    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2g
    build:
      context: .
      dockerfile: docker/app/Dockerfile
    command:
    - uvicorn
    - src.OSSS.main:app
    - --host
    - 0.0.0.0
    - --port
    - '8000'
    - --reload
    - --log-level
    - info
    - --access-log
    - --log-config
    - /workspace/docker/logging.yaml
    working_dir: /workspace
    ports:
    - 127.0.0.1:8081:8000
    entrypoint:
    - /usr/local/bin/app-entrypoint.sh
    volumes:
    - ./:/workspace:cached
    - ./docker/logging.yml:/workspace/docker/logging.yaml:ro
    - ./scripts/app-entrypoint.sh:/usr/local/bin/app-entrypoint.sh:ro

    labels:
      co.elastic.logs/enabled: 'true'
      co.elastic.logs/processors.1.decode_json_fields.fields: message
      co.elastic.logs/processors.1.decode_json_fields.target: ''
      co.elastic.logs/processors.1.decode_json_fields.overwrite_keys: 'true'
      co.elastic.logs/processors.2.add_fields.target: app
      co.elastic.logs/processors.2.add_fields.fields.service: osss-api
    environment:
      OSSS_VERBOSE_AUTH: '1'
      PYTHONUNBUFFERED: '1'
      PYTHONLOGLEVEL: DEBUG
      LOG_LEVEL: DEBUG
      UVICORN_LOG_LEVEL: debug
      UVICORN_ACCESS_LOG: '1'
      KEYCLOAK_ISSUER: ${KEYCLOAK_ISSUER}
      KEYCLOAK_JWKS_URL: ${KEYCLOAK_JWKS_URL}
      AUTHLIB_DEBUG: '1'
      OAUTHLIB_INSECURE_TRANSPORT: '1'
      HTTPX_LOG_LEVEL: DEBUG
      REQUESTS_LOG_LEVEL: DEBUG
      JOSE_LOG_LEVEL: DEBUG
      JWcrypto_LOG_LEVEL: DEBUG
      HOST: 0.0.0.0
      PORT: '8000'
      PYTHONPATH: /workspace/src
      WATCHFILES_FORCE_POLLING: 'true'
      CORS_ALLOW_ORIGINS: ${CORS_ALLOW_ORIGINS}
      CORS_ORIGINS: ${CORS_ORIGINS}

      # Using the baked-in system trust from your Dockerfile:
      REQUESTS_CA_BUNDLE: "/etc/ssl/certs/ca-certificates.crt"
      SSL_CERT_FILE: "/etc/ssl/certs/ca-certificates.crt"
      CURL_CA_BUNDLE: "/etc/ssl/certs/ca-certificates.crt"

      OIDC_DISCOVERY_URL_INTERNAL: "${OIDC_DISCOVERY_URL_INTERNAL}"
      OIDC_TOKEN_URL_INTERNAL: "${OIDC_TOKEN_URL_INTERNAL}"
      OIDC_JWKS_URL_INTERNAL: "${OIDC_JWKS_URL_INTERNAL}"
      KEYCLOAK_INTERNAL_BASE: "${KEYCLOAK_INTERNAL_BASE}"

      OIDC_ISSUER: ${OIDC_ISSUER}
      OIDC_CLIENT_ID: osss-api
      OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET:-password}
      OSSS_PUBLIC_BASE_URL: http://localhost:8081
      OIDC_REDIRECT_URL: http://localhost:8081/callback
      OIDC_LOGOUT_REDIRECT_URL: http://localhost:8081/
      OIDC_VERIFY_AUD: '0'
      ALLOWED_CLOCK_SKEW: '60'
      REDIS_URL: redis://redis:6379/0
      SESSION_REDIS_HOST: redis
      SESSION_REDIS_PORT: '6379'
      KEYCLOAK_CLIENT_ID: osss-api
      KEYCLOAK_CLIENT_SECRET: password
      ASYNC_DATABASE_URL: postgresql+asyncpg://${OSSS_DB_USER}:${OSSS_DB_PASSWORD}@osss_postgres:5432/${OSSS_DB_NAME}
      ALEMBIC_DATABASE_URL: postgresql+asyncpg://${OSSS_DB_USER}:${OSSS_DB_PASSWORD}@osss_postgres:5432/${OSSS_DB_NAME}
      OIDC_JWKS_URL_INTERNAL: ${OIDC_JWKS_URL_INTERNAL}
      OIDC_VERIFY_ISS: ${OIDC_VERIFY_ISS}
      MIGRATIONS_DIR: /app/src/OSSS/db/migrations
      REPO_ROOT: /app
      ALEMBIC_CMD: alembic
      ALEMBIC_INI: /app/alembic.ini
      OSSS_DB_PASSWORD: ${OSSS_DB_PASSWORD}
      OSSS_DB_NAME: ${OSSS_DB_NAME}
      OSSS_DB_USER: ${OSSS_DB_USER}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      DATABASE_URL: ${ASYNC_DATABASE_URL}
    depends_on:
      redis:
        condition: service_healthy
      osss_postgres:
        condition: service_started
    healthcheck:
      test: curl -fsS http://localhost:8000/healthz >/dev/null || exit 1
      interval: 5s
      timeout: 3s
      retries: 90
      start_period: 20s
  web:
    profiles:
    - web-app
    networks:
    - osss-net
    container_name: web
    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    build:
      context: ./src/osss-web
      dockerfile: ../../docker/osss-web/Dockerfile
    command: npm run dev -- --port 3000
    depends_on:
      app:
        condition: service_healthy
    environment:
      NODE_ENV: development
      CHOKIDAR_USEPOLLING: 'true'
      WATCHPACK_POLLING: 'true'
      OSSS_API_URL: ${OSSS_API_URL}
    volumes:
    - ./src/osss-web:/app:cached
    - /app/node_modules
    working_dir: /app
    ports:
    - 3000:3000
    labels:
      co.elastic.logs/enabled: 'true'
      co.elastic.logs/processors.1.decode_json_fields.fields: message
      co.elastic.logs/processors.1.decode_json_fields.target: ''
      co.elastic.logs/processors.1.decode_json_fields.overwrite_keys: 'true'
      co.elastic.logs/processors.2.add_fields.target: app
      co.elastic.logs/processors.2.add_fields.fields.service: osss-web
  osss_postgres:
    image: postgres:16-alpine
    profiles:
    - app
    container_name: osss_postgres
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      OSSS_DB_USER: ${OSSS_DB_USER}
      OSSS_DB_PASSWORD: ${OSSS_DB_PASSWORD}
      OSSS_DB_NAME: ${OSSS_DB_NAME}
      POSTGRES_INITDB_ARGS: ${POSTGRES_INITDB_ARGS}
    networks:
    - osss-net
    ports:
    - 5433:5432
    volumes:
    - osss_postgres_data:/var/lib/postgresql/data
    - ./scripts/init-osss.sh:/docker-entrypoint-initdb.d/20-init-osss.sh:ro
    healthcheck:
      test: pg_isready -U "$${POSTGRES_USER}" -d "$${POSTGRES_DB}" -h 127.0.0.1 -p
        5432 || exit 1
      interval: 5s
      timeout: 5s
      retries: 20
  redis:
    image: redis:7-alpine
    profiles:
    - app
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    container_name: redis
    command:
    - redis-server
    - --appendonly
    - 'yes'
    ports:
    - 6379:6379
    networks:
    - osss-net
    volumes:
    - redis-data:/data
    healthcheck:
      test:
      - CMD
      - redis-cli
      - ping
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped
  kc_postgres:
    image: postgres:16
    profiles:
    - keycloak
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    container_name: kc_postgres
    environment:
      POSTGRES_DB: ${KC_DB_NAME}
      POSTGRES_USER: ${KC_DB_USERNAME}
      POSTGRES_PASSWORD: ${KC_DB_PASSWORD}
    networks:
    - osss-net
    volumes:
    - kc_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: pg_isready -U ${KC_DB_USERNAME:-keycloak} -d ${KC_DB_NAME:-keycloak} -h
        127.0.0.1 -p 5432 || exit 1
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: always
  keycloak:
    build:
      context: .
      dockerfile: docker/keycloak/Dockerfile
    profiles:
    - keycloak
    container_name: keycloak
    networks:
      osss-net:
        aliases:
        - keycloak.local
    environment:
      KC_HTTP_PORT: 8080
      KC_DB: ${KC_DB}
      KC_DB_URL: jdbc:postgresql://${KC_DB_HOST}:${KC_DB_PORT}/${KC_DB_NAME}
      KC_DB_USERNAME: ${KC_DB_USERNAME}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD}
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/conf/tls/server.crt
      KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/conf/tls/server.key
      KC_HEALTH_ENABLED: 'true'
      KC_HOSTNAME: ${KC_HOSTNAME}
      KC_DB_SCHEMA: ${KC_DB_SCHEMA}
      QUARKUS_HIBERNATE_ORM_PERSISTENCE_XML_IGNORE: 'true'
      JAVA_OPTS: ${JAVA_OPTS}
      KC_DB_POOL_INITIAL_SIZE: '20'
      KC_DB_POOL_MIN_SIZE: '20'
      KC_DB_POOL_MAX_SIZE: '50'
      KC_LOG_LEVEL: ${KC_LOG_LEVEL}
      KC_PROXY: ${KC_PROXY}
      KC_HTTP_ENABLED: ${KC_HTTP_ENABLED}
      KC_HOSTNAME_STRICT: ${KC_HOSTNAME_STRICT}
      ADMIN_USER: ${KEYCLOAK_ADMIN}
      ADMIN_PWD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_URL: ${KC_URL}

    volumes:
    - ./realm-export.json:/opt/keycloak/data/import/10-OSSS.json:ro
    - ./docker/keycloak/quarkus.properties:/opt/keycloak/conf/quarkus.properties:ro
    - ./config_files/keycloak/secrets/keycloak:/opt/keycloak/conf/tls:ro

    ports:
    - 8443:8443
    - 8080:8080
    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    depends_on:
      kc_postgres:
        condition: service_healthy
    restart: always
    healthcheck:
      test:
      - CMD
      - /opt/keycloak/healthcheck.sh
      interval: 10s
      timeout: 10s
      retries: 190
  vault:
    image: hashicorp/vault:1.20.3
    container_name: vault
    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 512m
    profiles:
    - vault
    pull_policy: always
    ports:
    - 8200:8200
    cap_add:
    - IPC_LOCK
    networks:
    - osss-net
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_DEV_ROOT_TOKEN_ID}
      VAULT_DEV_LISTEN_ADDRESS: ${VAULT_DEV_LISTEN_ADDRESS}
      VAULT_UI: ${VAULT_UI}
      VAULT_API_ADDR: ${VAULT_API_ADDR}
    depends_on:
      keycloak:
        condition: service_healthy
    command:
    - server
    - -dev
    - -dev-root-token-id=root
    - -dev-listen-address=0.0.0.0:8200
    healthcheck:
      test: wget -qO- http://127.0.0.1:8200/v1/sys/health | grep -q '"initialized":true'
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: unless-stopped
  vault-oidc-setup:
    profiles:
    - vault
    container_name: vault-oidc-setup
    build:
      context: .
      dockerfile: docker/vault-oidc-setup/Dockerfile
    networks:
    - osss-net
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    depends_on:
      vault:
        condition: service_healthy
      keycloak:
        condition: service_healthy
    environment:
      VERBOSE: '1'
      DEBUG: '1'
      VAULT_LOG_LEVEL: debug
      GODEBUG: http2debug=2
      VAULT_ADDR: ${VAULT_ADDR}
      VAULT_TOKEN: ${VAULT_TOKEN}
      OIDC_DISCOVERY_URL: ${OIDC_DISCOVERY_URL}
      VAULT_OIDC_DISCOVERY_URL: ${VAULT_OIDC_DISCOVERY_URL}
      VAULT_OIDC_CLIENT_ID: ${VAULT_OIDC_CLIENT_ID}
      VAULT_OIDC_CLIENT_SECRET: ${VAULT_OIDC_CLIENT_SECRET}
      VAULT_OIDC_ROLE: ${VAULT_OIDC_ROLE}
      VAULT_TOKEN_FILE: /root/.vault-token
      OIDC_ADMIN_GROUP: /vault-admin
      VAULT_UI_REDIRECT_1: http://127.0.0.1:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_UI_REDIRECT_2: http://localhost:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_UI_REDIRECT_3: http://vault:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_CLI_REDIRECT_1: http://127.0.0.1:8250/oidc/callback
      VAULT_CLI_REDIRECT_2: http://localhost:8250/oidc/callback
      VAULT_CLI_REDIRECT_3: http://vault:8250/oidc/callback
    volumes:
    - ./scripts/vault-oidc-setup.sh:/setup.sh:ro
    - ~/.vault-token:/root/.vault-token:ro
    entrypoint:
    - /bin/sh
    - -lc
    - /setup.sh
    restart: 'no'
  vault-seed:
    image: alpine:3.20
    profiles:
    - vault
    container_name: vault-seed
    env_file: .env
    networks:
    - osss-net
    cpus: 0.25
    mem_limit: 256m
    mem_reservation: 128m
    environment:
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_TOKEN:-root}
      VAULT_KV_PATH: ${VAULT_KV_PATH:-app}
      SEED_VAULT_TOKEN: ${VAULT_TOKEN:-root}
      VERBOSE: '1'
      DEBUG: '1'
    depends_on:
      vault:
        condition: service_healthy
    volumes:
    - ./scripts/seed-vault.sh:/usr/local/bin/seed-vault:ro
    entrypoint:
    - /bin/sh
    - -lc
    - /usr/local/bin/seed-vault
    restart: 'no'
  shared-vol-init:
    image: alpine:3.20
    user: 0:0
    volumes:
    - es-shared:/shared
    container_name: shared-vol-init
    cpus: 0.1
    mem_limit: 128m
    mem_reservation: 64m
    entrypoint:
    - sh
    - -lc
    command: 'set -e

      mkdir -p /shared

      chmod 0777 /shared         # or 0770 with a shared group if you prefer

      '
    profiles:
    - elastic
    networks:
    - osss-net
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    profiles:
    - elastic
    container_name: elasticsearch
    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    environment:
    - # put the secret in your .env instead of hardcoding here
    - OIDC_CLIENT_SECRET=${KIBANA_OIDC_CLIENT_SECRET}
    - discovery.type=single-node
    - xpack.security.enabled=true
    - xpack.security.http.ssl.enabled=false
    - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    - ES_JAVA_OPTS=-Xms512m -Xmx512m
    - network.host=0.0.0.0
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
    - 9200:9200
    - 9300:9300
    networks:
    - osss-net
    command: [ "/bin/bash","-lc","set -euo pipefail; \
        [ -f config/elasticsearch.keystore ] || bin/elasticsearch-keystore create; \
        if ! bin/elasticsearch-keystore list | grep -qx 'xpack.security.authc.realms.oidc.oidc1.rp.client_secret'; then \
          echo \"$OIDC_CLIENT_SECRET\" | bin/elasticsearch-keystore add -xf xpack.security.authc.realms.oidc.oidc1.rp.client_secret; \
        fi; \
        exec /usr/local/bin/docker-entrypoint.sh eswrapper" ]
    volumes:
    - es-data:/usr/share/elasticsearch/data
    - ./config_files/elastic/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    - ./config_files/keycloak/secrets/ca/ca.crt:/usr/share/elasticsearch/config/keycloak-ca.crt:ro

    healthcheck:
      test:
      - CMD-SHELL
      - curl -fsS -u elastic:${ELASTIC_PASSWORD} http://elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=60s
        >/dev/null || exit 1
      interval: 30s
      timeout: 10s
      retries: 50
    restart: unless-stopped
  kibana-pass-init:
    image: curlimages/curl:8.8.0
    networks:
    - osss-net
    profiles:
    - elastic
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    container_name: kibana-pass-init
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      KIBANA_PASSWORD: ${KIBANA_PASSWORD}
      ES_URL: http://elasticsearch:9200
    entrypoint:
    - sh
    - -lc
    command:
    - "set -euo pipefail; now() { date -Iseconds; }; log() { printf '%s %s\\n' \"\
      $$(now)\" \"$$*\"; }; mask() { s=\"$$1\"; [ -z \"$$s\" ] && printf '(empty)\\\
      n' || { [ \"$${#s}\" -le 8 ] && printf '******\\n' || printf '%s******\\n' \"\
      $${s%??????}\"; }; }; log \"ES_URL=$$ES_URL\"; log \"ELASTIC_PASSWORD=$$(mask\
      \ \"$$ELASTIC_PASSWORD\")\"; log \"KIBANA_PASSWORD=$$(mask \"$$KIBANA_PASSWORD\"\
      )\"; log \"Waiting for Elasticsearch cluster health...\"; __tries=0; while :;\
      \ do\n  __code=\"$$(curl -sS -o /dev/null -w '%{http_code}' -u \"elastic:$$ELASTIC_PASSWORD\"\
      \ \"$$ES_URL/_cluster/health\" || echo 000)\";\n  log \"cluster health http_code=$$__code\"\
      ;\n  [ \"$$__code\" = \"200\" ] && break;\n  __tries=$$((__tries+1)); [ \"$$__tries\"\
      \ -le 180 ] || { log \"Elasticsearch not ready after 180 attempts\"; exit 1;\
      \ };\n  sleep 3;\ndone; log \"Elasticsearch reachable\"; log \"Setting kibana_system\
      \ password...\"; __resp=\"$$(curl -sS -u \"elastic:$$ELASTIC_PASSWORD\" -H 'Content-Type:\
      \ application/json' -w '\\nHTTP_STATUS:%{http_code}\\n' -X POST \"$$ES_URL/_security/user/kibana_system/_password\"\
      \ -d \"{\\\"password\\\":\\\"$$KIBANA_PASSWORD\\\"}\")\"; __rc=\"$$(printf '%s'\
      \ \"$$__resp\" | sed -n 's/^HTTP_STATUS://p')\"; __body=\"$$(printf '%s' \"\
      $$__resp\" | sed '$$d')\"; log \"POST /_security/user/kibana_system/_password\
      \ -> $$__rc\"; if [ -z \"$$__rc\" ] || [ \"$$__rc\" -ge 400 ]; then log \"Failed\
      \ to set kibana_system password; response follows:\"; printf '%s\\n' \"$$__body\"\
      ; exit 1; fi; log \"kibana_system password set\"; log \"Verifying kibana_system\
      \ authentication...\"; __v=\"$$(curl -sS -u \"kibana_system:$$KIBANA_PASSWORD\"\
      \ -w '\\nHTTP_STATUS:%{http_code}\\n' \"$$ES_URL/_security/_authenticate\" ||\
      \ true)\"; __v_code=\"$$(printf '%s' \"$$__v\" | sed -n 's/^HTTP_STATUS://p')\"\
      ; __v_body=\"$$(printf '%s' \"$$__v\" | sed '$$d')\"; log \"GET /_security/_authenticate\
      \ as kibana_system -> $$__v_code\"; [ \"$$__v_code\" = \"200\" ] || { printf\
      \ '%s\\n' \"$$__v_body\"; exit 1; }; log \"kibana-pass-init complete.\";\n"
    restart: 'no'

  es-rolemap-kibana-admins:
    image: curlimages/curl:8.8.0
    profiles: [ "elastic" ]
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      # Must already be set the same way you set it for Elasticsearch/Kibana
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ES_URL: "http://elasticsearch:9200"
    networks:
      - osss-net
    entrypoint:
      - sh
      - -ceu
      - |
        echo "[rolemap] Starting; ES_URL=${ES_URL}"
        deadline=$(( $(date +%s) + 300 ))  # 5 minute max wait

        # Show cluster reachability attempts (no -s)
        while :; do
          code=$(curl -u "elastic:${ELASTIC_PASSWORD}" \
                      -o /dev/null -w "%{http_code}" \
                      "${ES_URL}/_cluster/health?wait_for_status=yellow&timeout=10s" || true)
          echo "[rolemap] /_cluster/health → HTTP ${code}"
          [ "$code" = "200" ] && break
          [ $(date +%s) -ge $deadline ] && {
            echo "[rolemap] Timed out waiting for Elasticsearch to become yellow/200" >&2
            exit 1
          }
          sleep 3
        done

        echo "[rolemap] Applying kibana_admins role mapping…"
        curl --fail-with-body -v \
             -u "elastic:${ELASTIC_PASSWORD}" \
             -H 'Content-Type: application/json' \
             -X PUT "${ES_URL}/_security/role_mapping/kibana_admins" \
             -d @- <<'JSON'
        {
          "roles": ["kibana_admin"],
          "rules": { "all": [
            { "field": { "realm.name": "oidc1" }},
            { "field": { "groups": "kibana-admins" }}
          ]},
          "enabled": true
        }
        JSON

        echo "[rolemap] Done."

    restart: "no"

  es-rolemap-kibana-users:
    image: curlimages/curl:8.8.0
    profiles: [ "elastic" ]
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ES_URL: "http://elasticsearch:9200"
    networks: [ osss-net ]
    entrypoint:
      - sh
      - -ceu
      - |
        echo "[rolemap-users] Starting; ES_URL=${ES_URL}"
        deadline=$(( $(date +%s) + 300 ))  # max 5 minutes

        # Wait for ES to be reachable (show each attempt)
        while :; do
          code=$(curl -u "elastic:${ELASTIC_PASSWORD}" \
                      -o /dev/null -w "%{http_code}" \
                      "${ES_URL}/_cluster/health?wait_for_status=yellow&timeout=10s" || true)
          echo "[rolemap-users] /_cluster/health → HTTP ${code}"
          [ "$code" = "200" ] && break
          [ "$(date +%s)" -ge "$deadline" ] && {
            echo "[rolemap-users] Timed out waiting for Elasticsearch to become yellow/200" >&2
            exit 1
          }
          sleep 3
        done

        echo "[rolemap-users] Applying kibana_users role mapping…"
        curl --fail-with-body -v \
             -u "elastic:${ELASTIC_PASSWORD}" \
             -H 'Content-Type: application/json' \
             -X PUT "${ES_URL}/_security/role_mapping/kibana_users" \
             -d @- <<'JSON'
        {
          "roles": ["kibana_user"],
          "rules": { "all": [
            { "field": { "realm.name": "oidc1" }},
            { "field": { "groups": "kibana-users" }}
          ]},
          "enabled": true
        }
        JSON

        echo "[rolemap-users] Verifying role mapping…"
        curl -u "elastic:${ELASTIC_PASSWORD}" \
             -sS "${ES_URL}/_security/role_mapping/kibana_users"
        echo
        echo "[rolemap-users] Done."

    restart: "no"


  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    profiles:
    - elastic
    container_name: kibana
    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 512m
    volumes:
    - ./config_files/elastic/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana-pass-init:
        condition: service_completed_successfully
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      SERVER_PUBLICBASEURL: http://localhost:5601
      LOGGING_VERBOSE: 'true'
      ELASTICSEARCH_USERNAME: kibana_system
      ELASTICSEARCH_PASSWORD: ${KIBANA_PASSWORD}
    ports:
    - 5601:5601
    networks:
    - osss-net
    restart: unless-stopped
    healthcheck:
      test: curl -fsS -I http://localhost:5601/login | grep -q '200'
      interval: 20s
      timeout: 5s
      retries: 30
  api-key-init:
    image: curlimages/curl:8.8.0
    networks:
    - osss-net
    profiles:
    - elastic
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    container_name: api-key-init
    user: 0:0
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ES_URL: http://elasticsearch:9200
    depends_on:
      shared-vol-init:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_healthy
    volumes:
    - ./:/work
    - es-shared:/shared
    working_dir: /work
    entrypoint:
    - sh
    - -lc
    command:
    - "set -eu\n\n# Wait for ES\necho \"[wait] for Elasticsearch...\"\n__n=0\nwhile\
      \ :; do\n  __code=\"$$(curl -sS -o /dev/null -w '%{http_code}' \"$$ES_URL\"\
      \ || echo 000)\"\n  case \"$$__code\" in 200|401|302) break ;; esac\n  __n=$$((__n+1));\
      \ [ \"$$__n\" -le 180 ] || { echo \"ES not reachable (code=$$__code)\" >&2;\
      \ exit 1; }\n  sleep 5\ndone\n\necho \"[mint] API key for filebeat writer...\"\
      \nbody='{\n  \"name\": \"filebeat_osss_ingest\",\n  \"role_descriptors\": {\n\
      \    \"filebeat_writer\": {\n      \"cluster\": [\"monitor\",\"read_ilm\",\"\
      read_pipeline\"],\n      \"index\": [\n        { \"names\": [\"logs-*\",\"filebeat-*\"\
      ], \"privileges\": [\"auto_configure\",\"create_doc\",\"view_index_metadata\"\
      ] }\n      ]\n    }\n  }\n}'\n\nresp=$$(curl -fsS -u \"elastic:$$ELASTIC_PASSWORD\"\
      \ \\\n         -H \"Content-Type: application/json\" \\\n         -d \"$$body\"\
      \ \"$$ES_URL/_security/api_key\")\n\nid=$$(printf '%s' \"$$resp\" | sed -n 's/.*\"\
      id\"[[:space:]]*:[[:space:]]*\"\\([^\"]*\\)\".*/\\1/p')\nkey=$$(printf '%s'\
      \ \"$$resp\" | sed -n 's/.*\"api_key\"[[:space:]]*:[[:space:]]*\"\\([^\"]*\\\
      )\".*/\\1/p')\n[ -n \"$$id\" ] && [ -n \"$$key\" ] || { echo \"ERROR: could\
      \ not parse API key: $$resp\" >&2; exit 1; }\n\necho \"ELASTIC_API_KEY=$$id:$$key\"\
      \ > .env.apikey\necho \"[ok] wrote .env.apikey\"\n\necho \"ELASTIC_API_KEY=$$id:$$key\"\
      \ > /shared/filebeat.apikey.env\nchmod 600 /shared/filebeat.apikey.env\necho\
      \ \"[ok] wrote /shared/filebeat.apikey.env\"\n"
  filebeat-setup:
    image: docker.elastic.co/beats/filebeat:8.14.3
    networks:
    - osss-net
    profiles:
    - elastic
    container_name: filebeat-setup
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    env_file:
    - .env
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
      api-key-init:
        condition: service_completed_successfully
    volumes:
    - ./config_files/filebeat/filebeat.setup.yml:/usr/share/filebeat/filebeat.yml:ro
    entrypoint:
    - sh
    - -lc
    command:
    - "set -eu\necho \"[wait] for Kibana...\"\ni=0\nuntil code=\"$$(curl -sS -o /dev/null\
      \ -w '%{http_code}' \"$$KIBANA_URL/api/status\")\" \\\n  && { [ \"$$code\" =\
      \ \"200\" ] || [ \"$$code\" = \"302\" ] || [ \"$$code\" = \"401\" ]; }; do\n\
      \  i=$$((i+1)); [ $$i -le 180 ] || { echo \"Kibana not reachable (last http_code=$$code)\"\
      ; exit 1; }\n  sleep 1\ndone\n\necho \"[setup] ILM/templates/dashboards\"\n\
      filebeat setup \\\n  -E setup.ilm.overwrite=true \\\n  -E setup.dashboards.enabled=true\
      \ \\\n  -E setup.kibana.hosts=[\"$$KIBANA_URL\"] \\\n  -E setup.kibana.username=\"\
      $$KIBANA_USERNAME\" \\\n  -E setup.kibana.password=\"$$KIBANA_PASSWORD\" \\\n\
      \  -E output.elasticsearch.hosts=[\"$$ES_URL\"] \\\n  -E output.elasticsearch.username=\"\
      elastic\" \\\n  -E output.elasticsearch.password=\"$$ELASTIC_PASSWORD\"\n"
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.14.3
    container_name: filebeat-podman
    networks:
    - osss-net
    profiles:
    - elastic
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    env_file:
    - .env
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      api-key-init:
        condition: service_completed_successfully
      filebeat-setup:
        condition: service_completed_successfully
    volumes:
    - ./config_files/filebeat/filebeat.podman.yml:/usr/share/filebeat/filebeat.yml:ro
    - /var/lib/containers/storage/overlay-containers:/var/lib/containers/storage/overlay-containers:ro
    - es-shared:/shared
    command:
    - sh
    - -c
    - "set -eu\necho \"[wait] for /shared/filebeat.apikey.env...\"\ni=0\nwhile [ $$i\
      \ -lt 180 ]; do\n  [ -s /shared/filebeat.apikey.env ] && break\n  i=$$((i+1))\n\
      \  sleep 1\ndone\n[ -s /shared/filebeat.apikey.env ] || { echo \"missing API\
      \ key file\"; exit 1; }\nset -a\n. /shared/filebeat.apikey.env\necho \"[ok]\
      \ ELASTIC_API_KEY loaded (id=$${ELASTIC_API_KEY%%:*})\"\nexec filebeat -e --strict.perms=false\n"
  trino:
    image: trinodb/trino:443
    container_name: trino
    profiles:
    - trino
    user: 1000:1000
    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2560m
    restart: unless-stopped
    ports:
    - 8444:8443
    environment:
      KEYCLOAK_URL: https://keycloak:8443
      JAVA_TOOL_OPTIONS: -Djavax.net.ssl.trustStore=/etc/trino/keystore/trino-keystore.p12
        -Djavax.net.ssl.trustStorePassword=changeit -Djavax.net.ssl.trustStoreType=PKCS12
    volumes:
    - ./config_files/trino_data:/var/trino
    - ./config_files/trino/etc:/etc/trino:ro
    - ./config_files/trino/etc/keystore:/etc/trino/keystore:ro
    networks:
    - osss-net
  superset_redis:
    image: redis:7-alpine
    profiles:
    - superset
    container_name: superset_redis
    restart: unless-stopped
    command:
    - redis-server
    - --appendonly
    - 'yes'
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    volumes:
    - superset_redis_data:/data
    ports:
    - 6380:6379
    networks:
    - osss-net
  postgres-superset:
    image: postgres:16
    profiles:
    - superset
    container_name: postgres-superset
    restart: unless-stopped
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      POSTGRES_USER: osss
      POSTGRES_PASSWORD: osss
      POSTGRES_DB: superset
    volumes:
    - pg_superset_data:/var/lib/postgresql/data
    ports:
    - 5434:5432
    networks:
    - osss-net
  superset-init:
    image: apache/superset:latest
    profiles:
    - superset
    depends_on:
    - postgres-superset
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    networks:
    - osss-net
    environment:
      SUPERSET__SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
      SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
    entrypoint:
    - /bin/bash
    - -lc
    command: 'superset db upgrade && superset fab create-admin --username admin --firstname
      Admin --lastname User --email admin@example.com --password admin || true &&
      superset init

      '
  superset:
    image: apache/superset:latest
    container_name: superset
    profiles:
    - superset
    restart: unless-stopped
    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2g
    environment:
      SUPERSET_SECRET_KEY: please_change_me
      SUPERSET__SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
      SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
      FLASK_LIMITER_ENABLED: 'false'
    depends_on:
    - postgres-superset
    - superset_redis
    - superset-init
    ports:
    - 8088:8088
    command:
    - gunicorn
    - -w
    - '4'
    - --timeout
    - '300'
    - -b
    - 0.0.0.0:8088
    - superset.app:create_app()
    networks:
    - osss-net
  postgres-airflow:
    image: postgres:16
    profiles:
    - airflow
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
    - airflow-pgdata:/var/lib/postgresql/data
    networks:
    - osss-net
    ports:
    - 5435:5432
  airflow-init:
    image: apache/airflow:2.9.3-python3.11
    profiles:
    - airflow
    depends_on:
    - postgres-airflow
    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 768m
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
    entrypoint: /bin/bash
    command: -lc 'airflow db migrate && airflow users create \ --username admin --firstname
      Admin --lastname User --role Admin \ --email admin@example.com --password admin'
    volumes:
    - ./config_files/airflow/dags:/opt/airflow/dags
    networks:
    - osss-net
  airflow-webserver:
    image: apache/airflow:2.9.3-python3.11
    profiles:
    - airflow
    depends_on:
    - airflow-init
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      AIRFLOW__WEBSERVER__WEB_SERVER_CONFIG: /opt/airflow/webserver_config.py
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'True'
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8083
      AIRFLOW__WEBSERVER__SECRET_KEY: change-this-in-prod
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
      KEYCLOAK_URL: https://keycloak.local:8443
      KEYCLOAK_REALM: OSSS
      KEYCLOAK_AIRFLOW_CLIENT_ID: airflow
      KEYCLOAK_AIRFLOW_CLIENT_SECRET: password
    command: webserver
    ports:
    - 8083:8080
    volumes:
    - ./config_files/airflow/dags:/opt/airflow/dags
    - ./config_files/airflow/webserver_config.py:/opt/airflow/webserver_config.py:ro
    networks:
    - osss-net
  airflow-scheduler:
    image: apache/airflow:2.9.3-python3.11
    profiles:
    - airflow
    depends_on:
    - airflow-init
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      KEYCLOAK_URL: https://keycloak.local:8443
      KEYCLOAK_REALM: OSSS
    command: scheduler
    volumes:
    - ./config_files/airflow/dags:/opt/airflow/dags
    - ./config_files/airflow/webserver_config.py:/opt/airflow/webserver_config.py:ro
    networks:
    - osss-net
  openmetadata-server:
    image: openmetadata/server:1.4.0
    profiles:
    - openmetadata
    container_name: openmetadata-server
    networks:
    - osss-net
    restart: always
    ports:
    - 8585:8585
    environment:
    - DB_SCHEMAS_STORE=jdbc:mysql://mysql/openmetadata_db
    - DB_SCHEMAS_STORE_USER=openmetadata_user
    - DB_SCHEMAS_STORE_PASSWORD=openmetadata_password
    - DB_SCHEMAS_STORE_HOST=mysql
    - ELASTICSEARCH_HOST=elasticsearch
    - ELASTICSEARCH_PORT=9200
    - ELASTICSEARCH_PROTOCOL=http
    - PIPELINE_SERVICE_CLIENT_ENDPOINT=http://openmetadata-ingestion:8080
    command:
    - ./wait-for-db.sh
    - mysql
    - '3306'
    - java
    - -jar
    - /app/openmetadata.jar
    volumes:
    - ./scripts/wait-for-db.sh:/wait-for-db.sh:ro
    depends_on:
      mysql:
        condition: service_healthy
  mysql:
    image: mysql:8.0
    container_name: openmetadata-mysql
    networks:
    - osss-net
    profiles:
    - openmetadata
    restart: always
    ports:
    - 3306:3306
    environment:
    - MYSQL_ROOT_PASSWORD=openmetadata_password
    - MYSQL_DATABASE=openmetadata_db
    - MYSQL_USER=openmetadata_user
    - MYSQL_PASSWORD=openmetadata_password
    volumes:
    - mysql_data:/var/lib/mysql
    healthcheck:
      test:
      - CMD-SHELL
      - mysqladmin ping -h localhost -u root --password=openmetadata_password
      interval: 10s
      timeout: 5s
      retries: 5
  openmetadata-ingestion:
    image: openmetadata/ingestion-base:1.6.0
    container_name: openmetadata-ingestion
    networks:
    - osss-net
    profiles:
    - openmetadata
    restart: always
    environment:
    - AIRFLOW__CORE__DAGS_FOLDER=/usr/local/airflow/dags
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql://openmetadata_user:openmetadata_password@mysql:3306/openmetadata_db
    volumes:
    - ingestion_data:/usr/local/airflow
    ports:
    - 8080:8080
    command:
    - /bin/bash
    - -c
    - airflow standalone
    depends_on:
      mysql:
        condition: service_healthy
