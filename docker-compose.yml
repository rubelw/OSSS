# build once, reuse the same image name for both services
x-superset-image: &superset_image osss/superset:with-drivers

x-podman-sock: &podman_sock "${VM_PODMAN_SOCK:-/run/podman/podman.sock}"

x-kc-db-env:
  KC_DB: ${KC_DB}
  KC_DB_URL_HOST: ${KC_DB_URL_HOST}
  KC_DB_URL_PORT: ${KC_DB_URL_PORT}
  KC_DB_URL_DATABASE: ${KC_DB_NAME}
  KC_DB_USERNAME: ${KC_DB_USERNAME}
  KC_DB_PASSWORD: ${KC_DB_PASSWORD}
  KC_DB_URL: ${KC_DB_URL}
  KC_DB_URL_PROPERTIES: ${KC_DB_URL_PROPERTIES}
  KC_LOG_LEVEL: TRACE
  QUARKUS_TRANSACTION_MANAGER_DEFAULT_TRANSACTION_TIMEOUT: PT30M
  QUARKUS_DATASOURCE_JDBC_ACQUISITION_TIMEOUT: 600s
  QUARKUS_DATASOURCE_JDBC_BACKGROUND_VALIDATION_INTERVAL: 30S
  QUARKUS_DATASOURCE_JDBC_VALIDATION_QUERY: SELECT 1
  QUARKUS_LOG_LEVEL: INFO
  KC_DB_POOL_INITIAL_SIZE: '1'
  KC_DB_POOL_MIN_SIZE: '1'
  KC_DB_POOL_MAX_SIZE: '5'
  KEYCLOAK_URL: ${KEYCLOAK_URL}
  KEYCLOAK_ADMIN: admin
  KEYCLOAK_ADMIN_PASSWORD: admin
  KEYCLOAK_REALM: OSSS
  KEYCLOAK_HEALTH_URL: ${KEYCLOAK_HEALTH_URL}
  KC_FEATURES: hostname:v1
networks:
  osss-net:
    external: true
    name: osss-net
volumes:
  consul_data:
    name: consul_data
  kc_postgres_data:
    name: kc_postgres_data
  osss_postgres_data:
    name: osss_postgres_data
  tutor_postgres_data:
    name: tutor_postgres_data
  redis-data:
    name: redis-data
  es-data:
    name: es-data
  filebeat-data:
    name: filebeat-data
  filebeat-logs:
    name: filebeat-logs
  pg_superset_data:
    name: pg_superset_data
  superset_redis_data:
    name: superset_redis_data
  trino_data:
    name: trino_data
  mysql_data:
    name: mysql_data
  airflow-pgdata:
    name: airflow-pgdata
  ingestion_data:
    name: ingestion_data
  elasticsearch_data:
    name: elasticsearch_data
  ingestion-volume-dag-airflow:
    name: ingestion-volume-dag-airflow
  ingestion-volume-dags:
    name: ingestion-volume-daga
  ingestion-volume-tmp:
    name: ingestion-volume-tmp
  om-es-data:
    name: om-es-data
  web_node_modules:
    name: web_node_modules
  qdrant_data:
    name: qdrant_data
  minio_data:
    name: minio_data
  ai_pg_data:
    name: ai_pg_data
  ai_redis_data:
    name: ai_redis_data
  es-shared:
    name: es-shared
  dvc-cache:
    name: dvc-cache


services:
  consul:
    image: hashicorp/consul:1.18
    profiles:
    - consul
    container_name: consul
    command: [ "/bin/sh","-lc","update-ca-certificates || true; exec consul agent -server -bootstrap-expect=1 -client=0.0.0.0 -ui -log-level=INFO -config-dir=/consul/config" ]

    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    ports:
      - 8500:8500
      - 8600:8600/tcp
      - 8600:8600/udp
    volumes:
      - ./config_files/consul_data:/consul/data:z
      - ./config_files/consul/config:/consul/config:rw,z
      - ./config_files/consul/jwt:/consul/jwt:rw,z
      - ./config_files/keycloak/secrets/ca/ca.crt:/usr/local/share/ca-certificates/keycloak-ca.crt:ro

    networks:
      - osss-net
    environment:
      CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN:-}
    healthcheck:
      test: consul info >/dev/null 2>&1 || exit 1
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped
  consul-jwt-init:
    image: hashicorp/consul:1.18
    profiles:
      - consul
    container_name: consul-jwt-init
    networks:
      - osss-net
    depends_on:
      consul:
        condition: service_healthy
    environment:
      CONSUL_HTTP_ADDR: http://consul:8500
      CONSUL_HTTP_TOKEN: ${CONSUL_HTTP_TOKEN}
    volumes:
      - ./config_files/consul/jwt/jwt.json:/cfg/jwt.json:ro,z
      - ./config_files/consul/init-jwt.sh:/cfg/init-jwt.sh:ro,z
    entrypoint:
      - /bin/sh
      - -exc
    command:
      - /cfg/init-jwt.sh
    restart: 'no'
  app:
    profiles:
    - app
    networks:
      - osss-net
    container_name: app
    hostname: app
    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2g
    build:
      context: .
      dockerfile: docker/app/Dockerfile
    command:
      - uvicorn
      - src.OSSS.main:app
      - --host
      - 0.0.0.0
      - --port
      - '8000'
      - --reload
      - --log-level
      - info
      - --access-log
      - --log-config
      - /workspace/docker/logging.yaml
    working_dir: /workspace
    ports:
      - 127.0.0.1:8081:8000
    entrypoint:
      - /usr/local/bin/app-entrypoint.sh
    volumes:
      - ./:/workspace:cached
      - ./docker/logging.yml:/workspace/docker/logging.yaml:ro,z
      - ./scripts/app-entrypoint.sh:/usr/local/bin/app-entrypoint.sh:ro,z
    labels:
      co.elastic.logs/enabled: 'true'
      co.elastic.logs/processors.1.decode_json_fields.fields: message
      co.elastic.logs/processors.1.decode_json_fields.target: ''
      co.elastic.logs/processors.1.decode_json_fields.overwrite_keys: 'true'
      co.elastic.logs/processors.2.add_fields.target: app
      co.elastic.logs/processors.2.add_fields.fields.service: osss-api
      # if your app logs JSON:
      co.elastic.logs/json.keys_under_root: "true"
      co.elastic.logs/json.add_error_key: "true"
      co.elastic.logs/json.overwrite_keys: "true"
    environment:
      #SKIP_ALEMBIC: "1"
      OSSS_VERBOSE_AUTH: '1'
      PYTHONUNBUFFERED: '1'
      PYTHONLOGLEVEL: DEBUG
      LOG_LEVEL: DEBUG
      UVICORN_LOG_LEVEL: debug
      UVICORN_ACCESS_LOG: '1'
      KEYCLOAK_ISSUER: ${KEYCLOAK_ISSUER}
      KEYCLOAK_JWKS_URL: ${KEYCLOAK_JWKS_URL}
      AUTHLIB_DEBUG: '1'
      OAUTHLIB_INSECURE_TRANSPORT: '1'
      HTTPX_LOG_LEVEL: DEBUG
      REQUESTS_LOG_LEVEL: DEBUG
      JOSE_LOG_LEVEL: DEBUG
      JWcrypto_LOG_LEVEL: DEBUG
      HOST: 0.0.0.0
      PORT: '8000'
      PYTHONPATH: /workspace/src
      WATCHFILES_FORCE_POLLING: 'true'
      CORS_ALLOW_ORIGINS: ${CORS_ALLOW_ORIGINS}
      CORS_ORIGINS: ${CORS_ORIGINS}

      # Using the baked-in system trust from your Dockerfile:
      REQUESTS_CA_BUNDLE: "/etc/ssl/certs/ca-certificates.crt"
      SSL_CERT_FILE: "/etc/ssl/certs/ca-certificates.crt"
      CURL_CA_BUNDLE: "/etc/ssl/certs/ca-certificates.crt"

      OIDC_DISCOVERY_URL_INTERNAL: "${OIDC_DISCOVERY_URL_INTERNAL}"
      OIDC_TOKEN_URL_INTERNAL: "${OIDC_TOKEN_URL_INTERNAL}"
      KEYCLOAK_INTERNAL_BASE: "${KEYCLOAK_INTERNAL_BASE}"

      OIDC_ISSUER: ${OIDC_ISSUER}
      OIDC_CLIENT_ID: osss-api
      OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET:-password}
      OSSS_PUBLIC_BASE_URL: http://localhost:8081
      OIDC_REDIRECT_URL: http://localhost:8081/callback
      OIDC_LOGOUT_REDIRECT_URL: http://localhost:8081/
      OIDC_VERIFY_AUD: '0'
      ALLOWED_CLOCK_SKEW: '60'
      REDIS_URL: redis://redis:6379/0
      SESSION_REDIS_HOST: redis
      SESSION_REDIS_PORT: '6379'
      KEYCLOAK_CLIENT_ID: osss-api
      KEYCLOAK_CLIENT_SECRET: password
      ASYNC_DATABASE_URL: postgresql+asyncpg://${OSSS_DB_USER}:${OSSS_DB_PASSWORD}@osss_postgres:5432/${OSSS_DB_NAME}
      ALEMBIC_DATABASE_URL: postgresql+asyncpg://${OSSS_DB_USER}:${OSSS_DB_PASSWORD}@osss_postgres:5432/${OSSS_DB_NAME}
      OIDC_JWKS_URL_INTERNAL: ${OIDC_JWKS_URL_INTERNAL}
      OIDC_VERIFY_ISS: ${OIDC_VERIFY_ISS}
      MIGRATIONS_DIR: /app/src/OSSS/db/migrations
      REPO_ROOT: /app
      ALEMBIC_CMD: alembic
      ALEMBIC_INI: /app/alembic.ini
      OSSS_DB_PASSWORD: ${OSSS_DB_PASSWORD}
      OSSS_DB_NAME: ${OSSS_DB_NAME}
      OSSS_DB_USER: ${OSSS_DB_USER}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      DATABASE_URL: ${ASYNC_DATABASE_URL}
      TUTOR_CONFIG_DIR: /app/config/tutors
      OLLAMA_BASE: http://host.containers.internal:11434

      OSSS_TUTOR_DB_USER: ${OSSS_TUTOR_DB_USER}
      OSSS_TUTOR_DB_PASSWORD: ${OSSS_TUTOR_DB_PASSWORD}
      OSSS_TUTOR_DB_NAME: ${OSSS_TUTOR_DB_NAME}
      # optional if you want to be explicit:
      OSSS_TUTOR_DB_HOST: tutor-db
      OSSS_TUTOR_DB_PORT: "5432"
      OSSS_TUTOR_CONFIG_DIR: ${OSSS_TUTOR_CONFIG_DIR}
      SAFE_OPENAI_API_BASE: ${SAFE_OPENAI_API_BASE}
      OPENAI_API_BASE: ${OPENAI_API_BASE}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OSSS_ADDITIONAL_INDEX_PATH: /workspace/vector_indexes/main/embeddings.jsonl
      TUTOR_INDEX_PATH: /workspace/vector_indexes/tutor/embeddings.jsonl
      AGENT_INDEX_PATH: /workspace/vector_indexes/agent/embeddings.jsonl
      METAGPT_URL: http://metagpt:8001



    depends_on:
      redis:
        condition: service_healthy
      osss_postgres:
        condition: service_started
      tutor-db:
        condition: service_started
    healthcheck:
      test: curl -fsS http://localhost:8000/healthz >/dev/null || exit 1
      interval: 5s
      timeout: 3s
      retries: 90
      start_period: 20s
    logging:
      driver: k8s-file
      options:
        max-size: 10m
  metagpt:
    profiles:
      - app
    build:
      context: .
      dockerfile: docker/metagpt/Dockerfile
    container_name: metagpt
    hostname: metagpt
    environment:
    volumes:
      - ./src/MetaGPT:/work/src/MetaGPT
      - ./MetaGPT_workspace:/workspace
    networks:
      - osss-net
    depends_on:
      - app
    ports:
      - "8001:8001"


  tutor-db:
    profiles:
      - app
    image: pgvector/pgvector:pg16  # Postgres 16 with pgvector preinstalled
    container_name: tutor-db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
      OSSS_DB_USER: osss
      OSSS_DB_PASSWORD: password
      OSSS_DB_NAME: osss
      POSTGRES_INITDB_ARGS: ${POSTGRES_INITDB_ARGS}

    volumes:
      - tutor_postgres_data:/var/lib/postgresql/data:z

    ports: [ "5437:5432" ]
    networks: [ osss-net ]


  web:
    profiles:
      - web-app
    networks:
      - osss-net
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    container_name: web
    hostname: web
    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    build:
      context: .
      dockerfile: docker/osss-web/Dockerfile
    command: npm run dev -- --port 3000
    #depends_on:
      #app:
      #  condition: service_healthy
      #redis:
      #  condition: service_healthy
    environment:
      REDIS_URL: 'redis://redis:6379/0'
      NODE_EXTRA_CA_CERTS: '/app/certs/keycloak-ca.crt'
      NODE_ENV: development
      CHOKIDAR_USEPOLLING: 'true'
      WATCHPACK_POLLING: 'true'
      OSSS_API_URL: ${OSSS_API_URL}
    volumes:
    - ./src/osss-web:/app:cached
    - web_node_modules:/app/node_modules:z   # <â€” use the named volume
    - ./config_files/keycloak/secrets/ca/ca.crt:/app/certs/keycloak-ca.crt:ro,z
    working_dir: /app
    ports:
    - 3000:3000
    labels:
      co.elastic.logs/enabled: 'true'
      co.elastic.logs/processors.1.decode_json_fields.fields: message
      co.elastic.logs/processors.1.decode_json_fields.target: ''
      co.elastic.logs/processors.1.decode_json_fields.overwrite_keys: 'true'
      co.elastic.logs/processors.2.add_fields.target: app
      co.elastic.logs/processors.2.add_fields.fields.service: osss-web
    healthcheck:
      test:
        - CMD-SHELL
        - node -e "fetch('http://127.0.0.1:3000/').then(r=>process.exit(r.ok?0:1)).catch(()=>process.exit(1))"
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
  osss_postgres:
    image: postgres:16-alpine
    profiles:
    - app
    container_name: osss_postgres
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      OSSS_DB_USER: ${OSSS_DB_USER}
      OSSS_DB_PASSWORD: ${OSSS_DB_PASSWORD}
      OSSS_DB_NAME: ${OSSS_DB_NAME}
      POSTGRES_INITDB_ARGS: ${POSTGRES_INITDB_ARGS}

    networks:
      - osss-net
    ports:
    - 5433:5432
    volumes:
    - osss_postgres_data:/var/lib/postgresql/data:z
    - ./scripts/init-osss.sh:/docker-entrypoint-initdb.d/20-init-osss.sh:ro,z
    healthcheck:
      test: pg_isready -U "$${POSTGRES_USER}" -d "$${POSTGRES_DB}" -h 127.0.0.1 -p
        5432 || exit 1
      interval: 5s
      timeout: 5s
      retries: 20
    logging:
      driver: k8s-file
      options:
        max-size: 10m

  redis:
    image: redis:7-alpine
    profiles:
    - app
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    container_name: redis
    command:
    - redis-server
    - --appendonly
    - 'yes'
    ports:
    - 6379:6379
    networks:
      - osss-net
    volumes:
    - redis-data:/data:z
    healthcheck:
      test:
      - CMD
      - redis-cli
      - ping
      interval: 5s
      timeout: 3s
      retries: 20
    restart: unless-stopped
    logging:
      driver: k8s-file
      options:
        max-size: 10m

  kc_postgres:
    image: postgres:16
    profiles:
    - keycloak
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    container_name: kc_postgres
    environment:
      POSTGRES_DB: ${KC_DB_NAME}
      POSTGRES_USER: ${KC_DB_USERNAME}
      POSTGRES_PASSWORD: ${KC_DB_PASSWORD}
    networks:
      - osss-net
    volumes:
    - kc_postgres_data:/var/lib/postgresql/data:z
    healthcheck:
      test: pg_isready -U ${KC_DB_USERNAME:-keycloak} -d ${KC_DB_NAME:-keycloak} -h
        127.0.0.1 -p 5432 || exit 1
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: always
    logging:
      driver: k8s-file
      options:
        max-size: 10m

  keycloak:
    build:
      context: .
      dockerfile: docker/keycloak/Dockerfile
    profiles:
    - keycloak
    container_name: keycloak
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    networks:
      osss-net:
        aliases:
        - keycloak.local
    environment:
      KC_HTTP_MANAGEMENT_PORT: "9000"
      KC_HTTP_MANAGEMENT_SCHEME: "http"
      KC_HTTP_PORT: 8080
      KC_DB: ${KC_DB}
      KC_DB_URL: jdbc:postgresql://${KC_DB_HOST}:${KC_DB_PORT}/${KC_DB_NAME}
      KC_DB_USERNAME: ${KC_DB_USERNAME}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD}
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/conf/tls/server.crt
      KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/conf/tls/server.key
      KC_HEALTH_ENABLED: 'true'
      KC_HOSTNAME: ${KC_HOSTNAME}
      KC_DB_SCHEMA: ${KC_DB_SCHEMA}
      QUARKUS_HIBERNATE_ORM_PERSISTENCE_XML_IGNORE: 'true'
      JAVA_OPTS: ${JAVA_OPTS}
      KC_DB_POOL_INITIAL_SIZE: '20'
      KC_DB_POOL_MIN_SIZE: '20'
      KC_DB_POOL_MAX_SIZE: '50'
      KC_LOG_LEVEL: ${KC_LOG_LEVEL}
      KC_PROXY: ${KC_PROXY}
      KC_HTTP_ENABLED: ${KC_HTTP_ENABLED}
      KC_HOSTNAME_STRICT: ${KC_HOSTNAME_STRICT}
      ADMIN_USER: ${KEYCLOAK_ADMIN}
      ADMIN_PWD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_URL: ${KC_URL}
      # ðŸ‘‡ make management listen on all interfaces (not just loopback)
      QUARKUS_MANAGEMENT_HOST: "0.0.0.0"
      # (optional but explicit)
      QUARKUS_MANAGEMENT_ENABLED: "true"
      QUARKUS_MANAGEMENT_PORT: "9000"
      QUARKUS_HTTP_HOST: 0.0.0.0               # optional: bind to all


    volumes:
    - ./realm-export.json:/opt/keycloak/data/import/10-OSSS.json:ro,z
    - ./docker/keycloak/quarkus.properties:/opt/keycloak/conf/quarkus.properties:ro,z
    - ./config_files/keycloak/secrets/keycloak:/opt/keycloak/conf/tls:ro,z

    ports:
    - 8443:8443
    #- 8080:8080
    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    depends_on:
      kc_postgres:
        condition: service_healthy
    restart: always
    healthcheck:
      test:
        - CMD-SHELL
        - |
          set -e
          curl -fsSk "https://127.0.0.1:${KC_MANAGEMENT_HTTP_PORT:-9000}/health/ready" >/dev/null
      interval: 10s
      timeout: 10s
      retries: 600
      start_period: 300s
  vault:
    image: hashicorp/vault:1.20.3
    container_name: vault
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 512m
    profiles:
    - vault
    pull_policy: always
    ports:
    - 8200:8200
    cap_add:
    - IPC_LOCK
    networks:
      - osss-net
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_DEV_ROOT_TOKEN_ID}
      VAULT_DEV_LISTEN_ADDRESS: ${VAULT_DEV_LISTEN_ADDRESS}
      VAULT_UI: ${VAULT_UI}
      VAULT_API_ADDR: ${VAULT_API_ADDR}
    command:
    - server
    - -dev
    - -dev-root-token-id=root
    - -dev-listen-address=0.0.0.0:8200
    healthcheck:
      test: wget -qO- http://127.0.0.1:8200/v1/sys/health | grep -q '"initialized":true'
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s
    restart: unless-stopped
  vault-oidc-setup:
    profiles:
    - vault
    container_name: vault-oidc-setup
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    build:
      context: .
      dockerfile: docker/vault-oidc-setup/Dockerfile
    networks:
    - osss-net
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    depends_on:
      vault:
        condition: service_healthy
    environment:
      VERBOSE: '1'
      DEBUG: '1'
      VAULT_LOG_LEVEL: debug
      GODEBUG: http2debug=2
      VAULT_ADDR: ${VAULT_ADDR}
      VAULT_TOKEN: ${VAULT_TOKEN}
      OIDC_DISCOVERY_URL: ${OIDC_DISCOVERY_URL}
      VAULT_OIDC_DISCOVERY_URL: ${VAULT_OIDC_DISCOVERY_URL}
      VAULT_OIDC_CLIENT_ID: ${VAULT_OIDC_CLIENT_ID}
      VAULT_OIDC_CLIENT_SECRET: ${VAULT_OIDC_CLIENT_SECRET}
      VAULT_OIDC_ROLE: ${VAULT_OIDC_ROLE}
      VAULT_TOKEN_FILE: /root/.vault-token
      OIDC_ADMIN_GROUP: /vault-admin
      VAULT_UI_REDIRECT_1: http://127.0.0.1:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_UI_REDIRECT_2: http://localhost:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_UI_REDIRECT_3: http://vault:8200/ui/vault/auth/oidc/oidc/callback
      VAULT_CLI_REDIRECT_1: http://127.0.0.1:8250/oidc/callback
      VAULT_CLI_REDIRECT_2: http://localhost:8250/oidc/callback
      VAULT_CLI_REDIRECT_3: http://vault:8250/oidc/callback
      # Trust Keycloak CA inside this container:
      CURL_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt
      SSL_CERT_FILE: /etc/ssl/certs/keycloak-ca.crt
    volumes:
    - ./scripts/vault-oidc-setup.sh:/setup.sh:ro,z
    - ~/.vault-token:/root/.vault-token:ro,z
    - ./config_files/keycloak/secrets/ca/ca.crt:/etc/ssl/certs/keycloak-ca.crt:ro,z
    entrypoint:
    - /bin/sh
    - -lc
    - /setup.sh
    restart: 'no'
  vault-seed:
    image: alpine:3.20
    profiles:
    - vault
    container_name: vault-seed
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    env_file: .env
    networks:
      - osss-net
    cpus: 0.25
    mem_limit: 256m
    mem_reservation: 128m
    environment:
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_TOKEN:-root}
      VAULT_KV_PATH: ${VAULT_KV_PATH:-app}
      SEED_VAULT_TOKEN: ${VAULT_TOKEN:-root}
      VERBOSE: '1'
      DEBUG: '1'
    depends_on:
      vault:
        condition: service_healthy
    volumes:
    - ./scripts/seed-vault.sh:/usr/local/bin/seed-vault:ro,z
    entrypoint:
    - /bin/sh
    - -lc
    - /usr/local/bin/seed-vault
    restart: 'no'

  shared-vol-init:
    image: alpine:3.20
    user: 0:0
    networks:
      - osss-net
    volumes:
      - es-shared:/shared:z
    container_name: shared-vol-init
    security_opt:
      - seccomp=unconfined
      - label=disable
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    cpus: 0.1
    mem_limit: 128m
    mem_reservation: 64m
    entrypoint:
      - sh
      - -lc
    command: 'set -e

        mkdir -p /shared

        chmod 0777 /shared         # or 0770 with a shared group if you prefer

        '
    profiles:
      - elastic


  es-shared-init:
    image: alpine:3.19
    profiles: [ elastic ]
    security_opt:
      - seccomp=unconfined
      - label=disable
    networks:
      - osss-net
    command: >
      sh -lc "mkdir -p /shared/filebeat-{data,logs} &&
              chown -R 501:501 /shared/filebeat-{data,logs} &&
              chmod -R 775 /shared/filebeat-{data,logs} &&
              ls -ld /shared/filebeat-*"
    volumes:
      - es-shared:/shared:z
    user: "0:0"
    depends_on:
      shared-vol-init:
        condition: service_completed_successfully

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    profiles:
    - elastic
    container_name: elasticsearch
    hostname: elasticsearch
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    cpus: 2.0
    mem_limit: 2g
    mem_reservation: 1g
    security_opt:
      - seccomp=unconfined
      - label=disable
    environment:
      OIDC_CLIENT_SECRET: ${KIBANA_OIDC_CLIENT_SECRET}
      discovery.type: single-node
      xpack.security.enabled: "true"
      xpack.security.http.ssl.enabled: "false"
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      network.host: "0.0.0.0"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
    - 9200:9200
    - 9300:9300
    networks:
      - osss-net
    command: [ "/bin/bash","-lc","set -euo pipefail; \
        [ -f config/elasticsearch.keystore ] || bin/elasticsearch-keystore create; \
        if ! bin/elasticsearch-keystore list | grep -qx 'xpack.security.authc.realms.oidc.oidc1.rp.client_secret'; then \
          echo \"$OIDC_CLIENT_SECRET\" | bin/elasticsearch-keystore add -xf xpack.security.authc.realms.oidc.oidc1.rp.client_secret; \
        fi; \
        exec /usr/local/bin/docker-entrypoint.sh eswrapper" ]
    volumes:
    - es-data:/usr/share/elasticsearch/data:z
    - ./config_files/elastic/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro,z
    - ./config_files/keycloak/secrets/ca/ca.crt:/usr/share/elasticsearch/config/keycloak-ca.crt:ro,z

    healthcheck:
      test:
      - CMD-SHELL
      - curl -fsS -u elastic:${ELASTIC_PASSWORD} http://elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=60s
        >/dev/null || exit 1
      interval: 30s
      timeout: 10s
      retries: 50
    restart: unless-stopped
  kibana-pass-init:
    image: curlimages/curl:8.8.0
    networks:
      - osss-net
    profiles:
    - elastic
    cpus: 0.5
    mem_limit: 512m
    security_opt:
      - seccomp=unconfined
      - label=disable
    mem_reservation: 256m
    container_name: kibana-pass-init
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      KIBANA_PASSWORD: ${KIBANA_PASSWORD}
      ES_URL: http://elasticsearch:9200
    entrypoint:
    - sh
    - -lc
    command:
    - "set -euo pipefail; now() { date -Iseconds; }; log() { printf '%s %s\\n' \"\
      $$(now)\" \"$$*\"; }; mask() { s=\"$$1\"; [ -z \"$$s\" ] && printf '(empty)\\\
      n' || { [ \"$${#s}\" -le 8 ] && printf '******\\n' || printf '%s******\\n' \"\
      $${s%??????}\"; }; }; log \"ES_URL=$$ES_URL\"; log \"ELASTIC_PASSWORD=$$(mask\
      \ \"$$ELASTIC_PASSWORD\")\"; log \"KIBANA_PASSWORD=$$(mask \"$$KIBANA_PASSWORD\"\
      )\"; log \"Waiting for Elasticsearch cluster health...\"; __tries=0; while :;\
      \ do\n  __code=\"$$(curl -sS -o /dev/null -w '%{http_code}' -u \"elastic:$$ELASTIC_PASSWORD\"\
      \ \"$$ES_URL/_cluster/health\" || echo 000)\";\n  log \"cluster health http_code=$$__code\"\
      ;\n  [ \"$$__code\" = \"200\" ] && break;\n  __tries=$$((__tries+1)); [ \"$$__tries\"\
      \ -le 180 ] || { log \"Elasticsearch not ready after 180 attempts\"; exit 1;\
      \ };\n  sleep 3;\ndone; log \"Elasticsearch reachable\"; log \"Setting kibana_system\
      \ password...\"; __resp=\"$$(curl -sS -u \"elastic:$$ELASTIC_PASSWORD\" -H 'Content-Type:\
      \ application/json' -w '\\nHTTP_STATUS:%{http_code}\\n' -X POST \"$$ES_URL/_security/user/kibana_system/_password\"\
      \ -d \"{\\\"password\\\":\\\"$$KIBANA_PASSWORD\\\"}\")\"; __rc=\"$$(printf '%s'\
      \ \"$$__resp\" | sed -n 's/^HTTP_STATUS://p')\"; __body=\"$$(printf '%s' \"\
      $$__resp\" | sed '$$d')\"; log \"POST /_security/user/kibana_system/_password\
      \ -> $$__rc\"; if [ -z \"$$__rc\" ] || [ \"$$__rc\" -ge 400 ]; then log \"Failed\
      \ to set kibana_system password; response follows:\"; printf '%s\\n' \"$$__body\"\
      ; exit 1; fi; log \"kibana_system password set\"; log \"Verifying kibana_system\
      \ authentication...\"; __v=\"$$(curl -sS -u \"kibana_system:$$KIBANA_PASSWORD\"\
      \ -w '\\nHTTP_STATUS:%{http_code}\\n' \"$$ES_URL/_security/_authenticate\" ||\
      \ true)\"; __v_code=\"$$(printf '%s' \"$$__v\" | sed -n 's/^HTTP_STATUS://p')\"\
      ; __v_body=\"$$(printf '%s' \"$$__v\" | sed '$$d')\"; log \"GET /_security/_authenticate\
      \ as kibana_system -> $$__v_code\"; [ \"$$__v_code\" = \"200\" ] || { printf\
      \ '%s\\n' \"$$__v_body\"; exit 1; }; log \"kibana-pass-init complete.\";\n"
    restart: 'no'

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    profiles:
    - elastic
    container_name: kibana
    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 512m
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    security_opt:
      - seccomp=unconfined
      - label=disable
    volumes:
    - ./config_files/elastic/kibana.yml:/usr/share/kibana/config/kibana.yml:ro,z
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana-pass-init:
        condition: service_completed_successfully
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      SERVER_PUBLICBASEURL: http://kibana:5601
      LOGGING_VERBOSE: 'true'
      ELASTICSEARCH_USERNAME: kibana_system
      ELASTICSEARCH_PASSWORD: ${KIBANA_PASSWORD}
      KBN_SERVER_PUBLICBASEURL: http://localhost:5601
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: caeb7879368e3dd66d7302f6810daec1
      XPACK_REPORTING_ENCRYPTIONKEY: c1c89f500966ac710f7fa5eaf2939976
      XPACK_SECURITY_ENCRYPTIONKEY: e1458d710ffb321e4a4f4eb792c78b2b
    ports:
    - 5601:5601
    networks:
      - osss-net
    restart: unless-stopped
    healthcheck:
      test: curl -fsS -I http://localhost:5601/login | grep -q '200'
      interval: 20s
      timeout: 5s
      retries: 30

  api-key-init:
    image: curlimages/curl:8.8.0
    container_name: api-key-init
    profiles: [ elastic ]
    networks: [ osss-net ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m
    user: "0:0"
    environment:
      ES_URL: http://elasticsearch:9200
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-password}
    depends_on:
      shared-vol-init:
        condition: service_completed_successfully
      elasticsearch:
        condition: service_healthy
    security_opt:
      - seccomp=unconfined
      - label=disable
    volumes:
      # mount the script read-only into a fixed path:
      - ./config_files/filebeat/mint_apikey.sh:/usr/local/bin/mint_apikey.sh:ro,z
      # the shared volume where the .env.apikey file will be written
      - es-shared:/shared:z
    # Use command-only (no entrypoint) and give the shell + strict flags
    command:
      - /bin/sh
      - -exc
      - |
        echo "[api-key-init] begin"
        ls -l /usr/local/bin/mint_apikey.sh || { echo "missing mint_apikey.sh" >&2; exit 1; }
        # run script explicitly; we don't need +x on the file this way
        /bin/sh /usr/local/bin/mint_apikey.sh

        echo "[verify] checking /shared/.env.apikey"
        ls -l /shared || true
        if [ -s /shared/.env.apikey ]; then
          echo "[verify] permissions/owner for /shared/.env.apikey"
          stat -c 'mode=%a owner=%U group=%G' /shared/.env.apikey || true
          echo "[verify] masked key preview"
          awk -F= '/^ELASTIC_API_KEY=/{print "ELASTIC_API_KEY=" substr($$2,1,6) "..."}' /shared/.env.apikey || true
        else
          echo "âŒ /shared/.env.apikey missing or empty" >&2
          exit 2
        fi
        echo "[api-key-init] done"
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m

  filebeat-setup:
    image: docker.elastic.co/beats/filebeat:8.14.3
    container_name: filebeat-setup
    profiles: [ elastic ]
    networks: [ osss-net ]
    user: "0:0"
    security_opt: [ seccomp=unconfined, label=disable ]
    environment:
      KIBANA_URL: "http://host.containers.internal:5601"
      ES_URL: "http://host.containers.internal:9200"
      KIBANA_USERNAME: ${KIBANA_USERNAME:-elastic}
      KIBANA_PASSWORD: ${ELASTIC_PASSWORD:-password}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD:-password}
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
      es-shared-init:
        condition: service_completed_successfully
    volumes:
      - ./config_files/filebeat/filebeat.setup.yml:/usr/share/filebeat/filebeat.yml:ro,z
      - es-shared:/shared:z
      - ./config_files/filebeat/setup.sh:/usr/local/bin/setup.sh:ro,z
      - filebeat-data:/usr/share/filebeat/data:z
      - filebeat-logs:/usr/share/filebeat/logs:z
    entrypoint: [ "sh", "-lc" ]
    command: "/usr/local/bin/setup.sh"

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.14.3
    container_name: filebeat
    user: "0:0"
    hostname: filebeat
    profiles: [ elastic ]
    security_opt: [ "label=disable" ]
    group_add: [ "0", "190" ]
    environment:
      DOCKER_HOST: "unix:///var/run/podman.sock"
      VM_PROJ: "/home/core/OSSS"
    env_file:
      - /var/lib/containers/storage/volumes/es-shared/_data/.env.apikey
    volumes:
      - es-shared:/shared:z
      - ./:/work:ro
      # host journals, read-only + rslave + selinux relabel
      - /run/systemd/journal:/run/systemd/journal:ro,rslave,z
      - /run/log/journal:/run/log/journal:ro,rslave,z
      - /var/log/journal:/var/log/journal:ro,rslave,z
      # machine-id helps the reader on some hosts
      - /etc/machine-id:/etc/machine-id:ro,z
      - /var/lib/containers:/var/lib/containers:ro,rslave,z     # Podman container logs
      - /run/podman/podman.sock:/var/run/podman.sock:ro         # Podmanâ€™s Docker-compat socket
    command:
      - filebeat
      - -e
      - -c
      - /work/config_files/filebeat/filebeat.podman.yml
      - -E
      - path.data=/shared/filebeat-data
      - -E
      - path.logs=/shared/filebeat-logs
      - -E
      - logging.level=debug
      - -E
      - logging.selectors=journald


  trino:
    image: trinodb/trino:latest
    container_name: trino
    hostname: trino
    logging:
      driver: k8s-file
      options:
        max-size: 10m
    profiles:
    - trino
    user: 1000:1000
    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2560m
    restart: unless-stopped
    ports:
    - 8444:8443
    environment:
      #KEYCLOAK_URL: https://keycloak:8443
      JAVA_TOOL_OPTIONS: >
        -Djavax.net.ssl.trustStore=/opt/trust/osss-truststore.p12
        -Djavax.net.ssl.trustStorePassword=changeit
        -Djavax.net.ssl.trustStoreType=PKCS12
    volumes:
    - ./config_files/trino_data:/var/trino:z
    - ./config_files/trino/etc:/etc/trino:ro,z
    - ./config_files/trino/opt/osss-truststore.p12:/opt/trust/osss-truststore.p12:ro,z
    networks:
    - osss-net
  superset-build:
    build:
      context: .
      dockerfile: docker/superset/Dockerfile
    image: *superset_image
    profiles: [ "superset" ]
    command: [ "true" ]   # no-op build target
  superset_redis:
    image: redis:7-alpine
    profiles:
    - superset
    container_name: superset_redis
    hostname: superset_redis
    restart: unless-stopped
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    command:
    - redis-server
    - --appendonly
    - 'yes'
    cpus: 0.5
    mem_limit: 512m
    mem_reservation: 256m
    volumes:
    - superset_redis_data:/data:z
    ports:
    - 6381:6379
    networks:
    - osss-net
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 20
  postgres-superset:
    image: postgres:16
    profiles:
    - superset
    container_name: postgres-superset
    hostname: postgres-superset
    restart: unless-stopped
    cpus: 1.0
    mem_limit: 1536m
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    mem_reservation: 1g
    environment:
      POSTGRES_USER: osss
      POSTGRES_PASSWORD: osss
      POSTGRES_DB: superset
    volumes:
    - pg_superset_data:/var/lib/postgresql/data:z
    ports:
    - 5434:5432
    networks:
    - osss-net
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U osss -d superset -h 127.0.0.1 -p 5432 || exit 1" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 10s
  superset-init:
    image: *superset_image
    container_name: superset-init
    hostname: superset-init
    profiles: [ superset ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    depends_on:
      postgres-superset:
        condition: service_healthy
    networks: [ osss-net ]
    environment:
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      PYTHONUNBUFFERED: '1'
      PYTHONPATH: /app/pythonpath:/app/superset_home/pythonpath
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/osss-dev-ca-chain.pem
      SSL_CERT_FILE: /etc/ssl/certs/osss-dev-ca-chain.pem
      OAUTHLIB_INSECURE_TRANSPORT: 0

      # --- Keycloak OIDC ---
      KEYCLOAK_CLIENT_ID: superset
      KEYCLOAK_CLIENT_SECRET: password
      KEYCLOAK_BASE_URL: https://keycloak.local:8443/realms/OSSS
      KEYCLOAK_TOKEN_URL: https://keycloak.local:8443/realms/OSSS/protocol/openid-connect/token
      KEYCLOAK_AUTH_URL: https://keycloak.local:8443/realms/OSSS/protocol/openid-connect/auth
      KEYCLOAK_REALM: OSSS
      KEYCLOAK_HOST: keycloak.local:8443

    volumes:
      - ./config_files/superset:/app/pythonpath:ro,z
      - ./config_files/keycloak/secrets/ca/ca.crt:/etc/ssl/certs/keycloak-ca.crt:ro,z
      - ./config_files/keycloak/secrets/ca/ca-chain.pem:/etc/ssl/certs/osss-dev-ca-chain.pem:ro,z

    command:
      - bash
      - -lc
      - >
        set -euo pipefail &&
        echo "[deps] installing wheels into /app/superset_home/pythonpath..." &&
        REQUESTS_CA_BUNDLE= SSL_CERT_FILE= PIP_CERT= \
          pip install --no-cache-dir --target /app/superset_home/pythonpath \
          "psycopg2-binary==2.9.*" pillow redis Authlib &&
        echo "[init] db upgrade..." && /app/.venv/bin/superset db upgrade &&
        echo "[init] create admin if missing..." && /app/.venv/bin/superset fab create-admin \
          --username admin --firstname Admin --lastname User \
          --email admin@example.com --password admin || true &&
        echo "[init] superset init..." && /app/.venv/bin/superset init &&
        echo "[init] done."

  superset:
    image: *superset_image
    container_name: superset
    hostname: superset
    profiles: [ superset ]
    restart: unless-stopped
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    cpus: 2.0
    mem_limit: 3g
    mem_reservation: 2g
    environment:
      SUPERSET_SECRET_KEY: please_change_me
      SUPERSET__SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
      SQLALCHEMY_DATABASE_URI: postgresql+psycopg2://osss:osss@postgres-superset:5432/superset
      FLASK_LIMITER_ENABLED: 'false'     # already present, keep it
      GUNICORN_CMD_ARGS: "--limit-request-field_size 65536 --limit-request-line 16384"
      RATELIMIT_STORAGE_URI: "redis://superset_redis:6379/1"   # optional: removes in-memory warning
      ENABLE_PROXY_FIX: 'true'           # optional: if behind a proxy
      PYTHONPATH: /app/pythonpath:/app/superset_home/pythonpath
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/osss-dev-ca-chain.pem
      SSL_CERT_FILE: /etc/ssl/certs/osss-dev-ca-chain.pem
      OAUTHLIB_INSECURE_TRANSPORT: 0

      # --- Keycloak OIDC ---
      KEYCLOAK_CLIENT_ID: superset
      KEYCLOAK_CLIENT_SECRET: password
      KEYCLOAK_BASE_URL: https://keycloak.local:8443/realms/OSSS
      KEYCLOAK_TOKEN_URL: https://keycloak.local:8443/realms/OSSS/protocol/openid-connect/token
      KEYCLOAK_AUTH_URL: https://keycloak.local:8443/realms/OSSS/protocol/openid-connect/auth
      KEYCLOAK_REALM: OSSS
      KEYCLOAK_HOST: keycloak.local:8443

    depends_on:
      postgres-superset:
        condition: service_healthy
      superset_redis:
        condition: service_healthy
      superset-init:
        condition: service_completed_successfully
    ports:
      - 8088:8088
    volumes:
      - ./config_files/superset:/app/pythonpath:ro
      - ./config_files/keycloak/secrets/ca/ca.crt:/etc/ssl/certs/keycloak-ca.crt:ro,z
      - ./config_files/keycloak/secrets/ca/ca-chain.pem:/etc/ssl/certs/osss-dev-ca-chain.pem:ro,z

    # superset (web)
    command:
      - bash
      - -lc
      - >
        set -euo pipefail &&
        echo "[deps] installing wheels into /app/superset_home/pythonpath..." &&
        REQUESTS_CA_BUNDLE= SSL_CERT_FILE= PIP_CERT= \
          pip install --no-cache-dir --target /app/superset_home/pythonpath \
          "psycopg2-binary==2.9.*" pillow redis Authlib &&
        exec /app/.venv/bin/gunicorn -w 4 --timeout 300 -b 0.0.0.0:8088 'superset.app:create_app()'


    networks: [ osss-net ]

  postgres-airflow:
    image: postgres:16
    container_name: postgres-airflow
    hostname: postgres-airflow
    profiles:
    - airflow
    cpus: 1.0
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
    - airflow-pgdata:/var/lib/postgresql/data:z
    networks:
    - osss-net
    ports:
    - 5435:5432
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow -d airflow -h 127.0.0.1 -p 5432 || exit 1" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 20s
  airflow-init:
    image: apache/airflow:2.9.3-python3.11
    container_name: airflow-init
    hostname: airflow-init
    profiles: [ airflow ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    depends_on:
      postgres-airflow:
        condition: service_healthy
    cpus: 1.0
    mem_limit: 1g
    mem_reservation: 768m
    environment:
      # use the same DSN you already had, but quote it to be safe
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow"
      # optional: keep init fast & quiet
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__LOGGING__LOGGING_LEVEL: "INFO"
    entrypoint: [ "/bin/bash", "-lc" ]
    command:
      - |
        set -euo pipefail
        airflow db migrate
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true
    volumes:
      - ./config_files/airflow/dags:/opt/airflow/dags:z
    networks: [ osss-net ]
  airflow-webserver:
    image: apache/airflow:2.9.3-python3.11
    container_name: airflow-webserver
    hostname: airflow-webserver
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    profiles:
    - airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-redis:
        condition: service_healthy
    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      AIRFLOW__WEBSERVER__WEB_SERVER_CONFIG: /opt/airflow/webserver_config.py
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__WEBSERVER__AUTHENTICATE: 'True'
      AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8083
      AIRFLOW__WEBSERVER__SECRET_KEY: change-this-in-prod
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'True'
      KEYCLOAK_URL: https://keycloak.local:8443
      KEYCLOAK_REALM: OSSS
      KEYCLOAK_AIRFLOW_CLIENT_ID: airflow
      KEYCLOAK_AIRFLOW_CLIENT_SECRET: password
      AIRFLOW__FAB__LIMITER_ENABLED: 'True'
      FAB_LIMITER_STORAGE_URI: redis://airflow-redis:6379/0
      RATELIMIT_STORAGE_URI: redis://airflow-redis:6379/0
      GUNICORN_CMD_ARGS: "--limit-request-field_size 65536 --limit-request-line 16384"
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt
      SSL_CERT_FILE: /etc/ssl/certs/keycloak-ca.crt
      CURL_CA_BUNDLE: /etc/ssl/certs/keycloak-ca.crt

    command: webserver
    ports:
    - 8083:8080
    volumes:
    - ./config_files/airflow/dags:/opt/airflow/dags:z
    - ./config_files/airflow/webserver_config.py:/opt/airflow/webserver_config.py:ro,z
    - ./config_files/keycloak/secrets/ca/ca.crt:/etc/ssl/certs/keycloak-ca.crt:ro,z
    - ./config_files/keycloak/secrets/ca/ca-chain.pem:/etc/ssl/certs/osss-dev-ca-chain.pem:ro,z
    networks:
    - osss-net
  airflow-scheduler:
    image: apache/airflow:2.9.3-python3.11
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    profiles:
    - airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-redis:
        condition: service_healthy

    cpus: 1.0
    mem_limit: 1536m
    mem_reservation: 1g
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      KEYCLOAK_URL: https://keycloak.local:8443
      KEYCLOAK_REALM: OSSS
    command: scheduler
    volumes:
    - ./config_files/airflow/dags:/opt/airflow/dags:z
    - ./config_files/airflow/webserver_config.py:/opt/airflow/webserver_config.py:ro,z
    networks:
    - osss-net
  airflow-redis:
    image: redis:7-alpine
    container_name: airflow-redis
    hostname: airflow-redis
    profiles: [ airflow ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    networks: [ osss-net ]
    restart: unless-stopped
    ports:
      - "6380:6379"   # external host port 6380 â†’ internal Redis port 6379
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 3s
      retries: 20

  execute-migrate-all:
    container_name: execute_migrate_all
    hostname: execute_migrate_all
    profiles: [ openmetadata ]
    image: docker.getcollate.io/openmetadata/server:1.9.12
    command: "./bootstrap/openmetadata-ops.sh migrate"
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # Migration
      MIGRATION_LIMIT_PARAM: ${MIGRATION_LIMIT_PARAM:-1200}

      # OIDC
      AUTHENTICATION_PROVIDER: ${OM_AUTHENTICATION_PROVIDER:-basic}
      CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ${CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME:-"Keycloak"}
      AUTHENTICATION_RESPONSE_TYPE: ${OM_AUTHENTICATION_RESPONSE_TYPE:-id_token}
      AUTHENTICATION_CALLBACK_URL: ${OM_AUTHENTICATION_CALLBACK_URL:-http://localhost:8585/callback}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      # --- OIDC (these are the ones pac4j needs) ---
      OIDC_DISCOVERY_URI: ${OM_OIDC_DISCOVERY_URI:-""}
      OIDC_CLIENT_ID: ${OM_OIDC_CLIENT_ID:-""}
      OIDC_CLIENT_SECRET: ${OM_OIDC_CLIENT_SECRET:-""}
      OIDC_SCOPES: "openid profile email"

      # OpenMetadata Server Authentication Configuration
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"open-metadata.org"}
      AUTHORIZER_ALLOWED_DOMAINS: ${AUTHORIZER_ALLOWED_DOMAINS:-[]}
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
      AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
      AUTHENTICATION_AUTHORITY: ${AUTHENTICATION_AUTHORITY:-https://accounts.google.com}
      AUTHENTICATION_CLIENT_ID: ${AUTHENTICATION_CLIENT_ID:-""}
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS_MAPPING: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS_MAPPING:-[]}
      AUTHENTICATION_CLIENT_TYPE: ${AUTHENTICATION_CLIENT_TYPE:-public}
      #For OIDC Authentication, when client is confidential
      OIDC_TYPE: ${OIDC_TYPE:-""} # google, azure etc.
      OIDC_SCOPE: ${OIDC_SCOPE:-"openid email profile"}
      OIDC_USE_NONCE: ${OIDC_USE_NONCE:-true}
      OIDC_PREFERRED_JWS: ${OIDC_PREFERRED_JWS:-"RS256"}
      OIDC_RESPONSE_TYPE: ${OIDC_RESPONSE_TYPE:-"code"}
      OIDC_DISABLE_PKCE: ${OIDC_DISABLE_PKCE:-true}
      OIDC_CALLBACK: ${OIDC_CALLBACK:-"http://localhost:8585/callback"}
      OIDC_SERVER_URL: ${OIDC_SERVER_URL:-"http://localhost:8585"}
      OIDC_CLIENT_AUTH_METHOD: ${OIDC_CLIENT_AUTH_METHOD:-"client_secret_post"}
      OIDC_TENANT: ${OIDC_TENANT:-""}
      OIDC_MAX_CLOCK_SKEW: ${OIDC_MAX_CLOCK_SKEW:-""}
      OIDC_CUSTOM_PARAMS: ${OIDC_CUSTOM_PARAMS:-}
      OIDC_MAX_AGE: ${OIDC_MAX_AGE:-"0"}
      OIDC_PROMPT_TYPE: ${OIDC_PROMPT_TYPE:-"consent"}
      OIDC_SESSION_EXPIRY: ${OIDC_SESSION_EXPIRY:-"604800"}
      # For SAML Authentication
      # SAML_DEBUG_MODE: ${SAML_DEBUG_MODE:-false}
      # SAML_IDP_ENTITY_ID: ${SAML_IDP_ENTITY_ID:-""}
      # SAML_IDP_SSO_LOGIN_URL: ${SAML_IDP_SSO_LOGIN_URL:-""}
      # SAML_IDP_CERTIFICATE: ${SAML_IDP_CERTIFICATE:-""}
      # SAML_AUTHORITY_URL: ${SAML_AUTHORITY_URL:-"http://localhost:8585/api/v1/saml/login"}
      # SAML_IDP_NAME_ID: ${SAML_IDP_NAME_ID:-"urn:oasis:names:tc:SAML:2.0:nameid-format:emailAddress"}
      # SAML_SP_ENTITY_ID: ${SAML_SP_ENTITY_ID:-"http://localhost:8585/api/v1/saml/metadata"}
      # SAML_SP_ACS: ${SAML_SP_ACS:-"http://localhost:8585/api/v1/saml/acs"}
      # SAML_SP_CERTIFICATE: ${SAML_SP_CERTIFICATE:-""}
      # SAML_SP_CALLBACK: ${SAML_SP_CALLBACK:-"http://localhost:8585/saml/callback"}
      # SAML_STRICT_MODE: ${SAML_STRICT_MODE:-false}
      # SAML_SP_TOKEN_VALIDITY: ${SAML_SP_TOKEN_VALIDITY:-"3600"}
      # SAML_SEND_ENCRYPTED_NAME_ID: ${SAML_SEND_ENCRYPTED_NAME_ID:-false}
      # SAML_SEND_SIGNED_AUTH_REQUEST: ${SAML_SEND_SIGNED_AUTH_REQUEST:-false}
      # SAML_SIGNED_SP_METADATA: ${SAML_SIGNED_SP_METADATA:-false}
      # SAML_WANT_MESSAGE_SIGNED: ${SAML_WANT_MESSAGE_SIGNED:-false}
      # SAML_WANT_ASSERTION_SIGNED: ${SAML_WANT_ASSERTION_SIGNED:-false}
      # SAML_WANT_ASSERTION_ENCRYPTED: ${SAML_WANT_ASSERTION_ENCRYPTED:-false}
      # SAML_WANT_NAME_ID_ENCRYPTED: ${SAML_WANT_NAME_ID_ENCRYPTED:-false}
      # SAML_KEYSTORE_FILE_PATH: ${SAML_KEYSTORE_FILE_PATH:-""}
      # SAML_KEYSTORE_ALIAS: ${SAML_KEYSTORE_ALIAS:-""}
      # SAML_KEYSTORE_PASSWORD: ${SAML_KEYSTORE_PASSWORD:-""}
      # For LDAP Authentication
      # AUTHENTICATION_LDAP_HOST: ${AUTHENTICATION_LDAP_HOST:-}
      # AUTHENTICATION_LDAP_PORT: ${AUTHENTICATION_LDAP_PORT:-}
      # AUTHENTICATION_LOOKUP_ADMIN_DN: ${AUTHENTICATION_LOOKUP_ADMIN_DN:-""}
      # AUTHENTICATION_LOOKUP_ADMIN_PWD: ${AUTHENTICATION_LOOKUP_ADMIN_PWD:-""}
      # AUTHENTICATION_USER_LOOKUP_BASEDN: ${AUTHENTICATION_USER_LOOKUP_BASEDN:-""}
      # AUTHENTICATION_USER_MAIL_ATTR: ${AUTHENTICATION_USER_MAIL_ATTR:-}
      # AUTHENTICATION_LDAP_POOL_SIZE: ${AUTHENTICATION_LDAP_POOL_SIZE:-3}
      # AUTHENTICATION_LDAP_SSL_ENABLED: ${AUTHENTICATION_LDAP_SSL_ENABLED:-}
      # AUTHENTICATION_LDAP_TRUSTSTORE_TYPE: ${AUTHENTICATION_LDAP_TRUSTSTORE_TYPE:-TrustAll}
      # AUTHENTICATION_LDAP_TRUSTSTORE_PATH: ${AUTHENTICATION_LDAP_TRUSTSTORE_PATH:-}
      # AUTHENTICATION_LDAP_KEYSTORE_PASSWORD: ${AUTHENTICATION_LDAP_KEYSTORE_PASSWORD:-}
      # AUTHENTICATION_LDAP_SSL_KEY_FORMAT: ${AUTHENTICATION_LDAP_SSL_KEY_FORMAT:-}
      # AUTHENTICATION_LDAP_ALLOW_WILDCARDS: ${AUTHENTICATION_LDAP_ALLOW_WILDCARDS:-}
      # AUTHENTICATION_LDAP_ALLOWED_HOSTNAMES: ${AUTHENTICATION_LDAP_ALLOWED_HOSTNAMES:-[]}
      # AUTHENTICATION_LDAP_SSL_VERIFY_CERT_HOST: ${AUTHENTICATION_LDAP_SSL_VERIFY_CERT_HOST:-}
      # AUTHENTICATION_LDAP_EXAMINE_VALIDITY_DATES: ${AUTHENTICATION_LDAP_EXAMINE_VALIDITY_DATES:-true}

      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      # OpenMetadata Server Pipeline Service Client Configuration
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://ingestion:8080}
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: ${PIPELINE_SERVICE_CLIENT_VERIFY_SSL:-"no-ssl"}
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ${PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH:-""}
      # Database configuration for MySQL
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata}
      # ElasticSearch Configurations
      ELASTICSEARCH_HOST: ${OM_ELASTICSEARCH_HOST:-om-elasticsearch}
      ELASTICSEARCH_PORT: ${OM_ELASTICSEARCH_PORT:-9201}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      ELASTICSEARCH_USER: ${ELASTICSEARCH_USER:-""}
      ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-""}
      SEARCH_TYPE: ${SEARCH_TYPE:- "elasticsearch"}
      ELASTICSEARCH_TRUST_STORE_PATH: ${ELASTICSEARCH_TRUST_STORE_PATH:-""}
      ELASTICSEARCH_TRUST_STORE_PASSWORD: ${ELASTICSEARCH_TRUST_STORE_PASSWORD:-""}
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: ${ELASTICSEARCH_CONNECTION_TIMEOUT_SECS:-5}
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: ${ELASTICSEARCH_SOCKET_TIMEOUT_SECS:-60}
      ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: ${ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS:-600}
      ELASTICSEARCH_BATCH_SIZE: ${ELASTICSEARCH_BATCH_SIZE:-100}
      ELASTICSEARCH_PAYLOAD_BYTES_SIZE: ${ELASTICSEARCH_PAYLOAD_BYTES_SIZE:-10485760}   #max payLoadSize in Bytes
      ELASTICSEARCH_INDEX_MAPPING_LANG: ${ELASTICSEARCH_INDEX_MAPPING_LANG:-EN}

      #eventMonitoringConfiguration
      EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
      EVENT_MONITOR_BATCH_SIZE: ${EVENT_MONITOR_BATCH_SIZE:-10}
      EVENT_MONITOR_PATH_PATTERN: ${EVENT_MONITOR_PATH_PATTERN:-["/api/v1/tables/*", "/api/v1/health-check"]}
      EVENT_MONITOR_LATENCY: ${EVENT_MONITOR_LATENCY:-[]}

      #pipelineServiceClientConfiguration
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: ${PIPELINE_SERVICE_CLIENT_CLASS_NAME:-"org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"}
      PIPELINE_SERVICE_IP_INFO_ENABLED: ${PIPELINE_SERVICE_IP_INFO_ENABLED:-false}
      PIPELINE_SERVICE_CLIENT_HOST_IP: ${PIPELINE_SERVICE_CLIENT_HOST_IP:-""}
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: ${PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER:-"noop"}
      #airflow parameters
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      AIRFLOW_TIMEOUT: ${AIRFLOW_TIMEOUT:-10}
      AIRFLOW_TRUST_STORE_PATH: ${AIRFLOW_TRUST_STORE_PATH:-""}
      AIRFLOW_TRUST_STORE_PASSWORD: ${AIRFLOW_TRUST_STORE_PASSWORD:-""}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}

      #secretsManagerConfiguration
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      # AWS:
      OM_SM_REGION: ${OM_SM_REGION:-""}
      OM_SM_ACCESS_KEY_ID: ${OM_SM_ACCESS_KEY_ID:-""}
      OM_SM_ACCESS_KEY: ${OM_SM_ACCESS_KEY:-""}
      # Azure:
      OM_SM_VAULT_NAME: ${OM_SM_VAULT_NAME:-""}
      OM_SM_CLIENT_ID: ${OM_SM_CLIENT_ID:-""}
      OM_SM_CLIENT_SECRET: ${OM_SM_CLIENT_SECRET:-""}
      OM_SM_TENANT_ID: ${OM_SM_TENANT_ID:-""}

      #email configuration:
      OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-"OpenMetadata"}
      OM_SUPPORT_URL: ${OM_SUPPORT_URL:-"https://slack.open-metadata.org"}
      AUTHORIZER_ENABLE_SMTP : ${AUTHORIZER_ENABLE_SMTP:-false}
      OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-""}
      OPENMETADATA_SMTP_SENDER_MAIL: ${OPENMETADATA_SMTP_SENDER_MAIL:-""}
      SMTP_SERVER_ENDPOINT: ${SMTP_SERVER_ENDPOINT:-""}
      SMTP_SERVER_PORT: ${SMTP_SERVER_PORT:-""}
      SMTP_SERVER_USERNAME: ${SMTP_SERVER_USERNAME:-""}
      SMTP_SERVER_PWD: ${SMTP_SERVER_PWD:-""}
      SMTP_SERVER_STRATEGY: ${SMTP_SERVER_STRATEGY:-"SMTP_TLS"}

      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      # Mask passwords values in UI
      MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}

      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-"/api"}
      #HSTS
      WEB_CONF_HSTS_ENABLED: ${WEB_CONF_HSTS_ENABLED:-false}
      WEB_CONF_HSTS_MAX_AGE: ${WEB_CONF_HSTS_MAX_AGE:-"365 days"}
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: ${WEB_CONF_HSTS_INCLUDE_SUBDOMAINS:-"true"}
      WEB_CONF_HSTS_PRELOAD: ${WEB_CONF_HSTS_PRELOAD:-"true"}
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: ${WEB_CONF_FRAME_OPTION_ENABLED:-false}
      WEB_CONF_FRAME_OPTION: ${WEB_CONF_FRAME_OPTION:-"SAMEORIGIN"}
      WEB_CONF_FRAME_ORIGIN: ${WEB_CONF_FRAME_ORIGIN:-""}
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: ${WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED:-false}
      #XSS-Protection
      WEB_CONF_XSS_PROTECTION_ENABLED: ${WEB_CONF_XSS_PROTECTION_ENABLED:-false}
      WEB_CONF_XSS_PROTECTION_ON: ${WEB_CONF_XSS_PROTECTION_ON:-true}
      WEB_CONF_XSS_PROTECTION_BLOCK: ${WEB_CONF_XSS_PROTECTION_BLOCK:-true}
      #CSP
      WEB_CONF_XSS_CSP_ENABLED: ${WEB_CONF_XSS_CSP_ENABLED:-false}
      WEB_CONF_XSS_CSP_POLICY: ${WEB_CONF_XSS_CSP_POLICY:-"default-src 'self'"}
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ${WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY:-""}
      #Referrer-Policy
      WEB_CONF_REFERRER_POLICY_ENABLED: ${WEB_CONF_REFERRER_POLICY_ENABLED:-false}
      WEB_CONF_REFERRER_POLICY_OPTION: ${WEB_CONF_REFERRER_POLICY_OPTION:-"SAME_ORIGIN"}
      #Permission-Policy
      WEB_CONF_PERMISSION_POLICY_ENABLED: ${WEB_CONF_PERMISSION_POLICY_ENABLED:-false}
      WEB_CONF_PERMISSION_POLICY_OPTION: ${WEB_CONF_PERMISSION_POLICY_OPTION:-""}
      #Cache
      WEB_CONF_CACHE_CONTROL: ${WEB_CONF_CACHE_CONTROL:-""}
      WEB_CONF_PRAGMA: ${WEB_CONF_PRAGMA:-""}

      JAVA_TOOL_OPTIONS: >-
        -Djavax.net.ssl.trustStore=/opt/om-trust/om-truststore.p12
        -Djavax.net.ssl.trustStorePassword=changeit
        -Djavax.net.ssl.trustStoreType=PKCS12

    volumes:
      - ./config_files/openmetadata/openmetadata.yaml:/usr/local/openmetadata/conf/openmetadata.yaml:ro
      - ./config_files/openmetadata/openmetadata.yaml:/openmetadata/conf/openmetadata.yaml:ro
      - ./config_files/openmetadata/certs:/opt/om-trust:ro,z
    depends_on:
      om-elasticsearch:
        condition: service_healthy
      mysql:
        condition: service_healthy
    networks:
      - osss-net

  openmetadata-server:
    container_name: openmetadata-server
    hostname: openmetadata-server
    profiles: [ openmetadata ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    restart: always
    image: docker.getcollate.io/openmetadata/server:1.9.12
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # OIDC
      AUTHENTICATION_PROVIDER: ${OM_AUTHENTICATION_PROVIDER:-basic}
      CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ${CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME:-"Keycloak"}
      AUTHENTICATION_RESPONSE_TYPE: ${OM_AUTHENTICATION_RESPONSE_TYPE:-id_token}
      AUTHENTICATION_CALLBACK_URL: ${OM_AUTHENTICATION_CALLBACK_URL:-http://localhost:8585/callback}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS_MAPPING: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS_MAPPING:-[]}
      # --- OIDC (these are the ones pac4j needs) ---
      OIDC_DISCOVERY_URI: ${OM_OIDC_DISCOVERY_URI:-""}
      OIDC_CLIENT_ID: ${OM_OIDC_CLIENT_ID:-""}
      OIDC_CLIENT_SECRET: ${OM_OIDC_CLIENT_SECRET:-""}
      OIDC_SCOPES: "openid profile email"

      # OpenMetadata Server Authentication Configuration
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"open-metadata.org"}
      AUTHORIZER_ALLOWED_DOMAINS: ${AUTHORIZER_ALLOWED_DOMAINS:-[]}
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
      AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
      AUTHENTICATION_OIDC_DISCOVERY_URI: ${OM_AUTHENTICATION_OIDC_DISCOVERY_URI:-https://keycloak.local:8443/realms/OSSS/.well-known/openid-configuration}
      AUTHENTICATION_AUTHORITY: ${OM_AUTHENTICATION_AUTHORITY:-https://accounts.google.com}
      AUTHENTICATION_CLIENT_ID: ${OM_AUTHENTICATION_CLIENT_ID:-""}
      AUTHENTICATION_CLIENT_SECRET: ${OM_AUTHENTICATION_CLIENT_SECRET:-password}
      AUTHENTICATION_SCOPE: ${OM_AUTHENTICATION_SCOPE:-"openid profile email groups"}
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS:-[email,preferred_username,sub]}
      AUTHENTICATION_CLIENT_TYPE: ${OM_AUTHENTICATION_CLIENT_TYPE:-public}
      #For OIDC Authentication, when client is confidential
      OIDC_TYPE: ${OIDC_TYPE:-""} # google, azure etc.
      OIDC_SCOPE: ${OIDC_SCOPE:-"openid email profile"}
      OIDC_USE_NONCE: ${OIDC_USE_NONCE:-true}
      OIDC_PREFERRED_JWS: ${OIDC_PREFERRED_JWS:-"RS256"}
      OIDC_RESPONSE_TYPE: ${OIDC_RESPONSE_TYPE:-"code"}
      OIDC_DISABLE_PKCE: ${OIDC_DISABLE_PKCE:-true}
      OIDC_CALLBACK: ${OIDC_CALLBACK:-"http://localhost:8585/callback"}
      OIDC_SERVER_URL: ${OIDC_SERVER_URL:-"http://localhost:8585"}
      OIDC_CLIENT_AUTH_METHOD: ${OIDC_CLIENT_AUTH_METHOD:-"client_secret_post"}
      OIDC_TENANT: ${OIDC_TENANT:-""}
      OIDC_MAX_CLOCK_SKEW: ${OIDC_MAX_CLOCK_SKEW:-""}
      OIDC_CUSTOM_PARAMS: ${OIDC_CUSTOM_PARAMS:-}
      OIDC_MAX_AGE: ${OIDC_MAX_AGE:-"0"}
      OIDC_PROMPT_TYPE: ${OIDC_PROMPT_TYPE:-"consent"}
      OIDC_SESSION_EXPIRY: ${OIDC_SESSION_EXPIRY:-"604800"}
      # For SAML Authentication
      # SAML_DEBUG_MODE: ${SAML_DEBUG_MODE:-false}
      # SAML_IDP_ENTITY_ID: ${SAML_IDP_ENTITY_ID:-""}
      # SAML_IDP_SSO_LOGIN_URL: ${SAML_IDP_SSO_LOGIN_URL:-""}
      # SAML_IDP_CERTIFICATE: ${SAML_IDP_CERTIFICATE:-""}
      # SAML_AUTHORITY_URL: ${SAML_AUTHORITY_URL:-"http://localhost:8585/api/v1/saml/login"}
      # SAML_IDP_NAME_ID: ${SAML_IDP_NAME_ID:-"urn:oasis:names:tc:SAML:2.0:nameid-format:emailAddress"}
      # SAML_SP_ENTITY_ID: ${SAML_SP_ENTITY_ID:-"http://localhost:8585/api/v1/saml/metadata"}
      # SAML_SP_ACS: ${SAML_SP_ACS:-"http://localhost:8585/api/v1/saml/acs"}
      # SAML_SP_CERTIFICATE: ${SAML_SP_CERTIFICATE:-""}
      # SAML_SP_CALLBACK: ${SAML_SP_CALLBACK:-"http://localhost:8585/saml/callback"}
      # SAML_STRICT_MODE: ${SAML_STRICT_MODE:-false}
      # SAML_SP_TOKEN_VALIDITY: ${SAML_SP_TOKEN_VALIDITY:-"3600"}
      # SAML_SEND_ENCRYPTED_NAME_ID: ${SAML_SEND_ENCRYPTED_NAME_ID:-false}
      # SAML_SEND_SIGNED_AUTH_REQUEST: ${SAML_SEND_SIGNED_AUTH_REQUEST:-false}
      # SAML_SIGNED_SP_METADATA: ${SAML_SIGNED_SP_METADATA:-false}
      # SAML_WANT_MESSAGE_SIGNED: ${SAML_WANT_MESSAGE_SIGNED:-false}
      # SAML_WANT_ASSERTION_SIGNED: ${SAML_WANT_ASSERTION_SIGNED:-false}
      # SAML_WANT_ASSERTION_ENCRYPTED: ${SAML_WANT_ASSERTION_ENCRYPTED:-false}
      # SAML_WANT_NAME_ID_ENCRYPTED: ${SAML_WANT_NAME_ID_ENCRYPTED:-false}
      # SAML_KEYSTORE_FILE_PATH: ${SAML_KEYSTORE_FILE_PATH:-""}
      # SAML_KEYSTORE_ALIAS: ${SAML_KEYSTORE_ALIAS:-""}
      # SAML_KEYSTORE_PASSWORD: ${SAML_KEYSTORE_PASSWORD:-""}
      # For LDAP Authentication
      # AUTHENTICATION_LDAP_HOST: ${AUTHENTICATION_LDAP_HOST:-}
      # AUTHENTICATION_LDAP_PORT: ${AUTHENTICATION_LDAP_PORT:-}
      # AUTHENTICATION_LOOKUP_ADMIN_DN: ${AUTHENTICATION_LOOKUP_ADMIN_DN:-""}
      # AUTHENTICATION_LOOKUP_ADMIN_PWD: ${AUTHENTICATION_LOOKUP_ADMIN_PWD:-""}
      # AUTHENTICATION_USER_LOOKUP_BASEDN: ${AUTHENTICATION_USER_LOOKUP_BASEDN:-""}
      # AUTHENTICATION_USER_MAIL_ATTR: ${AUTHENTICATION_USER_MAIL_ATTR:-}
      # AUTHENTICATION_LDAP_POOL_SIZE: ${AUTHENTICATION_LDAP_POOL_SIZE:-3}
      # AUTHENTICATION_LDAP_SSL_ENABLED: ${AUTHENTICATION_LDAP_SSL_ENABLED:-}
      # AUTHENTICATION_LDAP_TRUSTSTORE_TYPE: ${AUTHENTICATION_LDAP_TRUSTSTORE_TYPE:-TrustAll}
      # AUTHENTICATION_LDAP_TRUSTSTORE_PATH: ${AUTHENTICATION_LDAP_TRUSTSTORE_PATH:-}
      # AUTHENTICATION_LDAP_KEYSTORE_PASSWORD: ${AUTHENTICATION_LDAP_KEYSTORE_PASSWORD:-}
      # AUTHENTICATION_LDAP_SSL_KEY_FORMAT: ${AUTHENTICATION_LDAP_SSL_KEY_FORMAT:-}
      # AUTHENTICATION_LDAP_ALLOW_WILDCARDS: ${AUTHENTICATION_LDAP_ALLOW_WILDCARDS:-}
      # AUTHENTICATION_LDAP_ALLOWED_HOSTNAMES: ${AUTHENTICATION_LDAP_ALLOWED_HOSTNAMES:-[]}
      # AUTHENTICATION_LDAP_SSL_VERIFY_CERT_HOST: ${AUTHENTICATION_LDAP_SSL_VERIFY_CERT_HOST:-}
      # AUTHENTICATION_LDAP_EXAMINE_VALIDITY_DATES: ${AUTHENTICATION_LDAP_EXAMINE_VALIDITY_DATES:-true}

      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      # OpenMetadata Server Pipeline Service Client Configuration
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://ingestion:8082}
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: ${PIPELINE_SERVICE_CLIENT_VERIFY_SSL:-"no-ssl"}
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ${PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH:-""}
      # Database configuration for MySQL
      DB_DRIVER_CLASS: ${DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
      DB_SCHEME: ${DB_SCHEME:-mysql}
      DB_PARAMS: ${DB_PARAMS:-allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC}
      DB_USER: ${DB_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${DB_USER_PASSWORD:-openmetadata_password}
      DB_HOST: ${DB_HOST:-mysql}
      DB_PORT: ${DB_PORT:-3306}
      OM_DATABASE: ${OM_DATABASE:-openmetadata}
      # ElasticSearch Configurations
      ELASTICSEARCH_HOST: ${OM_ELASTICSEARCH_HOST:-om-elasticsearch}
      ELASTICSEARCH_PORT: ${OM_ELASTICSEARCH_PORT:-9201}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      ELASTICSEARCH_USER: ${ELASTICSEARCH_USER:-""}
      ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-""}
      SEARCH_TYPE: ${SEARCH_TYPE:- "elasticsearch"}
      ELASTICSEARCH_TRUST_STORE_PATH: ${ELASTICSEARCH_TRUST_STORE_PATH:-""}
      ELASTICSEARCH_TRUST_STORE_PASSWORD: ${ELASTICSEARCH_TRUST_STORE_PASSWORD:-""}
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: ${ELASTICSEARCH_CONNECTION_TIMEOUT_SECS:-5}
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: ${ELASTICSEARCH_SOCKET_TIMEOUT_SECS:-60}
      ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: ${ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS:-600}
      ELASTICSEARCH_BATCH_SIZE: ${ELASTICSEARCH_BATCH_SIZE:-100}
      ELASTICSEARCH_PAYLOAD_BYTES_SIZE: ${ELASTICSEARCH_PAYLOAD_BYTES_SIZE:-10485760}   #max payLoadSize in Bytes
      ELASTICSEARCH_INDEX_MAPPING_LANG: ${ELASTICSEARCH_INDEX_MAPPING_LANG:-EN}

      #eventMonitoringConfiguration
      EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
      EVENT_MONITOR_BATCH_SIZE: ${EVENT_MONITOR_BATCH_SIZE:-10}
      EVENT_MONITOR_PATH_PATTERN: ${EVENT_MONITOR_PATH_PATTERN:-["/api/v1/tables/*", "/api/v1/health-check"]}
      EVENT_MONITOR_LATENCY: ${EVENT_MONITOR_LATENCY:-[]}

      #pipelineServiceClientConfiguration
      PIPELINE_SERVICE_CLIENT_ENABLED: ${PIPELINE_SERVICE_CLIENT_ENABLED:-true}
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: ${PIPELINE_SERVICE_CLIENT_CLASS_NAME:-"org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"}
      PIPELINE_SERVICE_IP_INFO_ENABLED: ${PIPELINE_SERVICE_IP_INFO_ENABLED:-false}
      PIPELINE_SERVICE_CLIENT_HOST_IP: ${PIPELINE_SERVICE_CLIENT_HOST_IP:-""}
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: ${PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER:-"noop"}
      #airflow parameters
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      AIRFLOW_TIMEOUT: ${AIRFLOW_TIMEOUT:-10}
      AIRFLOW_TRUST_STORE_PATH: ${AIRFLOW_TRUST_STORE_PATH:-""}
      AIRFLOW_TRUST_STORE_PASSWORD: ${AIRFLOW_TRUST_STORE_PASSWORD:-""}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}

      #secretsManagerConfiguration
      SECRET_MANAGER: ${SECRET_MANAGER:-db}
      #parameters:
      OM_SM_REGION: ${OM_SM_REGION:-""}
      OM_SM_ACCESS_KEY_ID: ${OM_SM_ACCESS_KEY_ID:-""}
      OM_SM_ACCESS_KEY: ${OM_SM_ACCESS_KEY:-""}

      #email configuration:
      OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-"OpenMetadata"}
      OM_SUPPORT_URL: ${OM_SUPPORT_URL:-"https://slack.open-metadata.org"}
      AUTHORIZER_ENABLE_SMTP: ${AUTHORIZER_ENABLE_SMTP:-false}
      OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-""}
      OPENMETADATA_SMTP_SENDER_MAIL: ${OPENMETADATA_SMTP_SENDER_MAIL:-""}
      SMTP_SERVER_ENDPOINT: ${SMTP_SERVER_ENDPOINT:-""}
      SMTP_SERVER_PORT: ${SMTP_SERVER_PORT:-""}
      SMTP_SERVER_USERNAME: ${SMTP_SERVER_USERNAME:-""}
      SMTP_SERVER_PWD: ${SMTP_SERVER_PWD:-""}
      SMTP_SERVER_STRATEGY: ${SMTP_SERVER_STRATEGY:-"SMTP_TLS"}

      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      # Mask passwords values in UI
      MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}

      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-"/api"}
      #HSTS
      WEB_CONF_HSTS_ENABLED: ${WEB_CONF_HSTS_ENABLED:-false}
      WEB_CONF_HSTS_MAX_AGE: ${WEB_CONF_HSTS_MAX_AGE:-"365 days"}
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: ${WEB_CONF_HSTS_INCLUDE_SUBDOMAINS:-"true"}
      WEB_CONF_HSTS_PRELOAD: ${WEB_CONF_HSTS_PRELOAD:-"true"}
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: ${WEB_CONF_FRAME_OPTION_ENABLED:-false}
      WEB_CONF_FRAME_OPTION: ${WEB_CONF_FRAME_OPTION:-"SAMEORIGIN"}
      WEB_CONF_FRAME_ORIGIN: ${WEB_CONF_FRAME_ORIGIN:-""}
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: ${WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED:-false}
      #XSS-Protection
      WEB_CONF_XSS_PROTECTION_ENABLED: ${WEB_CONF_XSS_PROTECTION_ENABLED:-false}
      WEB_CONF_XSS_PROTECTION_ON: ${WEB_CONF_XSS_PROTECTION_ON:-true}
      WEB_CONF_XSS_PROTECTION_BLOCK: ${WEB_CONF_XSS_PROTECTION_BLOCK:-true}
      #CSP
      WEB_CONF_XSS_CSP_ENABLED: ${WEB_CONF_XSS_CSP_ENABLED:-false}
      WEB_CONF_XSS_CSP_POLICY: ${WEB_CONF_XSS_CSP_POLICY:-"default-src 'self'"}
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ${WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY:-""}
      #Cache
      WEB_CONF_CACHE_CONTROL: ${WEB_CONF_CACHE_CONTROL:-""}
      WEB_CONF_PRAGMA: ${WEB_CONF_PRAGMA:-""}

      JAVA_TOOL_OPTIONS: >-
        -Djavax.net.ssl.trustStore=/opt/om-trust/om-truststore.p12
        -Djavax.net.ssl.trustStorePassword=changeit
        -Djavax.net.ssl.trustStoreType=PKCS12

    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      om-elasticsearch:
        condition: service_healthy
      mysql:
        condition: service_healthy
      execute-migrate-all:
        condition: service_completed_successfully
    volumes:
      - ./config_files/openmetadata/openmetadata.yaml:/usr/local/openmetadata/conf/openmetadata.yaml:ro,z
      - ./config_files/openmetadata/certs:/opt/om-trust:ro,z

    networks:
      - osss-net
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]

  mysql:
    image: docker.getcollate.io/openmetadata/db:1.9.12
    container_name: mysql
    hostname: mysql
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    profiles: [ openmetadata ]
    networks:
      - osss-net
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=password
      - MYSQL_DATABASE=openmetadata
      - MYSQL_USER=openmetadata_user
      - MYSQL_PASSWORD=openmetadata_password
    command: "--sort_buffer_size=10M"

    volumes:
    - mysql_data:/var/lib/mysql:z
    healthcheck:
      test: mysql --user=root --password=$$MYSQL_ROOT_PASSWORD --silent --execute "use openmetadata"
      interval: 15s
      timeout: 10s
      retries: 10

  om-elasticsearch:
      profiles: [ openmetadata ]
      image: docker.elastic.co/elasticsearch/elasticsearch:8.11.4
      container_name: om-elasticsearch
      hostname: om-elasticsearch
      logging:
        driver: k8s-file
        options:
          max-size: 10m

      environment:
        - discovery.type=single-node
        - ES_JAVA_OPTS=-Xms1024m -Xmx1024m
        - xpack.security.enabled=false
      networks:
        - osss-net
      ports:
        - "9201:9200"
        - "9301:9300"
      healthcheck:
        test:
          - CMD-SHELL
          - curl -fsS -u elastic:${ELASTIC_PASSWORD} http://om-elasticsearch:9200/_cluster/health?wait_for_status=yellow&timeout=60s
            >/dev/null || exit 1
        interval: 30s
        timeout: 10s
        retries: 50
      volumes:
        - om-es-data:/usr/share/elasticsearch/data:z

  ingestion:
    container_name: openmetadata-ingestion
    hostname: openmetadata-ingestion
    profiles: [ openmetadata ]
    image: docker.getcollate.io/openmetadata/ingestion:1.9.12
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    depends_on:
      om-elasticsearch:
        condition: service_started
      mysql:
        condition: service_healthy
      openmetadata-server:
        condition: service_started
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      DB_HOST: ${AIRFLOW_DB_HOST:-mysql}
      DB_PORT: ${AIRFLOW_DB_PORT:-3306}
      AIRFLOW_DB: ${AIRFLOW_DB:-airflow_db}
      DB_SCHEME: ${AIRFLOW_DB_SCHEME:-mysql+mysqldb}
      DB_USER: ${AIRFLOW_DB_USER:-airflow_user}
      DB_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_pass}
      # extra connection-string properties for the database
      # EXAMPLE
      # require SSL (only for Postgres)
      # properties: "?sslmode=require"
      DB_PROPERTIES: ${AIRFLOW_DB_PROPERTIES:-}
      # To test the lineage backend
      # AIRFLOW__LINEAGE__BACKEND: airflow_provider_openmetadata.lineage.backend.OpenMetadataLineageBackend
      # AIRFLOW__LINEAGE__AIRFLOW_SERVICE_NAME: local_airflow
      # AIRFLOW__LINEAGE__OPENMETADATA_API_ENDPOINT: http://openmetadata-server:8585/api
      # AIRFLOW__LINEAGE__JWT_TOKEN: ...
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    expose:
      - 8082
    ports:
      - "8082:8080"
    networks:
      - osss-net
    volumes:
      - ingestion-volume-dag-airflow:/opt/airflow/dag_generated_configs:z
      - ingestion-volume-dags:/opt/airflow/dags:z
      - ingestion-volume-tmp:/tmp:z



  # === Vector DB for RAG ===
  qdrant:
    build:
      context: .
      dockerfile: docker/qdrant/Dockerfile
    container_name: qdrant
    hostname: qdrant
    profiles: [ ai ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage:z
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:6333/readyz" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 15s
    networks: [ osss-net ]
    restart: unless-stopped

  # === Object Storage for curriculum/corpus ===
  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    profiles: [ ai ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${AI_MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${AI_MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web console
    volumes:
      - minio_data:/data:z
    healthcheck:
      test: [ "CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000 && exec 3>&-" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 20s
    networks: [ osss-net ]
    restart: unless-stopped

  # === Redis for caching answers / sessions ===
  ai-redis:
    image: redis:7
    container_name: ai-redis
    hostname: ai-redis
    profiles: [ ai ]
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    ports:
      - "6382:6379"
    volumes:
      - ai_redis_data:/data:z
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks: [osss-net]
    restart: unless-stopped

  # === Postgres for audit, usage, metadata ===
  ai-postgres:
    image: postgres:15
    container_name: ai-postgres
    hostname: ai-postgres
    profiles: [ ai ]
    logging:
      driver: k8s-file
      options:
        max-size: 10m

    environment:
      POSTGRES_DB: ${AI_POSTGRES_DB}
      POSTGRES_USER: ${AI_POSTGRES_USER}
      POSTGRES_PASSWORD: ${AI_POSTGRES_PASSWORD}
    ports:
      - "5436:5432"
    volumes:
      - ai_pg_data:/var/lib/postgresql/data:z

    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB -h localhost" ]
      interval: 10s
      timeout: 5s
      retries: 20
      start_period: 20s
    networks: [ osss-net ]
    restart: unless-stopped

  dvc:
    image: python:3.11-slim
    container_name: dvc
    working_dir: /workspace
    entrypoint:
      - bash
      - -lc
      - |
        set -e
        # install dvc + a tiny web server
        pip install --no-cache-dir 'dvc[s3]' flask

        # write a minimal health endpoint
        cat >/workspace/health.py <<'PY'
        from flask import Flask, jsonify
        import subprocess, os

        app = Flask(__name__)

        @app.get("/health")
        def health():
            try:
                # Cheap sanity check that dvc is callable
                subprocess.run(["dvc","--version"], check=True,
                               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                return jsonify(status="ok")
            except Exception as e:
                return jsonify(status="error", detail=type(e).__name__), 500

        if __name__ == "__main__":
            app.run(host="0.0.0.0", port=int(os.environ.get("HEALTH_PORT", 8010)))
        PY

        exec python /workspace/health.py
    environment:
      # for S3/MinIO remotes (optional)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-us-east-1}
      HEALTH_PORT: "8010"
    volumes:
      - ./:/workspace
      - dvc-cache:/workspace/.dvc/cache
      - dvc-cache:/root/.cache/dvc
    expose:
      - "8010"              # internal-only; Consul can hit http://dvc:8000/health
    networks:
      - osss-net
    profiles: [ "ai" ]
  rasa-mentor:
    image: rasa/rasa:3.6.20
    profiles: [ "ai" ]
    container_name: rasa-mentor
    entrypoint: [ ]   # <â€” clear "rasa" entrypoint so bash runs
    command: >
      bash -lc "set -euo pipefail;
          mkdir -p models;
          exec rasa run --enable-api --cors '*' --model models/current.tar.gz --port 5005 --debug"
    working_dir: /app
    ports: [ "5005:5005" ]
    volumes:
      - ./rasa:/app
    environment:
      - TZ=America/Chicago
      - RASA_TELEMETRY_ENABLED=false
      - SQLALCHEMY_SILENCE_UBER_WARNING=1
      - SANIC_REQUEST_MAX_SIZE=104857600        # 100MB bodies (safe default)
      - SANIC_REQUEST_MAX_HEADER_SIZE=65536     # 64KB headers
    restart: unless-stopped
    networks: [ osss-net ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -fsS http://localhost:5005/status >/dev/null || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 10

