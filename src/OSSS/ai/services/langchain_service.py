"""
LangChain service implementation with structured output support.

This service implements the patterns from the LangChain structured output article:
- Direct with_structured_output() usage
- Provider-specific method selection
- Fallback to PydanticOutputParser
- Rich Pydantic validation
- Dynamic model discovery and selection
"""

import json
import asyncio
from typing import Type, TypeVar, Optional, Dict, Any, Union, cast, List
from pydantic import BaseModel, ValidationError

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.language_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser

from OSSS.ai.observability import get_logger, get_observability_context
from OSSS.ai.exceptions import LLMError, LLMValidationError
from OSSS.ai.services.model_discovery_service import (
    get_model_discovery_service,
    ModelDiscoveryService,
    ModelCategory,
    ModelSpeed,
)

MAX_STRUCTURED_ATTEMPTS = 1


# Import pool for eliminating redundancy
try:
    from OSSS.ai.services.llm_pool import LLMServicePool

    POOL_AVAILABLE = True
except ImportError:
    POOL_AVAILABLE = False


T = TypeVar("T", bound=BaseModel)


class StructuredOutputResult:
    """Wrapper for structured output with debugging info."""

    def __init__(
        self,
        parsed: BaseModel,
        raw: Optional[str] = None,
        method_used: Optional[str] = None,
        fallback_used: bool = False,
        processing_time_ms: Optional[float] = None,
    ):
        self.parsed = parsed
        self.raw = raw
        self.method_used = method_used
        self.fallback_used = fallback_used
        self.processing_time_ms = processing_time_ms


class LangChainService:
    """
    LangChain service with structured output capabilities.

    Implements patterns from the LangChain structured output article:
    - Uses with_structured_output() as primary method
    - Provider-specific optimizations (json_schema, function_calling, etc.)
    - Fallback to PydanticOutputParser for models that don't support structured output
    - Rich validation and error handling
    - Dynamic model discovery and intelligent selection
    """

    # Provider-specific method mapping - SIMPLIFIED for base models
    PROVIDER_METHODS = {
        "llama3.1": "json_schema",  # Base llama3.1 has full json_schema support
        "llama3.1-mini": "json_schema",  # Mini is still a base model, keep it
        "llama3.1-nano": "json_schema",  # Nano is still a base model, keep it
        "gpt-4o": "json_schema",  # GPT-4o supports json_schema
        "gpt-4o-mini": "json_schema",  # GPT-4o-mini supports json_schema
        "gpt-4-turbo": "json_schema",  # GPT-4-turbo supports json_schema
        "gpt-4": "function_calling",  # GPT-4 does NOT support json_schema
        "gpt-3.5": "function_calling",
        "claude-3": "function_calling",
        "claude-2": "function_calling",
        "gemini": "json_mode",
        "llama": "json_mode",
        "mistral": "json_mode",
    }

    def __init__(
        self,
        model: Optional[str] = None,
        temperature: float = 0.1,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        agent_name: Optional[str] = None,
        use_discovery: bool = True,
        use_pool: bool = True,  # NEW: Use pool when available
    ):
        """
        Initialize LangChain service with model.

        Args:
            model: Model name (if None, uses discovery service or pool)
            temperature: Model temperature
            api_key: OpenAI API key
            base_url: Optional base URL for API
            agent_name: Agent name for model selection (e.g., "refiner")
            use_discovery: Whether to use model discovery service
            use_pool: Whether to use the LLMServicePool (eliminates redundancy)
        """
        self.logger = get_logger("services.langchain")
        self.agent_name = agent_name
        self.use_discovery = use_discovery
        self.use_pool = use_pool and POOL_AVAILABLE
        self.api_key = api_key
        self.temperature = temperature
        self.base_url = base_url  # used for provider detection (ollama vs openai)

        # Type declarations for pool client
        self._pool_client: Optional[BaseChatModel] = None
        self._use_pool_client = False
        self.llm: Optional[BaseChatModel] = None

        # Try to use pool first (eliminates redundancy)
        if self.use_pool and self.agent_name and not model:
            try:
                self.logger.info(f"Using LLMServicePool for {self.agent_name}")
                pool = LLMServicePool.get_instance()

                # Use async-safe method to get client (will be called later)
                self._use_pool_client = True
                self.model_name = (
                    "pooled"  # Temporary, will be set when client is created
                )

            except Exception as e:
                self.logger.warning(f"Could not use LLMServicePool: {e}, falling back")
                self._use_pool_client = False
        else:
            self._use_pool_client = False

        # Fallback to original logic if not using pool
        if not self._use_pool_client:
            # Initialize model discovery service if enabled
            self.discovery_service: Optional[ModelDiscoveryService] = None
            if use_discovery:
                try:
                    self.discovery_service = get_model_discovery_service(
                        api_key=api_key
                    )
                except Exception as e:
                    self.logger.warning(f"Could not initialize discovery service: {e}")

            # Determine model to use
            self.model_name = model or self._select_best_model()
            self.logger.info(
                f"Using model: {self.model_name} for agent: {agent_name or 'default'}"
            )

            # Create appropriate LLM instance based on provider
            self.llm = self._create_llm_instance(
                self.model_name, temperature, api_key, base_url
            )

        # Metrics tracking with proper typing
        self.metrics: Dict[str, Union[int, str]] = {
            "total_calls": 0,
            "successful_structured": 0,
            "fallback_used": 0,
            "validation_failures": 0,
            "model_selected": (
                self.model_name if hasattr(self, "model_name") else "unknown"
            ),
        }

    def _json_instruction(self, output_class: Type[BaseModel]) -> str:
        """
        Strict JSON-only instruction for prompted JSON mode.

        Key goals:
        - Exact top-level keys only (no nesting like {"Refined Query": {...}})
        - No extra keys
        - No renaming (case-sensitive)
        - Strong “shape” example to anchor the model
        """
        schema = output_class.model_json_schema()
        keys = list(output_class.model_fields.keys())
        keys_str = ", ".join(keys)

        # Build an "example shape" that doesn't accidentally force invalid nulls.
        # If a field is optional/nullable in schema, show null; otherwise show a plausible placeholder.
        props = schema.get("properties", {}) if isinstance(schema, dict) else {}

        def _placeholder_for(k: str) -> str:
            p = props.get(k, {})
            # nullable if anyOf includes {"type":"null"} OR type == "null"
            nullable = False
            if isinstance(p, dict):
                anyof = p.get("anyOf")
                if isinstance(anyof, list):
                    nullable = any(isinstance(x, dict) and x.get("type") == "null" for x in anyof)
                if p.get("type") == "null":
                    nullable = True

            if nullable:
                return "null"

            # simple placeholders to reduce model “creativity”
            if isinstance(p, dict) and p.get("type") == "array":
                return "[]"
            if isinstance(p, dict) and p.get("type") == "object":
                return "{}"
            if isinstance(p, dict) and p.get("type") == "number":
                return "0"
            if isinstance(p, dict) and p.get("type") == "boolean":
                return "false"

            # default string-ish placeholder
            return "\"\""

        example_lines = [f'  "{k}": {_placeholder_for(k)},' for k in keys]
        example = "{\n" + "\n".join(example_lines) + "\n}\n"

        return (
            "Return ONLY valid JSON (no markdown, no commentary).\n"
            f"Top-level keys MUST be exactly: {keys_str}\n"
            "Do NOT rename keys. Do NOT nest under other keys. Do NOT add extra keys.\n"
            "Values must match the schema types.\n"
            "Example shape (keys only):\n"
            f"{example}"
        )

    def _is_ollama_endpoint(self) -> bool:
        # Prefer explicit base_url
        u = (self.base_url or "").lower()

        # Fall back to whatever the ChatOpenAI instance is using
        if not u and self.llm is not None:
            u = str(getattr(self.llm, "base_url", "") or "").lower()

        return ("ollama" in u) or ("localhost" in u) or ("127.0.0.1" in u) or (":11434" in u)

    def _is_llama_family(self, model: str) -> bool:
        m = (model or "").lower()
        return "llama" in m

    def _should_avoid_json_schema(self, model: str) -> bool:
        """
        Option 1 rule:
          If using Ollama AND llama-family model => do NOT use json_schema.
        """
        return self._is_ollama_endpoint() and self._is_llama_family(model)

    def _should_force_prompted_json(self, model: str) -> bool:
        """
        Hard rule:
        - Ollama + llama (including vision variants) frequently violate schemas
        - Force prompted JSON instead of LangChain structured output
        """
        m = (model or "").lower()
        return self._is_ollama_endpoint() and (
                "llama" in m or "vision" in m
        )

    async def _ensure_pooled_client(self) -> None:
        """Ensure pooled client is initialized (async-safe)."""
        if self._use_pool_client and not self._pool_client:
            try:
                pool = LLMServicePool.get_instance()
                if self.agent_name:  # Type guard
                    (
                        self._pool_client,
                        self.model_name,
                    ) = await pool.get_optimal_client_for_agent(
                        self.agent_name, self.temperature
                    )
                    self.llm = self._pool_client
                else:
                    raise ValueError("agent_name is required for pooled client")
                self.logger.info(
                    f"Initialized pooled client: {self.model_name} for {self.agent_name}"
                )
            except Exception as e:
                self.logger.error(f"Failed to get pooled client: {e}")
                # Fallback to traditional client creation
                self._use_pool_client = False
                self.model_name = self._select_best_model()
                self.llm = self._create_llm_instance(
                    self.model_name, self.temperature, self.api_key, None
                )

    def _select_best_model(self) -> str:
        """Select the best available model using discovery service."""
        if not self.discovery_service or not self.agent_name:
            # Default fallback
            return "gpt-4o"

        try:
            # Check if we're already in an async context
            import asyncio

            try:
                # Try to get the current running loop
                loop = asyncio.get_running_loop()
                # We're in an async context, can't use run_until_complete
                # Use sync fallback for now
                self.logger.info(
                    f"In async context, using fallback model selection for '{self.agent_name}'"
                )
            except RuntimeError:
                # No running loop, we can create one
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    best_model = loop.run_until_complete(
                        self.discovery_service.get_best_model_for_agent(self.agent_name)
                    )
                    if best_model:
                        self.logger.info(
                            f"Discovery service selected '{best_model}' for agent '{self.agent_name}'"
                        )
                        return best_model
                finally:
                    loop.close()
        except Exception as e:
            self.logger.warning(f"Model discovery failed: {e}")

        # SIMPLIFIED fallbacks - prefer base llama3.1 when available
        # User wisdom: "just use the model: 'llama3.1'"
        agent_fallbacks = {
            "refiner": "llama3.1",  # Keep nano for ultra-fast requirement
            "historian": "llama3.1",  # Base llama3.1 for historian
            "critic": "llama3.1",  # Base llama3.1 for critic
            "synthesis": "llama3.1",  # Base llama3.1 for synthesis
        }
        fallback = agent_fallbacks.get(self.agent_name, "gpt-4o")
        self.logger.info(
            f"Using fallback model '{fallback}' for agent '{self.agent_name}'"
        )
        return fallback

    def _create_llm_instance(
        self,
        model: str,
        temperature: float,
        api_key: Optional[str],
        base_url: Optional[str],
    ) -> BaseChatModel:
        """Create appropriate LLM instance based on model name."""
        model_lower = model.lower()

        # Detect Ollama (your setup: http://host.containers.internal:11434/v1)
        is_ollama = bool(base_url) and "11434" in base_url

        if "gpt" in model_lower or "o1" in model_lower or is_ollama:
            kwargs: Dict[str, Any] = {
                "model": model,
                "api_key": api_key,
                "base_url": base_url,
            }

            # ---- HARD CAPS (big latency win) ----
            if is_ollama:
                kwargs["max_tokens"] = 300  # cap completion length
                kwargs["request_timeout"] = 12  # network timeout (seconds)

                # PER-AGENT override (your question)
                if self.agent_name == "synthesis":
                    kwargs["max_tokens"] = 600

            # Keep your llama3.1 temperature rule
            if "llama3.1" not in model_lower and not is_ollama:
                kwargs["temperature"] = temperature

            return ChatOpenAI(**kwargs)
        elif "claude" in model_lower:
            # Note: Would need anthropic API key configuration
            if api_key is None:
                api_key = "test-key-for-mock"  # Allow mock testing without real API key
            return ChatAnthropic(
                model=model,
                temperature=temperature,
                api_key=api_key,
            )
        else:
            # Default to OpenAI for unknown models
            openai_kwargs: Dict[str, Any] = {
                "model": model,
                "api_key": api_key,
                "base_url": base_url,
            }

            # CRITICAL FIX: llama3.1 models only support temperature=1 (default)
            # Exclude temperature parameter for llama3.1 to avoid API constraint errors
            if "llama3.1" not in model_lower:
                openai_kwargs["temperature"] = temperature
            else:
                self.logger.info(
                    f"Excluding temperature parameter for {model} (llama3.1 only supports default temperature=1)"
                )

            # CRITICAL FIX: llama3.1 models require max_completion_tokens instead of max_tokens
            # Transform parameter for llama3.1 models to avoid "Unsupported parameter" errors
            if "max_tokens" in openai_kwargs and "llama3.1" in model_lower:
                openai_kwargs["max_completion_tokens"] = openai_kwargs.pop("max_tokens")
                self.logger.info(
                    f"Transformed max_tokens → max_completion_tokens for {model} (llama3.1 parameter requirement)"
                )

            # CRITICAL FIX: Removing output_version for now as it breaks the endpoint
            # The native OpenAI parse() method will handle llama3.1 structured outputs
            # if "llama3.1" in model_lower:
            #     openai_kwargs["output_version"] = "responses/v1"  # This causes /v1/responses endpoint issue

            return ChatOpenAI(**openai_kwargs)

    def _get_structured_output_method(self, model: str) -> Optional[str]:
        model_lower = model.lower()

        # Option 1: Ollama + llama => avoid json_schema (slow/fragile); use json_mode instead
        if self._should_avoid_json_schema(model):
            self.logger.info(
                f"[STRUCTURED OUTPUT] Detected Ollama+llama for model={model}; forcing json_mode (no json_schema)."
            )
            return "json_mode"

        # Existing special-case handling
        if "llama3.1" in model_lower and "2025-08" in model_lower:
            self.logger.warning(
                f"Model {model} is a timestamped llama3.1 variant requiring function_calling method"
            )
            return "function_calling"

        if "-chat" in model_lower:
            self.logger.warning(
                f"Model {model} is a chat variant with limited structured output support, using json_mode"
            )
            return "json_mode"

        # Base llama3.1 models (ONLY if not Ollama)
        if model_lower in ["llama3.1", "llama3.1-nano", "llama3.1-mini"]:
            self.logger.info(f"Using json_schema for base model {model}")
            return "json_schema"

        # Standard lookup...
        for model_prefix, method in self.PROVIDER_METHODS.items():
            if model_prefix in model_lower:
                return method

        return "json_mode"

    async def get_structured_output(
        self,
        prompt: str,
        output_class: Type[T],
        *,
        include_raw: bool = False,
        max_retries: int = MAX_STRUCTURED_ATTEMPTS,
        system_prompt: Optional[str] = None,
    ) -> Union[T, StructuredOutputResult]:
        """
        Get structured output using LangChain's with_structured_output().

        Implements the exact pattern from the article:
        1. Try native structured output with provider-specific method
        2. Fallback to PydanticOutputParser if native fails
        3. Return parsed result or full result with debugging info

        Args:
            prompt: User prompt
            output_class: Pydantic model class for output structure
            include_raw: Whether to include raw response for debugging
            max_retries: Number of retry attempts
            system_prompt: Optional system prompt

        Returns:
            Either the parsed Pydantic model or StructuredOutputResult with debug info
        """
        import time

        start_time = time.time()

        # Ensure pooled client is ready if we're using pool
        await self._ensure_pooled_client()

        # Ensure we have an LLM instance
        if self.llm is None:
            raise ValueError("LLM instance not initialized")

        self.metrics["total_calls"] = int(self.metrics["total_calls"]) + 1
        obs_context = get_observability_context()

        # Build messages
        messages = []
        if system_prompt:
            messages.append(("system", system_prompt))
        messages.append(("human", prompt))

        # ------------------------------------------------------------------
        # HARD STOP: Ollama + llama OR vision models
        # One-shot prompted JSON only (plus the single repair inside _try_prompted_json)
        # Skip LangChain structured-output retries and skip parser fallback.
        # ------------------------------------------------------------------
        if self._should_force_prompted_json(self.model_name):
            self.logger.info(
                f"[STRUCTURED OUTPUT] Forced prompted JSON fast-path for model={self.model_name}"
            )
            result = await self._try_prompted_json(messages, output_class, include_raw)

            processing_time_ms = (time.time() - start_time) * 1000
            self.metrics["successful_structured"] = int(self.metrics["successful_structured"]) + 1

            if include_raw:
                return StructuredOutputResult(
                    parsed=result["parsed"] if isinstance(result, dict) else result,
                    raw=result.get("raw") if isinstance(result, dict) else None,
                    method_used="prompted_json",
                    fallback_used=False,
                    processing_time_ms=processing_time_ms,
                )

            return result["parsed"] if isinstance(result, dict) else result

        # Try native structured output first (article's primary approach)
        for attempt in range(max_retries):
            try:
                result = await self._try_native_structured_output(
                    messages, output_class, include_raw, attempt, start_time
                )

                processing_time_ms = (time.time() - start_time) * 1000

                self.metrics["successful_structured"] = (
                    int(self.metrics["successful_structured"]) + 1
                )

                self.logger.info(
                    "Structured output successful",
                    model=self.model_name,
                    output_class=output_class.__name__,
                    method="native",
                    attempt=attempt + 1,
                    processing_time_ms=processing_time_ms,
                    agent_name=obs_context.agent_name if obs_context else None,
                )

                if include_raw:
                    return StructuredOutputResult(
                        parsed=result["parsed"] if isinstance(result, dict) else result,
                        raw=result.get("raw") if isinstance(result, dict) else None,
                        method_used=self._get_structured_output_method(self.model_name),
                        fallback_used=False,
                        processing_time_ms=processing_time_ms,
                    )

                return result["parsed"] if isinstance(result, dict) else result

            except Exception as e:
                error_context = getattr(e, "context", {})
                error_type = error_context.get("error_type", "unknown")
                fallback_recommended = error_context.get("fallback_recommended", True)

                self.logger.warning(
                    f"Native structured output attempt {attempt + 1} failed",
                    error=str(e),
                    error_type=error_type,
                    model=self.model_name,
                    output_class=output_class.__name__,
                )

                # Smart fallback decision based on error type
                if error_type == "quota_exceeded":
                    # Don't retry on quota errors - fail fast
                    self.logger.error(
                        "API quota exceeded - failing immediately without retries"
                    )
                    raise e
                elif error_type == "schema_validation":
                    # Schema errors won't be fixed by retries, skip to fallback
                    self.logger.info(
                        "Schema validation error detected - skipping retries, going to fallback parser"
                    )
                    break
                elif not fallback_recommended:
                    # Error analysis suggests fallback won't help
                    raise e

                if attempt == max_retries - 1:
                    # Last attempt failed, try fallback
                    break

                # Progressive backoff with jitter for rate limiting
                base_delay = 0.5 * (attempt + 1)
                jitter = 0.1 * attempt  # Add small jitter to avoid thundering herd
                await asyncio.sleep(base_delay + jitter)

        # Fallback to PydanticOutputParser (article's fallback strategy)
        try:
            result = await self._fallback_to_parser(messages, output_class, include_raw)

            processing_time_ms = (time.time() - start_time) * 1000

            self.metrics["fallback_used"] = int(self.metrics["fallback_used"]) + 1

            self.logger.info(
                "Fallback parser successful",
                model=self.model_name,
                output_class=output_class.__name__,
                method="parser",
                processing_time_ms=processing_time_ms,
            )

            if include_raw:
                return StructuredOutputResult(
                    parsed=result,
                    raw=getattr(result, "_raw_response", None),
                    method_used="parser",
                    fallback_used=True,
                    processing_time_ms=processing_time_ms,
                )

            return result

        except Exception as e:
            self.metrics["validation_failures"] = (
                int(self.metrics["validation_failures"]) + 1
            )
            processing_time_ms = (time.time() - start_time) * 1000

            raise LLMValidationError(
                message=f"Failed to get structured output for {output_class.__name__} after all attempts",
                model_name=self.model_name,
                validation_errors=[str(e)],
                context={
                    "output_class": output_class.__name__,
                    "max_retries": max_retries,
                    "processing_time_ms": processing_time_ms,
                    "fallback_attempted": True,
                },
            )

    def _is_ollama(self) -> bool:
        return bool(self.base_url) and "11434" in self.base_url

    async def _try_native_structured_output(
        self,
        messages: List[tuple[str, str]],
        output_class: Type[T],
        include_raw: bool = False,
        attempt: int = 0,
        start_time: Optional[float] = None,
    ) -> Union[T, Dict[str, Any]]:
        """Try native structured output with provider-specific method.

        Args:
            messages: Chat messages
            output_class: Expected output structure
            include_raw: Whether to include raw response
            attempt: Current retry attempt number for timeout adjustment
            start_time: Start time for timeout budget calculation
        """
        import time

        if start_time is None:
            start_time = time.time()

        # TESTING: Re-enabling native OpenAI parse to check if bugs are fixed
        # Previous issue: beta.chat.completions.parse was returning None
        # Testing if OpenAI has fixed these bugs as of Nov 2025
        if "llama3.1" in self.model_name.lower()  and not self._is_ollama():  # Re-enabled for testing
            try:
                self.logger.info(
                    f"[NATIVE PARSE TEST] Attempting native OpenAI parse for {self.model_name}"
                )
                return await self._try_native_openai_parse(
                    messages, output_class, include_raw
                )
            except Exception as e:
                self.logger.warning(
                    f"[NATIVE PARSE TEST] Native OpenAI parse failed for llama3.1: {e}, falling back to LangChain"
                )
                # Fall through to LangChain attempt

        # ------------------------------------------------------------------
        # HARD STOP: Ollama + llama OR vision models
        # Always use prompted JSON + Pydantic validation
        # ------------------------------------------------------------------
        if self._should_force_prompted_json(self.model_name):
            self.logger.info(
                f"[STRUCTURED OUTPUT] Forced prompted JSON for model={self.model_name}"
            )
            return await self._try_prompted_json(
                messages, output_class, include_raw
            )

        # Get provider-specific method (key insight from article)
        method = self._get_structured_output_method(self.model_name)

        # If this is Ollama llama, ALWAYS use prompted JSON (1 call + optional repair)
        if self._should_avoid_json_schema(self.model_name):
            return await self._try_prompted_json(messages, output_class, include_raw)


        # ENHANCEMENT: Try alternative methods if primary fails
        methods_to_try = [method]

        # Add fallback methods based on primary method
        if method == "json_schema" and "llama3.1" in self.model_name.lower():
            methods_to_try.append(
                "function_calling"
            )  # Fallback for problematic llama3.1 variants
        if method != "json_mode":
            methods_to_try.append("json_mode")  # Ultimate fallback

        last_error = None
        for method_attempt in methods_to_try:
            try:
                assert self.llm is not None  # Type assertion - already checked above

                self.logger.debug(
                    f"Attempting structured output with method={method_attempt}"
                )

                # Add timeout protection to prevent hanging
                try:
                    # CRITICAL FIX: Use schema transformation for llama3.1 strict mode
                    # llama3.1 requires ALL properties in required array (strict mode)
                    use_schema_transformation = (
                        "llama3.1" in self.model_name.lower()
                        and method_attempt == "json_schema"
                    )

                    if use_schema_transformation:
                        # Transform schema for OpenAI strict mode compatibility
                        openai_schema = self._prepare_schema_for_openai(output_class)

                        # DEBUGGING: Log the transformed schema
                        schema_preview = {
                            "title": openai_schema.get("title", "N/A"),
                            "type": openai_schema.get("type", "N/A"),
                            "properties_count": len(
                                openai_schema.get("properties", {})
                            ),
                            "required_count": len(openai_schema.get("required", [])),
                            "has_defs": "$defs" in openai_schema,
                        }
                        self.logger.info(
                            f"[SCHEMA DEBUG] Transformed schema for {output_class.__name__}: {schema_preview}"
                        )

                        # CRITICAL FIX: Wrap schema with name for OpenAI API
                        # OpenAI requires: {"type": "json_schema", "json_schema": {"name": "...", "schema": {...}, "strict": true}}
                        # But LangChain's with_structured_output() expects just the schema
                        # Try adding title to help LangChain identify the schema
                        if "title" not in openai_schema:
                            openai_schema["title"] = output_class.__name__
                            self.logger.info(
                                f"[SCHEMA DEBUG] Added title '{output_class.__name__}' to schema"
                            )

                        self.logger.info(
                            f"[SCHEMA DEBUG] Calling with_structured_output(schema=dict, method={method_attempt})"
                        )
                        structured_llm = self.llm.with_structured_output(
                            schema=openai_schema,
                            method=method_attempt,
                            include_raw=include_raw,
                        )
                        self.logger.info(
                            f"[SCHEMA DEBUG] with_structured_output() created, about to invoke..."
                        )
                    else:
                        # Use standard Pydantic class for non-llama3.1 models
                        structured_llm = self.llm.with_structured_output(
                            output_class,
                            method=method_attempt,
                            include_raw=include_raw,
                        )

                    # Dynamic timeout calculation to prevent cascade failures
                    # Calculate remaining time budget based on total elapsed time
                    current_time = time.time()
                    elapsed_time = current_time - start_time

                    # Reserve time for fallback parser (5s) and buffer (2s)
                    reserved_time = 7.0
                    max_agent_timeout = 30.0  # Agent timeout constraint

                    remaining_budget = max_agent_timeout - elapsed_time - reserved_time

                    # Calculate optimal timeout for this attempt
                    if remaining_budget <= 0:
                        # Out of time budget, BREAK retry loop immediately
                        self.logger.warning(
                            f"Time budget exhausted ({elapsed_time:.1f}s elapsed), breaking retry loop"
                        )
                        # Set flag to break outer method loop
                        last_error = f"Time budget exhausted after {elapsed_time:.1f}s"
                        break  # Exit method retry loop

                    # Progressive timeout with budget constraints
                    base_timeouts = [8.0, 6.0, 4.0]  # Reduced base timeouts
                    attempt_timeout = min(
                        base_timeouts[min(attempt, len(base_timeouts) - 1)],
                        remaining_budget,
                    )

                    self.logger.debug(
                        f"Attempt {attempt + 1}: {attempt_timeout:.1f}s timeout "
                        f"(elapsed: {elapsed_time:.1f}s, budget: {remaining_budget:.1f}s)"
                    )

                    result = await asyncio.wait_for(
                        structured_llm.ainvoke(messages),
                        timeout=attempt_timeout,  # Progressive reduction: 10s, 8s, 5s
                    )

                    # Success! Log and return
                    self.logger.info(
                        f"Structured output succeeded with method={method_attempt} for {self.model_name}"
                    )

                    # CRITICAL FIX: Convert dict to Pydantic when using schema transformation
                    # LangChain returns plain dict when given schema dict instead of class
                    if use_schema_transformation and isinstance(result, dict):
                        # Handle include_raw case where result is {"parsed": ..., "raw": ...}
                        if "parsed" in result and isinstance(result["parsed"], dict):
                            result["parsed"] = output_class(**result["parsed"])
                            self.logger.debug(
                                f"Converted schema dict result to {output_class.__name__} (include_raw=True)"
                            )
                        else:
                            # Plain dict result, convert to Pydantic
                            result = output_class(**result)
                            self.logger.debug(
                                f"Converted schema dict result to {output_class.__name__}"
                            )

                    return cast(Union[T, Dict[str, Any]], result)

                except asyncio.TimeoutError:
                    self.logger.warning(
                        f"Structured output timed out after {attempt_timeout}s with method={method_attempt} for {self.model_name}"
                    )
                    last_error = (
                        f"Timeout after {attempt_timeout}s with method={method_attempt}"
                    )
                    continue  # Try next method

            except (AttributeError, ValueError, Warning) as e:
                # Method not supported or schema issue - try next method
                self.logger.warning(
                    f"Method {method_attempt} failed for {self.model_name}: {e}"
                )
                last_error = str(e)
                continue  # Try next method

        # All methods failed - raise final error
        if last_error:
            raise LLMError(
                message=f"All structured output methods failed for {self.model_name}. Last error: {last_error}",
                llm_provider=(
                    self.model_name.split("-")[0]
                    if "-" in self.model_name
                    else "unknown"
                ),
                context={"methods_tried": methods_to_try, "last_error": last_error},
            )

        # Shouldn't reach here, but handle gracefully
        raise LLMError(
            message="Unexpected error in structured output",
            llm_provider="unknown",
            context={},
        )

    async def _try_prompted_json(
            self,
            messages: List[tuple[str, str]],
            output_class: Type[T],
            include_raw: bool = False,
    ) -> Union[T, Dict[str, Any]]:
        """
        Prompted JSON strategy:
        - Ask for strict JSON
        - Parse manually
        - Validate with Pydantic
        - One repair attempt only
        """

        assert self.llm is not None

        # --- 1) Initial attempt ---
        json_messages = messages + [
            ("system", self._json_instruction(output_class))
        ]

        timeout_s = 5.0 if self._should_force_prompted_json(self.model_name) else 10.0
        response = await asyncio.wait_for(self.llm.ainvoke(json_messages), timeout=timeout_s)

        raw = str(response.content or "").strip()

        try:
            parsed_dict = json.loads(raw)
            parsed = output_class(**parsed_dict)
            return {"parsed": parsed, "raw": raw} if include_raw else parsed
        except Exception as first_error:
            self.logger.warning(
                "Prompted JSON parse failed, attempting single repair",
                error=str(first_error),
            )

        # --- 2) ONE repair attempt ---
        repair_prompt = (
            "The previous output was invalid JSON or did not match the required top-level schema.\n"
            "Fix it and return ONLY corrected JSON.\n"
            f"Top-level keys MUST be exactly: {', '.join(output_class.model_fields.keys())}\n"
            "Do NOT wrap under 'Refined Query' or any other parent key.\n\n"
            f"Invalid output:\n{raw}"
        )

        repair_messages = messages + [
            ("system", self._json_instruction(output_class)),
            ("human", repair_prompt),
        ]

        response = await self.llm.ainvoke(repair_messages)
        repaired_raw = str(response.content or "").strip()

        try:
            parsed_dict = json.loads(repaired_raw)
            parsed = output_class(**parsed_dict)
            return {"parsed": parsed, "raw": repaired_raw} if include_raw else parsed
        except Exception as repair_error:
            raise LLMValidationError(
                message="Prompted JSON failed after single repair attempt",
                model_name=self.model_name,
                validation_errors=[str(repair_error)],
                context={
                    "initial_error": str(first_error),
                    "repaired_output": repaired_raw[:500],
                },
            )

    async def _try_native_openai_parse(
        self,
        messages: List[tuple[str, str]],
        output_class: Type[T],
        include_raw: bool = False,
    ) -> Union[T, Dict[str, Any]]:
        """
        Use OpenAI's native parse() API for llama3.1 models.

        This is the PROVEN approach from user testing that works with llama3.1.
        LangChain's with_structured_output() breaks with output_version parameter,
        but native OpenAI API works perfectly.
        """
        try:
            from openai import AsyncOpenAI
        except ImportError:
            self.logger.warning("OpenAI library not available for native parse")
            raise

        # Initialize native OpenAI client
        client = AsyncOpenAI(api_key=self.api_key)

        # Convert LangChain message format to OpenAI format
        openai_messages = []
        for role, content in messages:
            if role == "system":
                openai_messages.append({"role": "system", "content": content})
            elif role == "human" or role == "user":
                openai_messages.append({"role": "user", "content": content})
            elif role == "assistant":
                openai_messages.append({"role": "assistant", "content": content})
            else:
                # Skip unknown roles
                self.logger.warning(f"Unknown message role: {role}")

        try:
            # CORRECTED FIX: Use stable create() API with MANUAL schema transformation
            # Why: OpenAI SDK's automatic Pydantic->schema conversion has bugs with Dict fields (Issue #2004)
            # Solution: Use our manually-fixed schema transformation with create() API + manual parsing
            # CRITICAL: .parse() API only accepts Pydantic classes directly, NOT manual schema dicts!
            #           .create() API accepts manual schema dicts and returns JSON string

            # Prepare OpenAI-compatible schema with Dict field fix (PR #2003)
            openai_schema = self._prepare_schema_for_openai(output_class)

            # Build kwargs for create call with manual schema
            create_kwargs: Dict[str, Any] = {
                "model": self.model_name,
                "messages": openai_messages,
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                        "name": output_class.__name__,
                        "schema": openai_schema,
                        "strict": True,
                    },
                },
            }

            # CRITICAL FIX: llama3.1 models only support temperature=1 (default)
            # Exclude temperature parameter for llama3.1 to avoid API constraint errors
            if "llama3.1" not in self.model_name.lower():
                create_kwargs["temperature"] = self.temperature
            else:
                self.logger.info(
                    f"Excluding temperature from native create for {self.model_name} (llama3.1 only supports default temperature=1)"
                )

            self.logger.info(
                f"[STABLE CREATE API] Using stable create() with manual schema for {self.model_name}"
            )

            # Use stable create() API (NOT parse) with our fixed schema
            completion = await client.chat.completions.create(**create_kwargs)

            # Check for refusals (safety-based model refusals)
            if completion.choices[0].message.refusal:
                raise LLMError(
                    message=f"Model refused request: {completion.choices[0].message.refusal}",
                    llm_provider="openai",
                    context={
                        "model": self.model_name,
                        "refusal": completion.choices[0].message.refusal,
                    },
                )

            # Extract raw JSON content
            raw_content = completion.choices[0].message.content

            if not raw_content:
                raise LLMError(
                    message=f"Stable create() returned empty content for {self.model_name}",
                    llm_provider="openai",
                    context={
                        "model": self.model_name,
                        "method": "stable_create_manual_schema",
                    },
                )

            # Parse JSON manually
            response_dict = json.loads(raw_content)

            # === PHASE 1 MEASUREMENT LOGGING - NO BEHAVIOR CHANGES ===
            self.logger.info(f"[BASELINE] Parsing {output_class.__name__} from OpenAI")

            # Priority 1: Length constraint violations
            LENGTH_CONSTRAINED_FIELDS = {
                "alternate_framings": 250,  # per item - increased to accommodate LLM natural language
                "critique_summary": 300,
                "logical_gaps": 150,  # per item
                "changes_made": 150,  # per item - increased to accommodate LLM natural language
                "assumptions": 150,  # per item
                "biases": 150,  # per item
            }

            for field_name, max_length in LENGTH_CONSTRAINED_FIELDS.items():
                if field_name in response_dict:
                    value = response_dict[field_name]
                    if isinstance(value, str):
                        actual_length = len(value)
                        violation = actual_length > max_length
                        self.logger.info(
                            f"[LENGTH] {field_name}: {actual_length} chars "
                            f"(limit: {max_length}) {'❌ VIOLATION' if violation else '✅ OK'}"
                        )
                    elif isinstance(value, list):
                        self.logger.info(f"[LENGTH] {field_name}: {len(value)} items")
                        for i, item in enumerate(value):
                            if isinstance(item, str):
                                actual_length = len(item)
                                violation = actual_length > max_length
                                self.logger.info(
                                    f"[LENGTH]   [{i}]: {actual_length} chars "
                                    f"(limit: {max_length}) {'❌ VIOLATION' if violation else '✅ OK'}"
                                )

            # Priority 2: None values for required fields
            for field_name, field_value in response_dict.items():
                if field_value is None:
                    self.logger.warning(f"[NONE] Field '{field_name}' returned as None")

            # SERVER-SIDE INJECTION: Calculate processing_time_ms from execution context
            # CRITICAL FIX: LLMs cannot accurately measure their own processing time
            # Instead, we inject the server-calculated time from AgentContext tracking
            # This prevents None values and ensures accurate performance metrics
            if (
                "processing_time_ms" in response_dict
                and response_dict["processing_time_ms"] is None
            ):
                # For agent outputs, we should have server-side timing available
                # However, at this point in LangChain service, we don't have access to AgentContext
                # So we'll set a sentinel value that agents can override after
                # NOTE: This is a temporary placeholder - agents should inject actual time
                self.logger.info(
                    "[SERVER-SIDE TIMING] processing_time_ms is None from LLM - will be injected by agent"
                )

            # Validate and instantiate Pydantic model manually
            parsed_result = output_class(**response_dict)

            self.logger.info(
                f"[STABLE CREATE API] Successfully parsed {output_class.__name__} for {self.model_name}"
            )

            if include_raw:
                # Return with raw content for debugging
                return {"parsed": parsed_result, "raw": raw_content}

            return parsed_result

        except Exception as e:
            error_message = str(e).lower()

            # Enhanced error classification for better fallback decisions
            is_schema_error = any(
                phrase in error_message
                for phrase in [
                    "invalid schema",
                    "required is required",
                    "missing",
                    "additional keywords",
                    "$ref",
                    "additionalproperties",
                ]
            )

            is_quota_error = any(
                phrase in error_message
                for phrase in [
                    "quota exceeded",
                    "rate limit",
                    "insufficient credits",
                    "billing",
                ]
            )

            is_timeout_error = any(
                phrase in error_message
                for phrase in ["timeout", "connection", "network"]
            )

            # Log different error types with appropriate levels
            if is_quota_error:
                self.logger.error(
                    f"OpenAI quota/billing error for {self.model_name}: {e}"
                )
            elif is_schema_error:
                self.logger.warning(
                    f"OpenAI schema validation error for {self.model_name}: {e}"
                )
            elif is_timeout_error:
                self.logger.warning(f"OpenAI timeout error for {self.model_name}: {e}")
            else:
                self.logger.error(
                    f"Native OpenAI parse failed for {self.model_name}: {e}"
                )

            # Provide context-aware error information for fallback decisions
            raise LLMError(
                message=f"Native OpenAI parse failed for {self.model_name}",
                llm_provider="openai",
                context={
                    "error": str(e),
                    "model": self.model_name,
                    "method": "native_parse",
                    "error_type": (
                        "schema_validation"
                        if is_schema_error
                        else (
                            "quota_exceeded"
                            if is_quota_error
                            else "timeout"
                            if is_timeout_error
                            else "unknown"
                        )
                    ),
                    "fallback_recommended": not is_quota_error,  # Don't fallback on quota errors
                    "schema_fix_needed": is_schema_error,
                },
            )

    def _prepare_schema_for_openai(
        self, model_class: Type[BaseModel]
    ) -> Dict[str, Any]:
        """
        Prepare Pydantic model schema for OpenAI's structured output API.

        CORRECTED REQUIREMENTS based on OpenAI beta.parse API testing:
        1. ALL properties MUST be in required array (OpenAI requirement)
        2. Optional/default fields should use anyOf with null type for nullable values
        3. Dict fields need explicit additionalProperties: false in all definitions
        4. $ref fields CANNOT have additional keywords like 'description'
        5. All unsupported constraints must be removed (maxLength, minLength, format, etc.)
        6. Nested models need same treatment as root model

        Args:
            model_class: The Pydantic model class to generate schema for

        Returns:
            Dict containing the OpenAI-compatible JSON schema
        """
        import copy
        from typing import get_origin, get_args

        # Generate the standard Pydantic JSON schema
        schema = model_class.model_json_schema()

        # Create a deep copy to avoid modifying the original
        fixed_schema = copy.deepcopy(schema)

        if "properties" not in fixed_schema:
            return fixed_schema

        # Get model fields to understand which are actually required
        model_fields = model_class.model_fields

        # Based on OpenAI error: "required is required to be supplied and to be an array including every key in properties"
        # OpenAI requires ALL properties to be in required array, but optional fields should be nullable
        # EXCEPTION: Dict fields (with additionalProperties) should NOT be in required array
        actual_required = list(fixed_schema["properties"].keys())
        dict_fields: list[str] = []  # Track Dict fields to exclude from required

        for field_name, field_info in model_fields.items():
            # Import PydanticUndefined for correct detection
            from pydantic_core import PydanticUndefined

            # Check if field has a default value or default_factory
            # In Pydantic v2, PydanticUndefined means "no default value"
            has_default_value = field_info.default is not PydanticUndefined
            has_default_factory = field_info.default_factory is not None
            has_any_default = has_default_value or has_default_factory

            # Check if field is Optional (Union with None)
            is_optional = get_origin(field_info.annotation) is Union and type(
                None
            ) in get_args(field_info.annotation)

            # For fields with defaults or Optional fields, ensure proper schema setup
            if field_name in fixed_schema["properties"]:
                prop_def = fixed_schema["properties"][field_name]

                # CRITICAL FIX: Distinguish default_factory from default=None
                # Only make nullable if: Optional[T] OR has default=None
                # Fields with default_factory should NOT be nullable (they have value generators)
                should_be_nullable = is_optional or (
                    has_default_value and field_info.default is None
                )

                # For truly nullable fields, make them nullable in schema
                if should_be_nullable and isinstance(prop_def, dict):
                    if "type" in prop_def and not isinstance(
                        prop_def.get("anyOf"), list
                    ):
                        # Convert to union type with null to indicate it can accept null values
                        original_type = prop_def.copy()
                        # DON'T remove the type - OpenAI requires it
                        # Clean unsupported constraints from the original type
                        unsupported_keys = [
                            "maxLength",
                            "minLength",
                            "format",
                            "pattern",
                            "maxItems",
                            "minItems",
                            "maximum",
                            "minimum",
                        ]
                        for key in unsupported_keys:
                            original_type.pop(key, None)
                        prop_def.clear()
                        prop_def["anyOf"] = [original_type, {"type": "null"}]

                # Clean up unsupported constraints
                if isinstance(prop_def, dict):
                    # Remove constraints that OpenAI doesn't support
                    unsupported_keys = [
                        "maxLength",
                        "minLength",
                        "format",
                        "pattern",
                        "maxItems",
                        "minItems",
                        "maximum",
                        "minimum",
                    ]
                    for key in unsupported_keys:
                        prop_def.pop(key, None)

                self.logger.debug(
                    f"Field {field_name}: has_default_value={has_default_value}, has_default_factory={has_default_factory}, "
                    f"is_optional={is_optional}, in_required=yes (all fields required by OpenAI), "
                    f"nullable={'yes' if should_be_nullable else 'no'} "
                    f"(default_factory fields are NOT nullable)"
                )

        # Set ALL properties as required (OpenAI requirement)
        # ALL properties MUST be in required array per OpenAI docs
        fixed_schema["required"] = actual_required

        self.logger.info(
            f"OpenAI schema correction for {model_class.__name__}: "
            f"{len(actual_required)} required fields (all properties): {actual_required}"
        )

        # Remove descriptions from $ref fields (OpenAI requirement)
        for prop_name, prop_def in fixed_schema["properties"].items():
            if isinstance(prop_def, dict) and "$ref" in prop_def:
                # Keep ONLY the $ref key, remove description or any other keys
                fixed_schema["properties"][prop_name] = {"$ref": prop_def["$ref"]}
                self.logger.debug(
                    f"Cleaned $ref field {prop_name} for OpenAI compatibility"
                )

        # Ensure additionalProperties is false (OpenAI requirement)
        fixed_schema["additionalProperties"] = False

        # Handle nested model definitions in $defs
        # CRITICAL FIX: Apply same ALL-fields-required rule to nested models
        if "$defs" in fixed_schema:
            for def_name, def_schema in fixed_schema["$defs"].items():
                if "properties" in def_schema and isinstance(
                    def_schema["properties"], dict
                ):
                    # Apply the same ALL-properties-required rule to nested models
                    nested_properties = def_schema["properties"]
                    nested_required = list(nested_properties.keys())
                    def_schema["required"] = nested_required
                    def_schema["additionalProperties"] = False

                    # Handle nullable fields in nested models too
                    for nested_prop_name, nested_prop_def in nested_properties.items():
                        if isinstance(nested_prop_def, dict):
                            # Check if this is an Optional field (anyOf with null)
                            is_nullable = isinstance(
                                nested_prop_def.get("anyOf"), list
                            ) and any(
                                item.get("type") == "null"
                                for item in nested_prop_def.get("anyOf", [])
                            )

                            # For Optional fields that aren't already nullable, make them nullable
                            if not is_nullable and "type" in nested_prop_def:
                                # Check if this looks like an Optional field (has default: null)
                                if nested_prop_def.get("default") is None:
                                    original_type = nested_prop_def.copy()
                                    original_type.pop(
                                        "default", None
                                    )  # Remove default from type definition
                                    # DON'T remove the type - OpenAI requires it
                                    # Clean unsupported constraints from the original type
                                    unsupported_keys = [
                                        "maxLength",
                                        "minLength",
                                        "format",
                                        "pattern",
                                        "maxItems",
                                        "minItems",
                                        "maximum",
                                        "minimum",
                                    ]
                                    for key in unsupported_keys:
                                        original_type.pop(key, None)
                                    nested_prop_def.clear()
                                    nested_prop_def["anyOf"] = [
                                        original_type,
                                        {"type": "null"},
                                    ]
                                    nested_prop_def["default"] = (
                                        None  # Preserve default at top level
                                    )

                            # Clean unsupported constraints
                            unsupported_keys = [
                                "maxLength",
                                "minLength",
                                "format",
                                "pattern",
                                "maxItems",
                                "minItems",
                                "maximum",
                                "minimum",
                            ]
                            for key in unsupported_keys:
                                nested_prop_def.pop(key, None)

                            # CRITICAL FIX: Handle Dict fields in nested models
                            # Dict fields should preserve their additionalProperties type information
                            # Do NOT set to false - that breaks Dict field functionality
                            if (
                                nested_prop_def.get("type") == "object"
                                and "properties" not in nested_prop_def
                                and "additionalProperties" in nested_prop_def
                            ):
                                # This is a Dict field - preserve existing additionalProperties
                                # Pydantic generates {"type": "string"} or similar for typed Dicts
                                pass  # Keep the existing additionalProperties value

                    self.logger.debug(
                        f"Fixed nested model {def_name}: {len(nested_required)} required fields (all properties): {nested_required}"
                    )

        # Recursively clean up $ref fields and unsupported constraints throughout the schema
        def clean_refs_recursive(obj: Any) -> Any:
            """Recursively clean $ref fields and unsupported constraints throughout the schema."""
            if isinstance(obj, dict):
                if "$ref" in obj and len(obj) > 1:
                    # Keep only the $ref key
                    return {"$ref": obj["$ref"]}
                else:
                    # Clean unsupported constraints and recursively process
                    cleaned = {}
                    unsupported_keys = [
                        "maxLength",
                        "minLength",
                        "format",
                        "pattern",
                        "maxItems",
                        "minItems",
                        "maximum",
                        "minimum",
                    ]
                    for k, v in obj.items():
                        if k not in unsupported_keys:
                            cleaned[k] = clean_refs_recursive(v)

                    # CRITICAL FIX: Ensure structured objects have additionalProperties: false
                    if cleaned.get("type") == "object":
                        has_properties = "properties" in cleaned or "$ref" in cleaned
                        has_additional_props = "additionalProperties" in cleaned

                        if has_properties and not has_additional_props:
                            # This is a structured object - set additionalProperties: false
                            cleaned["additionalProperties"] = False

                    return cleaned
            elif isinstance(obj, list):
                return [clean_refs_recursive(item) for item in obj]
            else:
                return obj

        # Apply recursive cleaning
        fixed_schema = clean_refs_recursive(fixed_schema)

        self.logger.debug(
            f"OpenAI schema finalized for {model_class.__name__}: "
            f"required={actual_required}, additionalProperties=false, refs cleaned"
        )

        return cast(Dict[str, Any], fixed_schema)

    async def _fallback_to_parser(
        self,
        messages: List[tuple[str, str]],
        output_class: Type[T],
        include_raw: bool = False,
    ) -> T:
        """Fallback to PydanticOutputParser (article's fallback strategy)."""

        # Create parser (article's pattern)
        parser: PydanticOutputParser[T] = PydanticOutputParser(
            pydantic_object=output_class
        )

        # Enhance prompt with format instructions (key from article)
        original_prompt = messages[-1][1]  # Get human message
        enhanced_prompt = f"{original_prompt}\n\n{parser.get_format_instructions()}"

        # Replace the human message with enhanced version
        enhanced_messages = messages[:-1] + [("human", enhanced_prompt)]

        # Get raw response
        assert self.llm is not None  # Type assertion - already checked above
        response = await self.llm.ainvoke(enhanced_messages)

        # Parse response content (article's pattern)
        content_str = ""  # Initialize for error handling
        try:
            # Ensure content is a string for parser
            content_str = str(response.content) if response.content is not None else ""
            result = parser.parse(content_str)

            # Store raw response for debugging if requested
            if include_raw:
                setattr(result, "_raw_response", content_str)

            return result

        except Exception as e:
            raise LLMValidationError(
                message=f"Failed to parse response into {output_class.__name__}",
                model_name=self.model_name,
                validation_errors=[str(e)],
                context={
                    "raw_response": content_str[:500],  # Truncated for logging
                    "parser_instructions": parser.get_format_instructions(),
                },
            )

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get service usage statistics."""
        total = int(self.metrics["total_calls"])
        successful = int(self.metrics["successful_structured"])
        fallback = int(self.metrics["fallback_used"])
        failures = int(self.metrics["validation_failures"])

        return {
            "total_calls": total,
            "success_rate": (successful / total if total > 0 else 0.0),
            "fallback_rate": (fallback / total if total > 0 else 0.0),
            "validation_failure_rate": (failures / total if total > 0 else 0.0),
            "metrics": self.metrics,
        }

    def clear_cache(self) -> None:
        """Reset service metrics."""
        self.metrics = {
            "total_calls": 0,
            "successful_structured": 0,
            "fallback_used": 0,
            "validation_failures": 0,
            "model_selected": (
                self.model_name if hasattr(self, "model_name") else "unknown"
            ),
        }