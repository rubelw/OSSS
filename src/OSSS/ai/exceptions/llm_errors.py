"""
LLM-specific exceptions for CogniVault.

This module defines exceptions related to Language Model interactions,
API failures, and LLM provider-specific error conditions with intelligent
retry policies and circuit breaker support.
"""

from typing import Optional, Dict, Any, List
from . import CogniVaultError, ErrorSeverity, RetryPolicy


class LLMError(CogniVaultError):
    """
    Base exception for LLM-related failures.

    Represents errors that occur during interactions with Language Model
    providers, designed for intelligent retry and circuit breaker patterns.
    """

    def __init__(
        self,
        message: str,
        llm_provider: str,
        error_code: str = "llm_error",
        severity: ErrorSeverity = ErrorSeverity.HIGH,
        retry_policy: RetryPolicy = RetryPolicy.BACKOFF,
        context: Optional[Dict[str, Any]] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        cause: Optional[Exception] = None,
        api_error_code: Optional[str] = None,
        api_error_type: Optional[str] = None,
    ) -> None:
        context = context or {}
        context.update(
            {
                "llm_provider": llm_provider,
                "api_error_code": api_error_code,
                "api_error_type": api_error_type,
            }
        )

        super().__init__(
            message=message,
            error_code=error_code,
            severity=severity,
            retry_policy=retry_policy,
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
        )
        self.llm_provider = llm_provider
        self.api_error_code = api_error_code
        self.api_error_type = api_error_type


class LLMQuotaError(LLMError):
    """
    Exception raised when LLM API quota is exhausted.

    Represents quota/billing limit exceeded errors that require
    manual intervention (billing update) and should not be retried.
    """

    def __init__(
        self,
        llm_provider: str,
        quota_type: str = "tokens",
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = message or f"{llm_provider} API quota exceeded for {quota_type}"

        context = context or {}
        context.update({"quota_type": quota_type, "billing_check_required": True})

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_quota_exceeded",
            severity=ErrorSeverity.CRITICAL,
            retry_policy=RetryPolicy.NEVER,  # Quota issues need billing fix
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="insufficient_quota",
        )
        self.quota_type = quota_type

    def get_user_message(self) -> str:
        """Get user-friendly error message with billing guidance."""
        return (
            f"âŒ {self.agent_id or 'LLM'} failed: API quota exceeded ({self.quota_type})\n"
            f"ğŸ’¡ Tip: Check your {self.llm_provider} billing dashboard and add credits."
        )


class LLMAuthError(LLMError):
    """
    Exception raised when LLM API authentication fails.

    Represents authentication failures due to invalid API keys,
    expired tokens, or insufficient permissions.
    """

    def __init__(
        self,
        llm_provider: str,
        auth_issue: str = "invalid_api_key",
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = message or f"{llm_provider} authentication failed: {auth_issue}"

        context = context or {}
        context.update({"auth_issue": auth_issue, "api_key_check_required": True})

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_auth_error",
            severity=ErrorSeverity.CRITICAL,
            retry_policy=RetryPolicy.NEVER,  # Auth issues need manual fix
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="invalid_api_key",
        )
        self.auth_issue = auth_issue

    def get_user_message(self) -> str:
        """Get user-friendly error message with API key guidance."""
        return (
            f"âŒ {self.agent_id or 'LLM'} failed: Authentication error ({self.auth_issue})\n"
            f"ğŸ’¡ Tip: Check your {self.llm_provider} API key in .env file."
        )


class LLMRateLimitError(LLMError):
    """
    Exception raised when LLM API rate limits are hit.

    Represents temporary rate limiting that should be retried
    with exponential backoff and circuit breaker patterns.
    """

    def __init__(
        self,
        llm_provider: str,
        rate_limit_type: str = "requests_per_minute",
        retry_after_seconds: Optional[float] = None,
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = message or f"{llm_provider} rate limit exceeded: {rate_limit_type}"

        context = context or {}
        context.update(
            {
                "rate_limit_type": rate_limit_type,
                "retry_after_seconds": retry_after_seconds,
                "temporary_failure": True,
            }
        )

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_rate_limit",
            severity=ErrorSeverity.MEDIUM,
            retry_policy=RetryPolicy.BACKOFF,  # Rate limits should be retried with backoff
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="rate_limit_exceeded",
        )
        self.rate_limit_type = rate_limit_type
        self.retry_after_seconds = retry_after_seconds

    def get_user_message(self) -> str:
        """Get user-friendly error message with rate limit guidance."""
        retry_msg = ""
        if self.retry_after_seconds:
            retry_msg = f" (retry in {self.retry_after_seconds}s)"

        return (
            f"âŒ {self.agent_id or 'LLM'} failed: Rate limit exceeded{retry_msg}\n"
            f"ğŸ’¡ Tip: The system will retry automatically with backoff."
        )


class LLMTimeoutError(LLMError):
    """
    Exception raised when LLM API requests time out.

    Represents timeout failures that may be retried depending
    on the timeout duration and context.
    """

    def __init__(
        self,
        llm_provider: str,
        timeout_seconds: float,
        timeout_type: str = "request",
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = (
            message or f"{llm_provider} {timeout_type} timeout after {timeout_seconds}s"
        )

        context = context or {}
        context.update(
            {
                "timeout_seconds": timeout_seconds,
                "timeout_type": timeout_type,
                "network_issue_possible": True,
            }
        )

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_timeout",
            severity=ErrorSeverity.MEDIUM,
            retry_policy=RetryPolicy.BACKOFF,  # Timeouts might be temporary
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="timeout",
        )
        self.timeout_seconds = timeout_seconds
        self.timeout_type = timeout_type

    def get_user_message(self) -> str:
        """Get user-friendly error message with timeout guidance."""
        return (
            f"âŒ {self.agent_id or 'LLM'} failed: Request timeout ({self.timeout_seconds}s)\n"
            f"ğŸ’¡ Tip: Check your network connection or try a simpler query."
        )


class LLMContextLimitError(LLMError):
    """
    Exception raised when LLM context/token limits are exceeded.

    Represents input context that's too large for the model,
    requiring input reduction or chunking strategies.
    """

    def __init__(
        self,
        llm_provider: str,
        model_name: str,
        token_count: int,
        max_tokens: int,
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = (
            message
            or f"{model_name} context limit exceeded: {token_count}/{max_tokens} tokens"
        )

        context = context or {}
        context.update(
            {
                "model_name": model_name,
                "token_count": token_count,
                "max_tokens": max_tokens,
                "token_overflow": token_count - max_tokens,
            }
        )

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_context_limit",
            severity=ErrorSeverity.HIGH,
            retry_policy=RetryPolicy.NEVER,  # Context limits need input reduction
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="context_length_exceeded",
        )
        self.model_name = model_name
        self.token_count = token_count
        self.max_tokens = max_tokens

    def get_user_message(self) -> str:
        """Get user-friendly error message with context limit guidance."""
        overflow = self.token_count - self.max_tokens
        return (
            f"âŒ {self.agent_id or 'LLM'} failed: Input too large ({overflow} tokens over limit)\n"
            f"ğŸ’¡ Tip: Reduce query length or break into smaller parts."
        )


class LLMModelNotFoundError(LLMError):
    """
    Exception raised when the specified LLM model is not available.

    Represents model availability issues, deprecation, or
    incorrect model names.
    """

    def __init__(
        self,
        llm_provider: str,
        model_name: str,
        available_models: Optional[List[str]] = None,
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = (
            message or f"{llm_provider} model '{model_name}' not found or unavailable"
        )

        context = context or {}
        context.update(
            {
                "model_name": model_name,
                "available_models": available_models or [],
                "model_deprecated_possible": True,
            }
        )

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_model_not_found",
            severity=ErrorSeverity.HIGH,
            retry_policy=RetryPolicy.NEVER,  # Model issues need config fix
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="model_not_found",
        )
        self.model_name = model_name
        self.available_models = available_models or []

    def get_user_message(self) -> str:
        """Get user-friendly error message with model guidance."""
        base_msg = (
            f"âŒ {self.agent_id or 'LLM'} failed: Model '{self.model_name}' not available\n"
            f"ğŸ’¡ Tip: Check model name in configuration."
        )

        if self.available_models:
            models_str = ", ".join(self.available_models[:3])
            base_msg += f" Available models: {models_str}"
            if len(self.available_models) > 3:
                base_msg += "..."

        return base_msg


class LLMServerError(LLMError):
    """
    Exception raised when LLM provider has server-side issues.

    Represents 5xx errors from the LLM provider that should
    trigger circuit breaker patterns and aggressive retries.
    """

    def __init__(
        self,
        llm_provider: str,
        http_status: int,
        error_details: Optional[str] = None,
        message: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        message = (
            message
            or f"{llm_provider} server error (HTTP {http_status}): {error_details or 'Unknown'}"
        )

        context = context or {}
        context.update(
            {
                "http_status": http_status,
                "error_details": error_details,
                "server_side_issue": True,
            }
        )

        super().__init__(
            message=message,
            llm_provider=llm_provider,
            error_code="llm_server_error",
            severity=ErrorSeverity.HIGH,
            retry_policy=RetryPolicy.CIRCUIT_BREAKER,  # Server errors should use circuit breaker
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code=f"http_{http_status}",
        )
        self.http_status = http_status
        self.error_details = error_details

    def get_user_message(self) -> str:
        """Get user-friendly error message with server error guidance."""
        return (
            f"âŒ {self.agent_id or 'LLM'} failed: {self.llm_provider} server error (HTTP {self.http_status})\n"
            f"ğŸ’¡ Tip: This is a provider issue. The system will retry automatically."
        )


class LLMValidationError(LLMError):
    """
    Exception raised when LLM response validation fails.

    Represents structured response parsing failures when using Pydantic AI
    or other structured output formats. These errors indicate the LLM
    returned content that doesn't match the expected schema.
    """

    def __init__(
        self,
        message: str,
        model_name: str,
        validation_errors: Optional[list[str]] = None,
        expected_schema: Optional[str] = None,
        actual_response: Optional[str] = None,
        step_id: Optional[str] = None,
        agent_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        cause: Optional[Exception] = None,
    ) -> None:
        context = context or {}
        context.update(
            {
                "model_name": model_name,
                "validation_errors": validation_errors or [],
                "expected_schema": expected_schema,
                "actual_response": actual_response,
                "response_parsing_failure": True,
            }
        )

        super().__init__(
            message=message,
            llm_provider="openai",  # Currently assumes OpenAI, could be parameterized
            error_code="llm_validation_error",
            severity=ErrorSeverity.MEDIUM,
            retry_policy=RetryPolicy.BACKOFF,  # Validation failures may succeed on retry
            context=context,
            step_id=step_id,
            agent_id=agent_id,
            cause=cause,
            api_error_code="validation_failed",
        )
        self.model_name = model_name
        self.validation_errors = validation_errors or []
        self.expected_schema = expected_schema
        self.actual_response = actual_response

    def get_user_message(self) -> str:
        """Get user-friendly error message with validation guidance."""
        error_count = len(self.validation_errors)
        base_msg = (
            f"âŒ {self.agent_id or 'LLM'} failed: Response validation failed "
            f"({error_count} error{'s' if error_count != 1 else ''})\n"
            f"ğŸ’¡ Tip: The system will retry with a clearer prompt."
        )

        if self.validation_errors:
            # Show first validation error for context
            first_error = self.validation_errors[0]
            if len(first_error) < 100:  # Only show if reasonably short
                base_msg += f"\n   Detail: {first_error}"

        return base_msg