"""
Production LangGraph orchestrator for OSSS agents.

This module provides LangGraph integration implementing production-ready
DAG execution with StateGraph orchestration.

Features:
- True DAG-based execution using LangGraph StateGraph
- Parallel execution where dependencies allow
- Type-safe state management with TypedDict schemas
- Circuit breaker patterns for error handling
- Comprehensive logging and metrics
- State bridge integration for AgentContext compatibility
"""

import time
import uuid
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from OSSS.ai.context import AgentContext
from OSSS.ai.agents.base_agent import BaseAgent
from OSSS.ai.agents.registry import get_agent_registry
from OSSS.ai.observability import get_logger
from OSSS.ai.correlation import (
    ensure_correlation_context,
    create_child_span,
    add_trace_metadata,
    CorrelationContext,
    context_correlation_id,
    context_workflow_id,
    context_trace_metadata,
)
from OSSS.ai.events import (
    emit_workflow_started,
    emit_workflow_completed,
    emit_routing_decision_from_object,
)
from OSSS.ai.orchestration.state_bridge import AgentContextStateBridge
from OSSS.ai.orchestration.state_schemas import (
    OSSSState,
    OSSSContext,
    RefinerState,
    CriticState,
    HistorianState,
    SynthesisState,
    create_initial_state,
    validate_state_integrity,
)
from OSSS.ai.orchestration.node_wrappers import (
    NodeExecutionError,
    get_node_dependencies,
)
from OSSS.ai.orchestration.memory_manager import (
    OSSSMemoryManager,
    create_memory_manager,
)
from OSSS.ai.langgraph_backend import (
    GraphFactory,
    GraphConfig,
    GraphBuildError,
    CacheConfig,
)
from OSSS.ai.langgraph_backend.graph_patterns.conditional import (
    EnhancedConditionalPattern,
    ContextAnalyzer,
    PerformanceTracker,
)
from OSSS.ai.routing import (
    ResourceOptimizer,
    ResourceConstraints,
    OptimizationStrategy,
    RoutingDecision,
)
from OSSS.ai.orchestration.routing import should_run_historian
from OSSS.ai.rag.jsonl_rag import rag_prefetch


logger = get_logger(__name__)

def _ensure_effective_queries(state: Dict[str, Any], base_query: str) -> None:
    """
    Ensure execution_state.effective_queries exists for the run.

    Node wrappers will overwrite per-agent values right before each agent runs:
      execution_state["effective_queries"][agent_name] = effective_query
    """
    exec_state = state.setdefault("execution_state", {})
    if not isinstance(exec_state, dict):
        state["execution_state"] = {}
        exec_state = state["execution_state"]

    effective = exec_state.setdefault("effective_queries", {})
    if not isinstance(effective, dict):
        exec_state["effective_queries"] = {}
        effective = exec_state["effective_queries"]

    # Seed the baseline query for reference (node wrappers can override)
    effective.setdefault("user", base_query)


class LangGraphOrchestrator:
    """
    Production LangGraph orchestrator for OSSS agents.

    This orchestrator uses LangGraph library to provide production-ready
    DAG-based execution with advanced state management, parallel processing, and
    conditional routing capabilities.

    Features:
    - StateGraph-based DAG execution with proper dependencies
    - Parallel execution of independent agents (Refiner → [Critic, Historian] → Synthesis)
    - Type-safe state management with comprehensive validation
    - Circuit breaker patterns for robust error handling
    - Optional memory checkpointing for stateful conversations
    - Comprehensive logging and performance metrics
    """

    def __init__(
        self,
        agents_to_run: Optional[List[str]] = None,
        enable_checkpoints: bool = False,
        thread_id: Optional[str] = None,
        memory_manager: Optional[OSSSMemoryManager] = None,
        use_enhanced_routing: bool = True,
        optimization_strategy: OptimizationStrategy = OptimizationStrategy.BALANCED,
    ) -> None:
        """
        Initialize the production LangGraph orchestrator.

        Parameters
        ----------
        agents_to_run : List[str], optional
            List of agent names to run. For Phase 2.1, defaults to refiner, critic, historian, synthesis.
        enable_checkpoints : bool, optional
            Whether to enable memory checkpointing for stateful conversations.
        thread_id : str, optional
            Thread ID for conversation scoping (auto-generated if not provided).
        memory_manager : OSSSMemoryManager, optional
            Custom memory manager instance. If None, one will be created.
        """
        # Default agents - will be optimized by enhanced routing if enabled
        self.default_agents = [
            "refiner",
            "critic",
            "historian",
            "synthesis",
        ]

        # Enhanced routing configuration
        self.use_enhanced_routing = use_enhanced_routing
        self.optimization_strategy = optimization_strategy

        # Initialize routing systems
        self.conditional_pattern: Optional[EnhancedConditionalPattern] = (
            EnhancedConditionalPattern() if self.use_enhanced_routing else None
        )
        self.resource_optimizer: Optional[ResourceOptimizer] = (
            ResourceOptimizer() if self.use_enhanced_routing else None
        )
        self.context_analyzer: Optional[ContextAnalyzer] = (
            ContextAnalyzer() if self.use_enhanced_routing else None
        )
        self.performance_tracker: Optional[PerformanceTracker] = (
            PerformanceTracker() if self.use_enhanced_routing else None
        )

        # Set initial agents (may be overridden by routing)
        self.agents_to_run = agents_to_run or self.default_agents
        self.enable_checkpoints = enable_checkpoints
        self.thread_id = thread_id
        self.registry = get_agent_registry()
        self.logger = get_logger(f"{__name__}.LangGraphOrchestrator")

        # Initialize memory manager
        if memory_manager:
            self.memory_manager = memory_manager
        else:
            self.memory_manager = create_memory_manager(
                enable_checkpoints=enable_checkpoints,
                thread_id=thread_id,
            )

        # Add agents property for compatibility with health checks and dry runs
        self.agents: List[BaseAgent] = []  # Will be populated when agents are created

        # Performance tracking
        self.total_executions = 0
        self.successful_executions = 0
        self.failed_executions = 0

        # State bridge for AgentContext <-> LangGraph state conversion
        self.state_bridge = AgentContextStateBridge()

        # Initialize GraphFactory for graph building
        cache_config = CacheConfig(max_size=10, ttl_seconds=1800)  # 30 minutes TTL
        self.graph_factory = GraphFactory(cache_config)

        # LangGraph components (initialized lazily)
        self._graph = None
        self._compiled_graph = None

        self.logger.info(
            f"Initialized LangGraphOrchestrator with agents: {self.agents_to_run}, "
            f"checkpoints: {self.enable_checkpoints}, thread_id: {self.thread_id}"
        )

    async def run(
        self, query: str, config: Optional[Dict[str, Any]] = None
    ) -> AgentContext:
        """
        Execute agents using LangGraph StateGraph orchestration.

        This method implements true DAG-based execution with:
        - Refiner → [Critic, Historian] → Synthesis pipeline
        - Parallel execution of Critic and Historian after Refiner
        - Type-safe state management
        - Comprehensive error handling and recovery
        - Correlation context propagation for tracing

        Parameters
        ----------
        query : str
            The query to process
        config : Dict[str, Any], optional
            Execution configuration options

        Returns
        -------
        AgentContext
            The final context after LangGraph execution

        Raises
        ------
        NodeExecutionError
            If LangGraph execution fails
        """
        config = config or {}
        start_time = time.time()

        # Handle correlation context - use provided correlation_id and workflow_id if available
        provided_correlation_id = config.get("correlation_id") if config else None
        provided_workflow_id = config.get("workflow_id") if config else None

        if provided_correlation_id:
            # Use provided workflow_id to prevent duplicate ID generation, or create new one if not provided
            workflow_id = provided_workflow_id or str(uuid.uuid4())
            context_correlation_id.set(provided_correlation_id)
            context_workflow_id.set(workflow_id)
            context_trace_metadata.set({})
            correlation_ctx = CorrelationContext(
                correlation_id=provided_correlation_id,
                workflow_id=workflow_id,
                metadata={},
            )
        else:
            # Ensure correlation context exists (create if not present)
            correlation_ctx = ensure_correlation_context()

        # Use correlation IDs for execution tracking
        execution_id = correlation_ctx.workflow_id
        correlation_id = correlation_ctx.correlation_id

        # Create orchestrator span for detailed tracing
        orchestrator_span = create_child_span("langgraph_orchestrator")

        # Add orchestrator metadata to trace
        add_trace_metadata("orchestrator_type", "langgraph-real")
        add_trace_metadata("agents_requested", self.agents_to_run)
        add_trace_metadata("query_length", len(query))
        add_trace_metadata("config", config)

        # Enhanced routing decision if enabled
        routing_decision = None
        if self.use_enhanced_routing:
            routing_decision = await self._make_routing_decision(
                query, self.default_agents, config
            )

            # Update agents based on routing decision
            self.agents_to_run = routing_decision.selected_agents

            # ------------------------------------------------------------
            # Fast-path: skip historian unless the query likely needs history
            # ------------------------------------------------------------
            if "historian" in [a.lower() for a in self.agents_to_run]:
                if not should_run_historian(query):
                    self.logger.info(
                        f"[orchestrator] Skipping historian (fast path) for query_len={len(query)}"
                    )

                    # Remove historian from this run
                    self.agents_to_run = [a for a in self.agents_to_run if a.lower() != "historian"]

                    # IMPORTANT: compiled graphs are cached per agent list.
                    # If we changed the agent set, force a rebuild.
                    self._compiled_graph = None
                    self._graph = None

            # Emit routing decision event
            emit_routing_decision_from_object(
                routing_decision,  # positional obj
                workflow_id=execution_id,
                correlation_id=correlation_id,
                metadata={
                    "orchestrator_type": "langgraph-real",
                    "optimization_strategy": self.optimization_strategy.value,
                    "routing_enabled": True,
                },
            )

        self.logger.info(
            f"Starting LangGraph execution for query: {query[:100]}... "
            f"(execution_id: {execution_id}, correlation_id: {correlation_id})"
        )
        self.logger.info("Execution mode: langgraph")
        self.logger.info(f"Agents to run: {self.agents_to_run}")
        if routing_decision:
            self.logger.info(f"Routing strategy: {routing_decision.routing_strategy}")
            self.logger.info(
                f"Routing confidence: {routing_decision.confidence_score:.2f}"
            )
        self.logger.info(f"Config: {config}")
        self.logger.info(f"Correlation context: {correlation_ctx.to_dict()}")

        # Emit workflow started event
        emit_workflow_started(
            workflow_id=execution_id,
            query=query,
            agents_requested=self.agents_to_run,  # ✅ change agents -> agents_requested
            execution_config=config,
            correlation_id=correlation_id,
            metadata={
                "orchestrator_type": "langgraph-real",
                "orchestrator_span": orchestrator_span,
                "phase": "phase2_1",
                "checkpoints_enabled": self.enable_checkpoints,
            },
        )

        self.total_executions += 1

        try:
            # Get or generate thread ID for this execution
            thread_id = self.memory_manager.get_thread_id(config.get("thread_id"))

            # Create initial LangGraph state
            from OSSS.ai.orchestration.intent_classifier import classify_intent_llm, to_query_profile

            # Create initial LangGraph state
            initial_state = create_initial_state(query, execution_id, correlation_id)

            # Ensure execution_state.effective_queries exists
            _ensure_effective_queries(initial_state, query)

            # ✅ LLM intent profiling (once per workflow)
            use_llm_intent = bool(config.get("execution_config", {}).get("use_llm_intent", False))
            if use_llm_intent:
                intent_result = await classify_intent_llm(query)
                profile = to_query_profile(intent_result)

                exec_state = initial_state.setdefault("execution_state", {})
                if isinstance(exec_state, dict):
                    meta = exec_state.setdefault("agent_output_meta", {})
                    if isinstance(meta, dict):
                        meta["_query_profile"] = profile

            # ---------------------------------------------------------------------
            # ✅ Optional RAG prefetch (JSONL embeddings) - inject into execution_state
            # ---------------------------------------------------------------------
            rag_cfg = (config.get("execution_config") or {}).get("rag", {}) if isinstance(config, dict) else {}
            rag_enabled = bool(rag_cfg.get("enabled", False))

            if rag_enabled:
                try:
                    ollama_base = rag_cfg.get("ollama_base", "http://localhost:11434")
                    embed_model = rag_cfg.get("embed_model", "nomic-embed-text")
                    jsonl_path = rag_cfg.get("jsonl_path")  # REQUIRED when enabled
                    top_k = int(rag_cfg.get("top_k", 5))

                    if not jsonl_path:
                        raise ValueError("RAG enabled but execution_config.rag.jsonl_path is missing")

                    rag = await rag_prefetch_jsonl(
                        query,
                        ollama_base=ollama_base,
                        embed_model=embed_model,
                        jsonl_path=jsonl_path,
                        top_k=top_k,
                    )

                    exec_state = initial_state.setdefault("execution_state", {})
                    if not isinstance(exec_state, dict):
                        initial_state["execution_state"] = {}
                        exec_state = initial_state["execution_state"]

                    # Put both the prompt-ready context and structured hits in state
                    exec_state["rag_context"] = rag.get("context", "")
                    exec_state["rag_hits"] = rag.get("hits", [])
                    exec_state["rag_enabled"] = True
                    exec_state["rag_meta"] = {
                        "provider": "ollama",
                        "embed_model": embed_model,
                        "jsonl_path": jsonl_path,
                        "top_k": top_k,
                    }

                    self.logger.info(
                        f"[orchestrator] RAG prefetch complete: hits={len(exec_state['rag_hits'])}, top_k={top_k}"
                    )

                except Exception as e:
                    # Decide if you want hard-fail vs soft-fail.
                    # I strongly recommend soft-fail first, with observability.
                    self.logger.warning(f"[orchestrator] RAG prefetch failed (continuing without RAG): {e}")

                    exec_state = initial_state.setdefault("execution_state", {})
                    if isinstance(exec_state, dict):
                        exec_state["rag_enabled"] = False
                        exec_state["rag_error"] = str(e)


            # Ensure execution_state.effective_queries exists (node wrappers will fill per-agent)
            _ensure_effective_queries(initial_state, query)

            # Validate initial state
            if not validate_state_integrity(initial_state):
                raise NodeExecutionError("Initial state validation failed")

            # Create initial checkpoint if enabled
            if self.memory_manager.is_enabled():
                self.memory_manager.create_checkpoint(
                    thread_id=thread_id,
                    state=initial_state,
                    agent_step="initialization",
                    metadata={"execution_id": execution_id, "query": query},
                )

            # Build and compile StateGraph if not already done
            compiled_graph = await self._get_compiled_graph()

            # Execute the StateGraph
            self.logger.info(
                f"Executing LangGraph StateGraph with thread_id: {thread_id}"
            )

            # Create context for LangGraph 0.6.0 execution
            context = OSSSContext(
                thread_id=thread_id,
                execution_id=execution_id,
                query=query,
                correlation_id=ensure_correlation_context().correlation_id,
                enable_checkpoints=self.enable_checkpoints,
            )

            # Run the StateGraph with new Context API
            final_state = await compiled_graph.ainvoke(initial_state, context=context)

            # Validate final state
            if not validate_state_integrity(final_state):
                self.logger.warning("Final state validation failed, but proceeding")

            # Create final checkpoint if enabled
            if self.memory_manager.is_enabled():
                self.memory_manager.create_checkpoint(
                    thread_id=thread_id,
                    state=final_state,
                    agent_step="completion",
                    metadata={
                        "execution_id": execution_id,
                        "query": query,
                        "successful_agents": final_state["successful_agents"],
                        "failed_agents": final_state["failed_agents"],
                        "completion_status": "success",
                    },
                )

            # Convert LangGraph state back to AgentContext
            agent_context = await self._convert_state_to_context(final_state)

            # Add execution metadata with correlation context
            total_time_ms = (time.time() - start_time) * 1000
            agent_context.execution_state.update(
                {
                    "orchestrator_type": "langgraph-real",
                    "phase": "phase2_1",
                    "execution_id": execution_id,
                    "correlation_id": correlation_id,
                    "orchestrator_span": orchestrator_span,
                    "thread_id": thread_id,
                    "agents_requested": self.agents_to_run,
                    "config": config,
                    "execution_time_ms": total_time_ms,
                    "langgraph_execution": True,
                    "checkpoints_enabled": self.memory_manager.is_enabled(),
                    "successful_agents_count": len(final_state["successful_agents"]),
                    "failed_agents_count": len(final_state["failed_agents"]),
                    "errors_count": len(final_state["errors"]),
                    "correlation_context": correlation_ctx.to_dict(),
                }
            )

            # Emit workflow completed event (with truncated outputs for logging)
            # Helper to extract main content from structured or string outputs
            def truncate_output(output: Any) -> str:
                """Extract and truncate output for logging purposes."""
                if isinstance(output, str):
                    return output[:200] + "..." if len(output) > 200 else output
                elif isinstance(output, dict):
                    # Extract main content field if available
                    main_fields = [
                        "refined_question",
                        "historical_summary",
                        "critique",
                        "final_analysis",
                    ]
                    for field in main_fields:
                        if field in output:
                            content = str(output[field])
                            return (
                                content[:200] + "..." if len(content) > 200 else content
                            )
                    # Fallback to string representation
                    content = str(output)
                    return content[:200] + "..." if len(content) > 200 else content
                else:
                    content = str(output)
                    return content[:200] + "..." if len(content) > 200 else content

            emit_workflow_completed(
                workflow_id=execution_id,
                status=(
                    "completed"
                    if not final_state["failed_agents"]
                    else "partial_failure"
                ),
                execution_time_seconds=total_time_ms / 1000,
                agent_outputs={
                    agent: truncate_output(output)
                    for agent, output in agent_context.agent_outputs.items()
                },
                successful_agents=list(final_state["successful_agents"]),
                failed_agents=list(final_state["failed_agents"]),
                correlation_id=correlation_id,
                metadata={
                    "orchestrator_type": "langgraph-real",
                    "orchestrator_span": orchestrator_span,
                    "thread_id": thread_id,
                    "total_agents": len(self.agents_to_run),
                },
            )

            # Update statistics
            if final_state["failed_agents"]:
                self.failed_executions += 1
                self.logger.warning(
                    f"LangGraph execution completed with failures: {final_state['failed_agents']}"
                )
            else:
                self.successful_executions += 1

            self.logger.info(
                f"LangGraph execution completed in {total_time_ms:.2f}ms "
                f"(successful: {len(final_state['successful_agents'])}, "
                f"failed: {len(final_state['failed_agents'])})"
            )

            return agent_context

        except Exception as e:
            self.failed_executions += 1
            total_time_ms = (time.time() - start_time) * 1000

            self.logger.error(
                f"LangGraph execution failed after {total_time_ms:.2f}ms: {e}"
            )

            # Emit workflow failed event
            emit_workflow_completed(
                workflow_id=execution_id,
                status="failed",
                execution_time_seconds=total_time_ms / 1000,
                error_message=str(e),
                successful_agents=[],
                failed_agents=self.agents_to_run,  # All requested agents failed
                error_type=e.__class__.__name__,  # ✅ add this
                error_details={  # ✅ optional but very useful
                    "exception_module": e.__class__.__module__,
                    "exception_qualname": getattr(e.__class__, "__qualname__", e.__class__.__name__),
                },
                correlation_id=correlation_id,
                metadata={
                    "orchestrator_type": "langgraph-real",
                    "orchestrator_span": orchestrator_span,
                    "error_type": type(e).__name__,
                },
            )

            # Create fallback context with error information and correlation
            error_context = AgentContext(query=query)
            error_context.execution_state.update(
                {
                    "orchestrator_type": "langgraph-real",
                    "phase": "phase2_1",
                    "execution_id": execution_id,
                    "correlation_id": correlation_id,
                    "orchestrator_span": orchestrator_span,
                    "agents_requested": self.agents_to_run,
                    "config": config,
                    "execution_time_ms": total_time_ms,
                    "langgraph_execution": True,
                    "execution_error": str(e),
                    "execution_error_type": type(e).__name__,
                    "correlation_context": correlation_ctx.to_dict(),
                }
            )

            # Add error output
            error_context.add_agent_output(
                "langgraph_error",
                f"LangGraph execution failed: {e}\n"
                f"Execution ID: {execution_id}\n"
                f"Requested agents: {', '.join(self.agents_to_run)}\n"
                f"This indicates an issue with the DAG execution pipeline.",
            )

            raise NodeExecutionError(f"LangGraph execution failed: {e}") from e

    async def _get_compiled_graph(self) -> Any:
        """
        Get or create compiled LangGraph StateGraph using GraphFactory.

        Returns
        -------
        CompiledGraph
            Compiled LangGraph StateGraph ready for execution
        """
        if self._compiled_graph is None:
            self.logger.info("Building LangGraph StateGraph using GraphFactory...")

            try:
                # Create graph configuration
                config = GraphConfig(
                    agents_to_run=self.agents_to_run,
                    enable_checkpoints=self.enable_checkpoints,
                    memory_manager=self.memory_manager,
                    pattern_name="standard",  # Use standard pattern for Phase 2
                    cache_enabled=True,
                )

                # Validate agents before building
                if not self.graph_factory.validate_agents(self.agents_to_run):
                    raise GraphBuildError(f"Invalid agents: {self.agents_to_run}")

                # Create compiled graph using factory
                self._compiled_graph = self.graph_factory.create_graph(config)

                self.logger.info(
                    f"Successfully built LangGraph StateGraph with {len(self.agents_to_run)} agents "
                    f"(checkpoints: {self.enable_checkpoints})"
                )

            except GraphBuildError as e:
                self.logger.error(f"Graph building failed: {e}")
                raise NodeExecutionError(
                    f"Failed to build LangGraph StateGraph: {e}"
                ) from e
            except Exception as e:
                self.logger.error(f"Unexpected error during graph building: {e}")
                raise NodeExecutionError(
                    f"Failed to build LangGraph StateGraph: {e}"
                ) from e

        return self._compiled_graph

    async def _convert_state_to_context(
        self, final_state: OSSSState
    ) -> AgentContext:
        """
        Convert final LangGraph state back to AgentContext.

        Parameters
        ----------
        final_state : OSSSState
            Final state from LangGraph execution

        Returns
        -------
        AgentContext
            AgentContext with all agent outputs
        """
        # Create AgentContext
        context = AgentContext(query=final_state["query"])

        # Extract structured_outputs from LangGraph state if available
        # This contains the full Pydantic model dumps from agents
        if "structured_outputs" in final_state:
            context.execution_state["structured_outputs"] = final_state[
                "structured_outputs"
            ]
        else:
            context.execution_state["structured_outputs"] = {}

        # Carry forward per-agent effective queries if node wrappers recorded them
        try:
            exec_state = final_state.get("execution_state", {})
            if isinstance(exec_state, dict):
                eff = exec_state.get("effective_queries")
                if isinstance(eff, dict):
                    context.execution_state["effective_queries"] = eff
        except Exception:
            pass

        # Add agent outputs
        if final_state.get("refiner"):
            refiner_output: Optional[RefinerState] = final_state["refiner"]
            if refiner_output is not None:
                context.add_agent_output("refiner", refiner_output["refined_question"])
                context.execution_state["refiner_topics"] = refiner_output["topics"]
                context.execution_state["refiner_confidence"] = refiner_output[
                    "confidence"
                ]

        if final_state.get("critic"):
            critic_output: Optional[CriticState] = final_state["critic"]
            if critic_output is not None:
                context.add_agent_output("critic", critic_output["critique"])
                context.execution_state["critic_suggestions"] = critic_output[
                    "suggestions"
                ]
                context.execution_state["critic_severity"] = critic_output["severity"]

        if final_state.get("historian"):
            historian_output: Optional[HistorianState] = final_state["historian"]
            if historian_output is not None:
                context.add_agent_output(
                    "historian", historian_output["historical_summary"]
                )
                context.execution_state["historian_retrieved_notes"] = historian_output[
                    "retrieved_notes"
                ]
                context.execution_state["historian_search_strategy"] = historian_output[
                    "search_strategy"
                ]
                context.execution_state["historian_topics_found"] = historian_output[
                    "topics_found"
                ]
                context.execution_state["historian_confidence"] = historian_output[
                    "confidence"
                ]

        if final_state.get("synthesis"):
            synthesis_output: Optional[SynthesisState] = final_state["synthesis"]
            if synthesis_output is not None:
                context.add_agent_output(
                    "synthesis", synthesis_output["final_analysis"]
                )
                context.execution_state["synthesis_insights"] = synthesis_output[
                    "key_insights"
                ]
                context.execution_state["synthesis_themes"] = synthesis_output[
                    "themes_identified"
                ]

        # Track successful and failed agents
        for agent in final_state["successful_agents"]:
            context.successful_agents.add(agent)

        # Add error information if any
        if final_state["errors"]:
            context.execution_state["langgraph_errors"] = final_state["errors"]

        return context

    def get_execution_statistics(self) -> Dict[str, Any]:
        """
        Get orchestrator execution statistics.

        Returns
        -------
        Dict[str, Any]
            Execution statistics
        """
        success_rate = (
            self.successful_executions / self.total_executions
            if self.total_executions > 0
            else 0
        )

        return {
            "orchestrator_type": "langgraph-real",
            "implementation_status": "phase2_production_with_graph_factory",
            "total_executions": self.total_executions,
            "successful_executions": self.successful_executions,
            "failed_executions": self.failed_executions,
            "success_rate": success_rate,
            "agents_to_run": self.agents_to_run,
            "state_bridge_available": True,
            "checkpoints_enabled": self.enable_checkpoints,
            "dag_structure": "refiner → [critic, historian] → synthesis",
            "graph_factory_stats": self.graph_factory.get_cache_stats(),
            "available_patterns": self.graph_factory.get_available_patterns(),
        }

    def get_dag_structure(self) -> Dict[str, Any]:
        """
        Get information about the DAG structure.

        Returns
        -------
        Dict[str, Any]
            DAG structure information
        """
        dependencies = get_node_dependencies()

        return {
            "nodes": self.agents_to_run,
            "dependencies": dependencies,
            "execution_order": ["refiner", "critic", "historian", "synthesis"],
            "parallel_capable": [
                "critic",
                "historian",
            ],  # Can run in parallel after refiner
            "entry_point": "refiner",
            "terminal_nodes": ["synthesis"],
        }

    async def rollback_to_checkpoint(
        self, thread_id: Optional[str] = None, checkpoint_id: Optional[str] = None
    ) -> Optional[AgentContext]:
        """
        Rollback to a specific checkpoint and return the restored context.

        Parameters
        ----------
        thread_id : str, optional
            Thread ID for conversation. If None, uses current thread_id.
        checkpoint_id : str, optional
            Specific checkpoint ID. If None, uses latest checkpoint.

        Returns
        -------
        AgentContext, optional
            Restored context from checkpoint, or None if not found
        """
        if not self.memory_manager.is_enabled():
            self.logger.warning("Rollback requested but checkpointing is disabled")
            return None

        target_thread_id = thread_id or self.thread_id
        if not target_thread_id:
            self.logger.error("No thread ID available for rollback")
            return None

        # Attempt rollback through memory manager
        restored_state = self.memory_manager.rollback_to_checkpoint(
            thread_id=target_thread_id, checkpoint_id=checkpoint_id
        )

        if restored_state:
            # Convert restored state back to AgentContext
            context = await self._convert_state_to_context(restored_state)
            context.execution_state["rollback_performed"] = True
            context.execution_state["rollback_thread_id"] = target_thread_id
            context.execution_state["rollback_checkpoint_id"] = checkpoint_id

            self.logger.info(
                f"Successfully rolled back to checkpoint for thread {target_thread_id}"
            )
            return context
        else:
            self.logger.warning(
                f"Rollback failed - no checkpoint found for thread {target_thread_id}"
            )
            return None

    def get_checkpoint_history(
        self, thread_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get checkpoint history for a thread.

        Parameters
        ----------
        thread_id : str, optional
            Thread ID to get history for. If None, uses current thread_id.

        Returns
        -------
        List[Dict[str, Any]]
            List of checkpoint information dictionaries
        """
        target_thread_id = thread_id or self.thread_id
        if not target_thread_id:
            return []

        checkpoints = self.memory_manager.get_checkpoint_history(target_thread_id)
        return [
            {
                "checkpoint_id": checkpoint.checkpoint_id,
                "timestamp": checkpoint.timestamp.isoformat(),
                "agent_step": checkpoint.agent_step,
                "state_size_bytes": checkpoint.state_size_bytes,
                "success": checkpoint.success,
                "metadata": checkpoint.metadata,
            }
            for checkpoint in checkpoints
        ]

    def get_memory_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive memory and checkpoint statistics.

        Returns
        -------
        Dict[str, Any]
            Memory usage and checkpoint statistics
        """
        memory_stats = self.memory_manager.get_memory_stats()

        # Add orchestrator-specific stats
        orchestrator_stats = {
            "orchestrator_type": "langgraph-real",
            "checkpointing_enabled": self.memory_manager.is_enabled(),
            "current_thread_id": self.thread_id,
            "execution_statistics": self.get_execution_statistics(),
        }

        return {**memory_stats, **orchestrator_stats}

    def cleanup_expired_checkpoints(self) -> int:
        """
        Clean up expired checkpoints.

        Returns
        -------
        int
            Number of checkpoints removed
        """
        return self.memory_manager.cleanup_expired_checkpoints()

    def get_graph_cache_stats(self) -> Dict[str, Any]:
        """
        Get graph factory cache statistics.

        Returns
        -------
        Dict[str, Any]
            Cache statistics from the graph factory
        """
        return self.graph_factory.get_cache_stats()

    def clear_graph_cache(self) -> None:
        """Clear the graph compilation cache."""
        self.graph_factory.clear_cache()
        self.logger.info("Graph compilation cache cleared")

    def get_available_graph_patterns(self) -> List[str]:
        """
        Get list of available graph patterns.

        Returns
        -------
        List[str]
            List of pattern names
        """
        return self.graph_factory.get_available_patterns()

    def set_graph_pattern(self, pattern_name: str) -> None:
        """
        Set the graph pattern for future graph builds.

        Note: This will clear the current compiled graph to force rebuild
        with the new pattern.

        Parameters
        ----------
        pattern_name : str
            Name of the pattern to use
        """
        if pattern_name not in self.graph_factory.get_available_patterns():
            raise ValueError(
                f"Unknown pattern: {pattern_name}. Available: {self.graph_factory.get_available_patterns()}"
            )

        # Clear current graph to force rebuild with new pattern
        self._compiled_graph = None
        self._graph = None

        # Store pattern for next build (could be stored as instance variable if needed)
        self.logger.info(
            f"Graph pattern set to: {pattern_name}. Next graph build will use this pattern."
        )

    # Phase 2.0 Implementation Complete ✅
    # ✅ Add real LangGraph dependency to pyproject.toml (done in Phase 1)
    # ✅ Import real LangGraph StateGraph and related classes
    # ✅ Convert agents to LangGraph StateGraph nodes (node_wrappers.py)
    # ✅ Implement actual StateGraph execution with typed state management
    # ✅ Add comprehensive error handling with circuit breakers
    # ✅ Performance tracking and execution statistics

    # Phase 2.1 Complete ✅
    # ✅ Add Historian agent back into pipeline
    # ✅ Implement parallel execution of Critic and Historian
    # ✅ Enhanced CLI integration with checkpointing and rollback
    # ✅ Performance optimization and benchmarking vs legacy mode

    # Phase 2.2 Complete ✅ - Graph Builder Extraction
    # ✅ Extract graph building logic to dedicated GraphFactory
    # ✅ Implement graph patterns for different execution modes
    # ✅ Add graph compilation caching for performance
    # ✅ Separate concerns: orchestration vs graph building
    # ✅ Maintain backward compatibility with enhanced functionality

    async def _make_routing_decision(
        self, query: str, available_agents: List[str], config: Dict[str, Any]
    ) -> "RoutingDecision":
        """
        Make intelligent routing decision using enhanced routing system.

        Parameters
        ----------
        query : str
            The user query to analyze
        available_agents : List[str]
            Available agents to choose from
        config : Dict[str, Any]
            Configuration parameters

        Returns
        -------
        RoutingDecision
            Comprehensive routing decision with reasoning
        """
        # Import here to avoid circular imports - already imported at top of file

        # Analyze query complexity
        if not self.context_analyzer:
            raise ValueError("Context analyzer not available for routing decision")
        context_analysis = self.context_analyzer.analyze_context(query)

        # Get performance data from registry
        performance_data = {}
        for agent in available_agents:
            agent_lower = agent.lower()
            # Get performance metrics from the pattern's performance tracker
            if self.performance_tracker:
                performance_data[agent_lower] = {
                    "success_rate": self.performance_tracker.get_success_rate(
                        agent_lower
                    )
                    or 0.8,
                    "average_time_ms": self.performance_tracker.get_average_time(
                        agent_lower
                    )
                    or 2000.0,
                    "performance_score": self.performance_tracker.get_performance_score(
                        agent_lower
                    ),
                }
            else:
                # Fallback performance data when tracker is not available
                performance_data[agent_lower] = {
                    "success_rate": 0.8,
                    "average_time_ms": 2000.0,
                    "performance_score": 0.7,
                }

        # Build resource constraints from config
        constraints = ResourceConstraints(
            max_execution_time_ms=config.get("max_execution_time_ms"),
            max_agents=config.get("max_agents", 4),
            min_agents=config.get("min_agents", 1),
            min_success_rate=config.get("min_success_rate", 0.7),
        )

        # Extract context requirements
        context_requirements = {
            "requires_research": context_analysis.requires_research,
            "requires_criticism": context_analysis.requires_criticism,
            "requires_synthesis": True,  # Always needed for final output
            "requires_refinement": True,  # Always needed for input processing
        }

        # Make routing decision
        if not self.resource_optimizer:
            raise ValueError("Resource optimizer not available for routing decision")
        routing_decision = self.resource_optimizer.select_optimal_agents(
            available_agents=available_agents,
            complexity_score=context_analysis.complexity_score,
            performance_data=performance_data,
            constraints=constraints,
            strategy=self.optimization_strategy,
            context_requirements=context_requirements,
        )

        # Update performance tracker with routing decision
        if self.conditional_pattern:
            self.conditional_pattern.update_performance_metrics(
                "routing_decision",
                0.0,  # No execution time for decision
                routing_decision.confidence_score > 0.5,  # Success if high confidence
            )

        return routing_decision

    def update_agent_performance(
        self, agent: str, duration_ms: float, success: bool
    ) -> None:
        """
        Update performance metrics for an agent.

        Parameters
        ----------
        agent : str
            Agent name
        duration_ms : float
            Execution duration in milliseconds
        success : bool
            Whether execution was successful
        """
        if self.use_enhanced_routing and self.performance_tracker:
            self.performance_tracker.record_execution(agent, duration_ms, success)

    def get_routing_statistics(self) -> Dict[str, Any]:
        """
        Get routing system statistics.

        Returns
        -------
        Dict[str, Any]
            Routing statistics including performance and decision metrics
        """
        if not self.use_enhanced_routing:
            return {"enhanced_routing": False}

        stats = {
            "enhanced_routing": True,
            "optimization_strategy": self.optimization_strategy.value,
        }

        if self.conditional_pattern:
            stats.update(self.conditional_pattern.get_routing_statistics())

        return stats